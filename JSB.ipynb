{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrx7zGA6OAbIh/MP3yrB3a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BSteiner1/Music-Gen/blob/main/JSB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e7XmysgEHWMw"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "KmsLaUk-LAuW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/jsb-chorales-quarter.pkl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_n_eIYtLGAV",
        "outputId": "e137db6d-1fe6-489d-d554-f666d88192d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'rb') as p:\n",
        "    data = pickle.load(p, encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "HiR3FSk2K1CO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data['train'] + data['test'] + data['valid']"
      ],
      "metadata": {
        "id": "akdedT3s5YAk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMuIAwpy59C6",
        "outputId": "7f8e8ad3-c820-4a23-f3ca-d5f1fe7273f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "382"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_rests(data):\n",
        "\n",
        "  filled_phrases = []\n",
        "\n",
        "  for phrase in data:\n",
        "    for i in range(len(phrase)):\n",
        "      if len(phrase[i]) == 3:\n",
        "        fill_note = random.randint(65,95)\n",
        "        phrase[i] = phrase[i] + (fill_note,)\n",
        "\n",
        "    filled_phrases.append(phrase)\n",
        "\n",
        "  return filled_phrases"
      ],
      "metadata": {
        "id": "sT36T6iw6J26"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = fill_rests(data)"
      ],
      "metadata": {
        "id": "FSbvPl5X8p7I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(l[22]).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfnIdDdj9BnX",
        "outputId": "04aeaa65-35bb-469b-ea49-ce6ef8960643"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[72, 71, 67, 72, 76, 65, 67, 60, 67, 72, 71, 74, 62, 67, 67, 67,\n",
              "        67, 64, 67, 71, 72, 64, 65, 67, 64, 69, 72, 72, 69, 69, 67, 64,\n",
              "        73, 74, 69, 65, 67, 60, 60, 60, 60],\n",
              "       [76, 74, 79, 76, 79, 77, 74, 72, 71, 79, 74, 81, 74, 74, 74, 74,\n",
              "        74, 76, 74, 74, 79, 72, 72, 79, 71, 72, 76, 77, 77, 76, 69, 71,\n",
              "        76, 77, 81, 81, 79, 76, 76, 76, 76],\n",
              "       [79, 79, 83, 84, 84, 84, 83, 79, 79, 88, 83, 91, 83, 83, 83, 83,\n",
              "        83, 84, 83, 79, 88, 76, 81, 83, 88, 84, 84, 81, 83, 81, 84, 88,\n",
              "        81, 81, 84, 84, 83, 79, 79, 79, 79],\n",
              "       [84, 91, 91, 91, 91, 93, 91, 88, 86, 76, 91, 75, 90, 91, 91, 91,\n",
              "        91, 91, 91, 91, 69, 91, 89, 86, 91, 89, 88, 86, 86, 84, 88, 91,\n",
              "        91, 89, 88, 86, 86, 84, 84, 84, 84]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "for i in range(len(data)):\n",
        "  lengths.append(len(data[i]))\n",
        "\n",
        "max_length = max(lengths)\n",
        "max(lengths)"
      ],
      "metadata": {
        "id": "FnL-wQK9Ny-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cbf755-6fc6-4ccb-c726-0ce059cc2bca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_phrases(data_train):\n",
        "\n",
        "  padded_phrases = []\n",
        "\n",
        "  for phrase in data_train:\n",
        "    phrase_length = len(phrase)\n",
        "    padding_length = max_length - phrase_length\n",
        "\n",
        "    for _ in range(padding_length):\n",
        "      phrase.append((0,0,0,0))\n",
        "\n",
        "    padded_phrases.append(phrase)\n",
        "\n",
        "  return padded_phrases"
      ],
      "metadata": {
        "id": "HHkp2WJBewKm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_phrases = pad_phrases(l)"
      ],
      "metadata": {
        "id": "1wdqN3I0ftLr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(padded_phrases[32]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r4sbshRsvvV",
        "outputId": "3437ddb0-3113-40b2-ad57-5c0bf152be37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def four_part_phrases(padded_phrases):\n",
        "\n",
        "  four_parts = []\n",
        "\n",
        "  for phrase in padded_phrases:\n",
        "      phrase_array = np.array(phrase, dtype=object)\n",
        "      if phrase_array.shape == (160,4):\n",
        "        transposed_phrase = phrase_array.T\n",
        "        #sliced_array = transposed_phrase[:, :25]\n",
        "        four_parts.append(transposed_phrase)\n",
        "\n",
        "\n",
        "  # Stack the list of arrays into a single NumPy array\n",
        "  array_data = np.stack(four_parts)\n",
        "\n",
        "  return four_parts"
      ],
      "metadata": {
        "id": "6vbxm1a3ea6_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = four_part_phrases(padded_phrases)"
      ],
      "metadata": {
        "id": "UiFvblm4px0J"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYkQAT6EtM7v",
        "outputId": "8690a196-c44b-4b85-ff3c-f2ac73d821d7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "350"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_data = torch.tensor(arr, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "PvePDdjDj0be"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3F2yAfxwNe",
        "outputId": "53c88a98-e1bf-44db-f5f2-f1a0dfc6a185"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 160])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1N4u8RktUbo",
        "outputId": "5b3ec432-496c-43ff-89e0-b3761199ce8a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "350"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Generator and Discriminator networks\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.2)\n",
        "\n",
        "        # Add 3 fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fc2 = nn.Linear(80, output_dim)\n",
        "        self.fc3 = nn.Linear(150, output_dim)\n",
        "        self.fc4 = nn.Linear(150, output_dim)\n",
        "\n",
        "        # Sigmoid activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # ReLU activation\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Pass LSTM output through 3 FC layers with ReLU activation\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.sigmoid(out)\n",
        "        #out = self.fc2(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.fc3(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.fc4(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.dropout(out)\n",
        "\n",
        "        # Apply sigmoid activation to squash the values between 0 and 1\n",
        "        # Then scale to the range [0, 128]\n",
        "        out = torch.sigmoid(out) * 128\n",
        "\n",
        "        return out\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.2)\n",
        "\n",
        "        # FC layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fc2 = nn.Linear(80, output_dim)\n",
        "        self.fc3 = nn.Linear(150, output_dim)\n",
        "        self.fc4 = nn.Linear(150, output_dim)\n",
        "\n",
        "        # ReLU activation\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Apply the FC layers and activations to LSTM output\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.dropout(out)\n",
        "        out = self.sigmoid(out)\n",
        "        #out = self.fc2(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.fc3(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.fc4(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        #out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class LSTMGAN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(LSTMGAN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.generator = Generator(input_dim, hidden_dim, input_dim, num_layers)\n",
        "        self.discriminator = Discriminator(input_dim, hidden_dim, 1, num_layers)\n",
        "\n",
        "    def train(self, data_loader, epochs):\n",
        "        self.generator\n",
        "        self.discriminator\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.005)\n",
        "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.005)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            d_loss_sum = 0.0\n",
        "            g_loss_sum = 0.0\n",
        "\n",
        "            for batch in data_loader:\n",
        "                real_data = batch\n",
        "\n",
        "                batch_size = real_data.size(0)\n",
        "                real_labels = torch.ones(batch_size, 1).long()\n",
        "                fake_labels = torch.zeros(batch_size, 1).long()\n",
        "\n",
        "                d_optimizer.zero_grad()\n",
        "                real_labels = torch.ones(batch_size, 1).long()\n",
        "                fake_labels = torch.zeros(batch_size, 1).long()\n",
        "\n",
        "                noise = torch.randn(batch_size, 4, input_dim)\n",
        "                fake_data = self.generator(noise)\n",
        "\n",
        "                real_data = real_data.view(batch_size, 4, input_dim)\n",
        "                real_outputs = self.discriminator(real_data)\n",
        "\n",
        "                fake_outputs = self.discriminator(fake_data.detach())\n",
        "\n",
        "                real_outputs = real_outputs.view(batch_size, -1)\n",
        "                fake_outputs = fake_outputs.view(batch_size, -1)\n",
        "\n",
        "                d_real_loss = criterion(real_outputs, real_labels.float())\n",
        "                d_fake_loss = criterion(fake_outputs, fake_labels.float())\n",
        "\n",
        "                d_loss = d_real_loss + d_fake_loss\n",
        "                d_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 2)\n",
        "                d_optimizer.step()\n",
        "\n",
        "                g_optimizer.zero_grad()\n",
        "                noise = torch.randn(batch_size, 4, input_dim)#.to(device)\n",
        "                fake_data = self.generator(noise)\n",
        "                fake_data = fake_data.view(batch_size, 4, input_dim)\n",
        "                fake_outputs = self.discriminator(fake_data)\n",
        "\n",
        "                fake_outputs = fake_outputs.view(batch_size, -1)\n",
        "\n",
        "                g_loss = criterion(fake_outputs, fake_labels.float())\n",
        "                g_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 2)\n",
        "                g_optimizer.step()\n",
        "\n",
        "                d_loss_sum += d_loss.item()\n",
        "                g_loss_sum += g_loss.item()\n",
        "\n",
        "            # Calculate the average loss for this epoch\n",
        "            avg_d_loss = d_loss_sum / len(data_loader)\n",
        "            avg_g_loss = g_loss_sum / len(data_loader)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, D Loss: {avg_d_loss}, G Loss: {avg_g_loss}\")\n",
        "\n",
        "            # Generate and print an example at the end of each epoch\n",
        "            with torch.no_grad():\n",
        "                example_noise = torch.randn(1, 4, input_dim)\n",
        "                sample = self.generator(example_noise).int()\n",
        "                print(sample)\n"
      ],
      "metadata": {
        "id": "XT3N-fNMts7e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A class to build a dataloader\n",
        "\n",
        "    Args:\n",
        "        - data (list): A list of the arrays\n",
        "\n",
        "    Returns:\n",
        "        - A dataset that can be converted into a PyTorch dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample\n",
        "\n",
        "batch_size = 12\n",
        "dataset = MyDataset(tensor_data)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "RPdRDH5WuiY4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the GAN\n",
        "input_dim = 160\n",
        "# Number of LSTM layers\n",
        "num_layers = 1\n",
        "# Number of neurons in each LSTM layer\n",
        "hidden_dim = 800\n",
        "\n",
        "# Create an instance of the GAN\n",
        "gan = LSTMGAN(input_dim, hidden_dim, num_layers)\n",
        "\n",
        "# Train the GAN using the DataLoader\n",
        "epochs = 3\n",
        "gan.train(dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzE_Dz2HuzYf",
        "outputId": "b037fa11-cd82-4137-dfec-c9e9c9bbb718"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, D Loss: 1.2375133119780442, G Loss: 0.8378636055979235\n",
            "tensor([[[79, 79, 79, 75, 77, 77, 75, 76, 76, 77, 79, 75, 79, 75, 81, 76, 79,\n",
            "          72, 79, 79, 78, 76, 78, 79, 76, 79, 79, 79, 79, 75, 79, 79, 75, 82,\n",
            "          79, 76, 79, 79, 79, 74, 75, 79, 79, 79, 77, 79, 79, 75, 80, 80, 79,\n",
            "          79, 79, 79, 77, 80, 75, 80, 79, 82, 79, 78, 79, 79, 79, 79, 76, 77,\n",
            "          75, 79, 80, 79, 79, 75, 81, 79, 79, 79, 80, 75, 76, 82, 79, 79, 79,\n",
            "          79, 79, 79, 83, 79, 83, 82, 84, 80, 79, 81, 79, 79, 79, 79, 79, 79,\n",
            "          79, 79, 77, 79, 84, 79, 78, 79, 79, 75, 79, 83, 79, 78, 77, 80, 79,\n",
            "          79, 84, 79, 83, 79, 79, 74, 81, 81, 79, 79, 79, 79, 84, 85, 79, 79,\n",
            "          83, 82, 79, 79, 79, 83, 82, 79, 79, 80, 79, 82, 79, 79, 81, 77, 79,\n",
            "          79, 84, 79, 83, 83, 81, 81],\n",
            "         [79, 79, 79, 79, 78, 79, 78, 79, 79, 79, 74, 80, 79, 77, 79, 82, 79,\n",
            "          79, 78, 80, 76, 77, 79, 79, 79, 79, 79, 79, 79, 74, 79, 77, 80, 82,\n",
            "          79, 80, 77, 77, 77, 79, 79, 80, 79, 79, 79, 79, 80, 79, 79, 84, 79,\n",
            "          79, 76, 79, 79, 78, 78, 82, 81, 79, 79, 79, 81, 79, 79, 79, 79, 79,\n",
            "          79, 79, 79, 79, 81, 78, 80, 77, 79, 79, 79, 79, 79, 79, 79, 80, 77,\n",
            "          79, 79, 79, 79, 76, 79, 79, 81, 80, 79, 79, 76, 79, 80, 80, 83, 76,\n",
            "          80, 79, 76, 78, 79, 81, 79, 79, 77, 78, 81, 81, 81, 76, 79, 79, 79,\n",
            "          80, 80, 81, 79, 79, 83, 79, 79, 79, 81, 79, 79, 79, 81, 85, 84, 80,\n",
            "          79, 79, 79, 79, 81, 80, 79, 79, 79, 79, 79, 79, 80, 81, 78, 79, 80,\n",
            "          79, 82, 79, 81, 79, 79, 79],\n",
            "         [79, 71, 79, 79, 68, 72, 71, 72, 70, 69, 69, 79, 79, 79, 79, 79, 73,\n",
            "          79, 79, 69, 79, 79, 72, 79, 79, 79, 73, 79, 79, 79, 79, 71, 70, 88,\n",
            "          79, 77, 78, 79, 79, 79, 79, 79, 79, 79, 79, 86, 79, 79, 79, 79, 70,\n",
            "          79, 79, 79, 78, 79, 70, 87, 72, 88, 79, 80, 75, 79, 70, 74, 79, 69,\n",
            "          70, 82, 84, 79, 87, 79, 86, 82, 79, 79, 79, 79, 79, 79, 74, 79, 74,\n",
            "          79, 79, 79, 79, 85, 79, 86, 82, 81, 85, 79, 83, 87, 86, 74, 86, 79,\n",
            "          79, 89, 72, 79, 87, 79, 79, 79, 67, 74, 79, 79, 90, 79, 79, 79, 88,\n",
            "          79, 79, 79, 88, 79, 79, 71, 79, 79, 88, 79, 82, 89, 79, 79, 90, 79,\n",
            "          79, 79, 88, 79, 79, 87, 79, 88, 79, 79, 82, 79, 89, 83, 81, 79, 79,\n",
            "          79, 79, 76, 89, 79, 84, 82],\n",
            "         [64, 68, 67, 79, 79, 66, 79, 79, 66, 68, 79, 68, 79, 66, 79, 67, 79,\n",
            "          79, 79, 79, 67, 68, 66, 67, 79, 89, 68, 66, 79, 79, 70, 79, 65, 89,\n",
            "          79, 79, 80, 70, 67, 79, 79, 79, 79, 88, 65, 79, 74, 68, 79, 79, 79,\n",
            "          69, 68, 79, 81, 79, 79, 84, 79, 91, 79, 79, 68, 89, 79, 69, 76, 79,\n",
            "          66, 88, 79, 84, 91, 65, 79, 82, 66, 85, 79, 66, 68, 79, 67, 85, 68,\n",
            "          79, 79, 71, 79, 90, 90, 90, 79, 79, 92, 87, 91, 79, 79, 79, 89, 73,\n",
            "          79, 79, 79, 79, 79, 79, 77, 79, 79, 67, 91, 79, 92, 79, 79, 91, 79,\n",
            "          74, 92, 79, 91, 79, 79, 79, 79, 79, 90, 79, 84, 79, 88, 79, 92, 89,\n",
            "          79, 79, 92, 79, 79, 79, 90, 79, 79, 79, 79, 88, 92, 79, 79, 79, 92,\n",
            "          79, 92, 78, 79, 79, 79, 87]]], dtype=torch.int32)\n",
            "Epoch 2/3, D Loss: 1.2247332827798252, G Loss: 0.8304371155541519\n",
            "tensor([[[79, 66, 65, 79, 66, 79, 65, 68, 79, 79, 79, 79, 69, 65, 79, 79, 79,\n",
            "          64, 79, 79, 67, 79, 79, 65, 79, 87, 69, 69, 79, 79, 64, 70, 79, 79,\n",
            "          71, 71, 79, 71, 79, 79, 79, 79, 79, 91, 79, 91, 68, 66, 82, 79, 64,\n",
            "          76, 69, 79, 79, 72, 67, 90, 68, 79, 79, 79, 73, 79, 79, 79, 67, 79,\n",
            "          64, 81, 79, 84, 92, 66, 85, 83, 67, 79, 79, 66, 72, 88, 68, 79, 75,\n",
            "          79, 92, 71, 92, 79, 91, 79, 91, 79, 79, 79, 90, 88, 92, 66, 79, 79,\n",
            "          79, 79, 79, 88, 91, 70, 86, 89, 66, 67, 79, 79, 79, 88, 91, 87, 91,\n",
            "          79, 79, 91, 79, 79, 92, 68, 79, 79, 91, 88, 79, 92, 89, 79, 92, 79,\n",
            "          90, 88, 92, 79, 79, 79, 79, 92, 89, 75, 90, 91, 92, 85, 79, 79, 79,\n",
            "          89, 79, 79, 79, 89, 85, 89],\n",
            "         [64, 64, 79, 64, 64, 79, 79, 79, 79, 64, 64, 67, 64, 64, 79, 79, 79,\n",
            "          79, 79, 64, 64, 64, 79, 79, 79, 79, 65, 64, 79, 79, 79, 64, 64, 92,\n",
            "          79, 65, 79, 65, 64, 64, 64, 68, 79, 93, 79, 92, 65, 79, 79, 79, 64,\n",
            "          79, 79, 64, 68, 68, 79, 89, 79, 93, 79, 79, 79, 79, 79, 65, 79, 64,\n",
            "          64, 86, 79, 89, 79, 79, 91, 88, 79, 79, 79, 64, 67, 79, 64, 79, 79,\n",
            "          93, 93, 79, 93, 79, 79, 79, 79, 79, 93, 79, 79, 79, 93, 67, 91, 84,\n",
            "          90, 79, 79, 79, 93, 67, 89, 79, 64, 64, 93, 79, 79, 79, 79, 79, 93,\n",
            "          72, 93, 79, 79, 91, 79, 79, 76, 79, 93, 79, 79, 79, 92, 79, 93, 79,\n",
            "          79, 92, 93, 79, 92, 79, 79, 79, 79, 81, 90, 92, 93, 79, 79, 67, 79,\n",
            "          93, 93, 79, 79, 92, 79, 92],\n",
            "         [79, 79, 79, 79, 64, 64, 64, 64, 79, 64, 79, 64, 64, 64, 79, 64, 79,\n",
            "          79, 79, 64, 79, 64, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 64, 79,\n",
            "          79, 79, 79, 64, 64, 79, 79, 79, 64, 79, 64, 79, 79, 79, 79, 79, 79,\n",
            "          64, 64, 64, 66, 64, 64, 92, 64, 93, 79, 90, 65, 92, 64, 64, 79, 79,\n",
            "          79, 91, 88, 91, 93, 79, 79, 90, 64, 79, 93, 79, 64, 93, 64, 93, 64,\n",
            "          93, 93, 65, 79, 93, 79, 79, 79, 91, 79, 79, 79, 93, 79, 64, 79, 79,\n",
            "          93, 79, 64, 93, 93, 64, 91, 93, 79, 64, 93, 93, 79, 79, 79, 79, 79,\n",
            "          67, 79, 79, 79, 92, 79, 64, 79, 90, 79, 93, 79, 93, 93, 79, 93, 93,\n",
            "          93, 93, 79, 79, 79, 79, 93, 79, 93, 79, 93, 93, 79, 79, 79, 64, 93,\n",
            "          79, 93, 79, 93, 93, 79, 93],\n",
            "         [64, 79, 79, 79, 79, 64, 79, 64, 64, 79, 79, 64, 79, 64, 79, 79, 79,\n",
            "          64, 64, 79, 79, 79, 64, 64, 64, 79, 64, 64, 64, 79, 64, 79, 79, 93,\n",
            "          79, 79, 79, 79, 79, 79, 79, 65, 64, 79, 79, 79, 79, 79, 79, 90, 79,\n",
            "          64, 79, 79, 79, 64, 64, 93, 79, 93, 66, 79, 79, 93, 64, 79, 79, 64,\n",
            "          79, 79, 79, 79, 79, 79, 93, 79, 64, 79, 79, 64, 79, 79, 79, 79, 64,\n",
            "          79, 79, 64, 79, 79, 93, 93, 79, 92, 79, 93, 79, 93, 93, 64, 79, 91,\n",
            "          79, 79, 79, 93, 93, 64, 79, 93, 79, 79, 79, 79, 79, 79, 93, 79, 79,\n",
            "          66, 79, 79, 93, 79, 79, 79, 76, 79, 79, 93, 79, 79, 79, 93, 93, 93,\n",
            "          93, 93, 93, 79, 93, 79, 79, 93, 79, 79, 79, 79, 79, 79, 93, 64, 93,\n",
            "          93, 93, 82, 93, 79, 93, 93]]], dtype=torch.int32)\n",
            "Epoch 3/3, D Loss: 1.225121440558598, G Loss: 0.8384651007323429\n",
            "tensor([[[64, 66, 68, 79, 79, 69, 79, 79, 67, 79, 68, 69, 79, 67, 79, 67, 79,\n",
            "          64, 79, 70, 69, 79, 67, 71, 79, 79, 69, 79, 72, 65, 79, 67, 79, 88,\n",
            "          79, 67, 79, 79, 67, 69, 66, 79, 79, 90, 66, 88, 79, 79, 79, 83, 66,\n",
            "          76, 79, 79, 79, 74, 67, 79, 76, 79, 79, 77, 79, 83, 79, 71, 79, 65,\n",
            "          79, 79, 79, 84, 90, 66, 79, 82, 68, 79, 79, 79, 70, 79, 66, 85, 79,\n",
            "          79, 90, 79, 79, 88, 91, 86, 89, 79, 79, 87, 90, 79, 79, 79, 90, 79,\n",
            "          79, 79, 71, 79, 79, 79, 83, 88, 66, 79, 91, 79, 91, 81, 79, 88, 79,\n",
            "          79, 79, 79, 79, 82, 91, 79, 77, 79, 87, 79, 86, 79, 87, 79, 92, 88,\n",
            "          91, 79, 91, 87, 89, 91, 79, 91, 79, 82, 83, 79, 92, 79, 84, 73, 91,\n",
            "          91, 91, 79, 79, 90, 79, 79],\n",
            "         [64, 64, 79, 65, 79, 64, 79, 64, 64, 79, 79, 79, 79, 64, 85, 79, 79,\n",
            "          79, 68, 79, 79, 65, 64, 79, 64, 91, 64, 79, 79, 64, 64, 79, 64, 91,\n",
            "          79, 79, 69, 65, 79, 79, 79, 66, 64, 93, 79, 93, 66, 79, 78, 79, 64,\n",
            "          79, 79, 79, 79, 69, 79, 79, 66, 92, 79, 79, 79, 88, 79, 65, 65, 79,\n",
            "          79, 87, 80, 87, 79, 79, 90, 79, 79, 90, 79, 79, 67, 91, 64, 90, 79,\n",
            "          93, 93, 69, 79, 93, 79, 79, 93, 79, 79, 79, 79, 79, 79, 65, 92, 87,\n",
            "          79, 93, 79, 92, 93, 66, 90, 79, 79, 64, 79, 92, 93, 91, 91, 90, 79,\n",
            "          79, 93, 92, 79, 79, 79, 64, 79, 79, 92, 92, 79, 79, 79, 79, 79, 90,\n",
            "          93, 79, 93, 93, 92, 79, 79, 79, 93, 79, 79, 93, 93, 79, 79, 65, 93,\n",
            "          93, 93, 79, 79, 79, 90, 79],\n",
            "         [79, 64, 79, 64, 64, 79, 79, 79, 64, 79, 79, 64, 79, 79, 88, 64, 64,\n",
            "          64, 64, 64, 64, 79, 79, 79, 79, 93, 79, 64, 79, 79, 79, 79, 64, 93,\n",
            "          64, 64, 79, 64, 79, 79, 79, 64, 64, 93, 79, 79, 64, 79, 79, 82, 79,\n",
            "          79, 64, 79, 79, 64, 64, 79, 79, 93, 65, 87, 66, 79, 64, 64, 64, 64,\n",
            "          79, 79, 79, 89, 93, 64, 93, 79, 79, 92, 93, 79, 65, 93, 64, 93, 79,\n",
            "          79, 93, 79, 93, 79, 93, 93, 79, 87, 93, 79, 79, 93, 93, 79, 93, 92,\n",
            "          79, 79, 79, 93, 93, 79, 92, 79, 64, 64, 93, 79, 93, 93, 93, 93, 93,\n",
            "          66, 93, 79, 93, 79, 93, 64, 76, 79, 79, 93, 79, 79, 93, 93, 79, 79,\n",
            "          79, 79, 79, 93, 79, 93, 79, 93, 93, 72, 79, 79, 79, 92, 93, 64, 93,\n",
            "          93, 93, 80, 79, 93, 93, 79],\n",
            "         [64, 64, 64, 79, 79, 79, 79, 64, 79, 64, 64, 64, 64, 64, 91, 64, 79,\n",
            "          79, 79, 79, 79, 64, 64, 79, 79, 93, 64, 64, 79, 79, 64, 64, 79, 79,\n",
            "          64, 64, 79, 79, 64, 79, 79, 79, 79, 79, 79, 93, 64, 64, 79, 79, 79,\n",
            "          79, 79, 64, 66, 64, 79, 79, 79, 93, 79, 79, 64, 79, 64, 79, 79, 79,\n",
            "          64, 79, 87, 92, 79, 64, 93, 89, 79, 93, 79, 64, 64, 93, 79, 93, 64,\n",
            "          93, 93, 64, 79, 79, 79, 93, 93, 91, 79, 93, 93, 93, 93, 64, 79, 79,\n",
            "          79, 79, 79, 93, 93, 79, 79, 79, 64, 64, 93, 93, 93, 79, 79, 79, 93,\n",
            "          79, 79, 79, 93, 93, 79, 64, 79, 92, 79, 79, 93, 93, 93, 93, 79, 79,\n",
            "          93, 79, 93, 79, 79, 93, 79, 79, 79, 79, 93, 93, 93, 93, 93, 64, 93,\n",
            "          93, 79, 81, 79, 93, 93, 93]]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# Create a copy of the generator from your GAN model\n",
        "generator_copy = deepcopy(gan.generator)\n",
        "\n",
        "# Load the trained weights into the copy\n",
        "generator_copy.load_state_dict(gan.generator.state_dict())\n",
        "\n",
        "# Set the generator copy to evaluation mode\n",
        "generator_copy.eval()\n",
        "\n",
        "# Generate a single noise vector\n",
        "noise = torch.randn(1, 4, 160)\n",
        "\n",
        "# Generate a single sample using the generator copy\n",
        "with torch.no_grad():\n",
        "    generated_sample = generator_copy(noise)\n",
        "\n",
        "# Post-process or visualize the generated sample as needed"
      ],
      "metadata": {
        "id": "yeYK0AzXu1L_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sample = torch.round(generated_sample).int()\n",
        "generated_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fc_7HRIy9Fu",
        "outputId": "fb191710-f5df-4821-bcf1-f87457599d7d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[80, 77, 80, 80, 77, 79, 79, 78, 77, 81, 79, 78, 78, 81, 83, 76, 81,\n",
              "          77, 82, 79, 81, 79, 78, 82, 78, 81, 81, 81, 81, 79, 83, 79, 79, 81,\n",
              "          80, 79, 83, 82, 80, 78, 80, 77, 78, 80, 81, 80, 81, 76, 78, 78, 80,\n",
              "          81, 77, 80, 83, 81, 77, 80, 78, 82, 78, 80, 78, 82, 80, 78, 80, 79,\n",
              "          79, 83, 81, 83, 82, 78, 81, 78, 79, 81, 80, 79, 76, 83, 79, 78, 76,\n",
              "          82, 81, 77, 79, 80, 82, 80, 82, 79, 80, 81, 82, 78, 80, 78, 81, 79,\n",
              "          79, 80, 78, 82, 79, 77, 77, 82, 80, 79, 81, 78, 82, 78, 76, 80, 81,\n",
              "          77, 81, 82, 78, 77, 81, 80, 78, 83, 78, 77, 79, 82, 80, 84, 80, 81,\n",
              "          81, 80, 81, 80, 81, 82, 82, 79, 82, 79, 80, 80, 81, 82, 81, 79, 80,\n",
              "          80, 81, 76, 81, 82, 79, 82],\n",
              "         [73, 75, 72, 77, 71, 72, 72, 74, 71, 73, 73, 75, 75, 74, 80, 71, 74,\n",
              "          75, 75, 71, 74, 77, 72, 72, 71, 82, 74, 76, 72, 72, 75, 74, 74, 83,\n",
              "          77, 77, 78, 75, 74, 75, 73, 77, 71, 81, 74, 85, 76, 73, 81, 81, 72,\n",
              "          78, 75, 73, 82, 76, 73, 83, 73, 83, 78, 85, 77, 79, 75, 76, 79, 73,\n",
              "          73, 82, 83, 79, 87, 71, 83, 81, 74, 84, 85, 72, 75, 84, 76, 82, 72,\n",
              "          86, 86, 75, 85, 87, 86, 87, 85, 82, 86, 84, 86, 81, 84, 75, 85, 83,\n",
              "          84, 85, 76, 84, 84, 72, 80, 85, 74, 76, 84, 82, 85, 84, 82, 81, 86,\n",
              "          74, 86, 85, 85, 83, 87, 76, 78, 84, 85, 84, 80, 87, 83, 87, 88, 84,\n",
              "          86, 84, 85, 87, 84, 87, 85, 85, 86, 80, 83, 87, 85, 85, 86, 76, 86,\n",
              "          85, 87, 78, 87, 87, 84, 82],\n",
              "         [67, 71, 69, 73, 68, 68, 67, 71, 68, 69, 69, 71, 73, 70, 82, 67, 69,\n",
              "          70, 72, 68, 71, 72, 70, 70, 67, 85, 68, 70, 68, 69, 69, 71, 70, 84,\n",
              "          72, 72, 76, 73, 68, 72, 69, 75, 68, 86, 69, 88, 75, 70, 83, 80, 68,\n",
              "          75, 74, 70, 82, 73, 71, 85, 71, 86, 75, 85, 75, 82, 72, 72, 75, 69,\n",
              "          68, 84, 86, 82, 89, 68, 87, 83, 70, 87, 89, 68, 73, 86, 71, 85, 71,\n",
              "          88, 90, 73, 88, 91, 89, 90, 89, 82, 91, 86, 89, 82, 86, 72, 88, 84,\n",
              "          85, 89, 74, 86, 89, 71, 83, 86, 70, 71, 86, 86, 87, 86, 86, 84, 90,\n",
              "          73, 89, 86, 89, 86, 91, 73, 75, 84, 88, 89, 83, 90, 88, 90, 92, 87,\n",
              "          90, 87, 89, 89, 85, 91, 89, 90, 89, 80, 86, 90, 90, 88, 89, 74, 90,\n",
              "          89, 90, 81, 91, 90, 87, 85],\n",
              "         [64, 65, 65, 66, 65, 65, 64, 66, 65, 65, 65, 68, 67, 65, 83, 65, 66,\n",
              "          65, 68, 65, 65, 68, 66, 65, 64, 89, 65, 66, 65, 64, 65, 66, 65, 90,\n",
              "          69, 68, 73, 67, 65, 67, 65, 72, 65, 90, 65, 92, 69, 65, 84, 83, 64,\n",
              "          71, 67, 65, 79, 68, 65, 89, 66, 92, 73, 87, 70, 86, 66, 67, 71, 65,\n",
              "          65, 86, 87, 85, 93, 65, 90, 85, 66, 90, 92, 65, 69, 90, 67, 90, 68,\n",
              "          93, 93, 70, 93, 93, 93, 93, 92, 85, 93, 91, 92, 88, 92, 68, 92, 88,\n",
              "          90, 93, 68, 91, 93, 67, 87, 92, 65, 67, 92, 91, 93, 90, 91, 91, 93,\n",
              "          70, 93, 92, 93, 90, 93, 66, 76, 88, 93, 92, 88, 93, 92, 93, 93, 91,\n",
              "          93, 92, 93, 93, 91, 93, 92, 93, 92, 80, 89, 93, 93, 91, 91, 69, 93,\n",
              "          93, 93, 80, 93, 92, 91, 89]]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Qeuwd0DzGvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}