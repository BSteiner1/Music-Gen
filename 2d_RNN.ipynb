{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORg2KzNz27695rKwzs+eOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BSteiner1/Music-Gen/blob/main/2d_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QXBFUMaLWyf8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "_m7Uv0PpWy-U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/jsb-chorales-quarter.pkl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQQOpgQvW0NF",
        "outputId": "93f07541-e16d-4771-d031-f7f7230f38a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'rb') as p:\n",
        "    data = pickle.load(p, encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "OYjfO8SyW1TY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data['train'] + data['test'] + data['valid']"
      ],
      "metadata": {
        "id": "rhIFEt6lW53K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qBmpPErW7Kg",
        "outputId": "572513a0-e794-4209-e82f-5f10269cae86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "382"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[55] = data[55][2:]"
      ],
      "metadata": {
        "id": "V_-DYmY0f4EG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_rests(data):\n",
        "\n",
        "  filled_phrases = []\n",
        "\n",
        "  for phrase in data:\n",
        "    for i in range(len(phrase)):\n",
        "      if len(phrase[i]) == 3:\n",
        "        fill_note = random.randint(65,95)\n",
        "        phrase[i] = phrase[i] + (fill_note,)\n",
        "\n",
        "    filled_phrases.append(phrase)\n",
        "\n",
        "  return filled_phrases"
      ],
      "metadata": {
        "id": "VdBPAz5jW8M9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = fill_rests(data)"
      ],
      "metadata": {
        "id": "Q9LMVTRyW9e4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "for i in range(len(data)):\n",
        "  lengths.append(len(data[i]))\n",
        "\n",
        "max_length = max(lengths)\n",
        "max(lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWKiy8-JW-7r",
        "outputId": "229f5737-1c39-47e3-d187-c008ad1d7d49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def phrases_to_arrays(data):\n",
        "\n",
        "  array_data = []\n",
        "\n",
        "  for phrase in data:\n",
        "    transposed_phrase = np.array(phrase).T\n",
        "    array_data.append(transposed_phrase)\n",
        "\n",
        "  return array_data"
      ],
      "metadata": {
        "id": "Lf4sri5DW_69"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_data = phrases_to_arrays(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopPzmgEXA3b",
        "outputId": "c90323b6-f296-409d-bf71-4a5fd5f29630"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-da10f5cb17ab>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  transposed_phrase = np.array(phrase).T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array_data[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0CW-16TbNQ-",
        "outputId": "958b7647-ccf8-4c15-cff1-c8fb3d391b14"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[60, 60, 62, 62, 63, 62, 60, 59, 60, 62, 63, 60, 56, 53, 58, 58,\n",
              "        51, 51, 51, 51, 51, 53, 55, 56, 58, 58, 57, 57, 58, 58, 56, 56,\n",
              "        55, 55, 55, 55, 48, 58, 56, 53, 48, 48, 48, 48],\n",
              "       [63, 63, 65, 65, 67, 65, 63, 62, 63, 65, 65, 63, 63, 63, 62, 62,\n",
              "        63, 63, 63, 63, 63, 63, 63, 63, 62, 65, 63, 72, 70, 70, 65, 65,\n",
              "        67, 67, 65, 65, 64, 64, 65, 65, 65, 65, 64, 64],\n",
              "       [72, 72, 70, 70, 70, 71, 72, 74, 72, 70, 70, 72, 72, 72, 70, 70,\n",
              "        70, 70, 70, 70, 70, 70, 70, 70, 70, 74, 72, 75, 74, 74, 72, 72,\n",
              "        72, 72, 72, 71, 67, 67, 68, 68, 68, 68, 67, 67],\n",
              "       [79, 79, 82, 82, 79, 79, 79, 79, 80, 80, 79, 79, 77, 77, 77, 77,\n",
              "        75, 75, 75, 75, 79, 79, 82, 82, 77, 77, 77, 77, 77, 77, 66, 74,\n",
              "        75, 75, 74, 74, 72, 72, 72, 72, 72, 72, 72, 72]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def melody_and_bass(array_data):\n",
        "\n",
        "  two_part_phrases = []\n",
        "\n",
        "  for phrase in array_data:\n",
        "    if len(phrase) == 4:\n",
        "      melody = phrase[3]\n",
        "      bass = phrase[0]\n",
        "      new_phrase = np.vstack((bass, melody))\n",
        "      two_part_phrases.append(new_phrase)\n",
        "\n",
        "  return two_part_phrases"
      ],
      "metadata": {
        "id": "uIXEpinabJsc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_parts = melody_and_bass(array_data)"
      ],
      "metadata": {
        "id": "mWarmRHuZmlM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greatest_multiple(melodies):\n",
        "\n",
        "  sliced_melodies = []\n",
        "\n",
        "  for melody in melodies:\n",
        "    seq_length = len(melody[0])\n",
        "    remainder = seq_length % 4\n",
        "    max_length = seq_length - remainder\n",
        "    sliced_melody = melody[:max_length]\n",
        "    sliced_melodies.append(sliced_melody)\n",
        "\n",
        "  return sliced_melodies\n"
      ],
      "metadata": {
        "id": "PhiieUECXC3Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sliced_melodies = greatest_multiple(two_parts)"
      ],
      "metadata": {
        "id": "XFlF8v9_XD3Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_transpositions(sliced_melodies):\n",
        "\n",
        "  all_transposed = []\n",
        "  count = 0\n",
        "\n",
        "  for melody in sliced_melodies:\n",
        "    if melody != ():\n",
        "      diff = melody[0][0] - 60\n",
        "      melody = melody - diff\n",
        "      for i in range(12):\n",
        "        transposed = melody + i\n",
        "        all_transposed.append(transposed)\n",
        "\n",
        "  return all_transposed"
      ],
      "metadata": {
        "id": "v69yeNQEXFA2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transposed = all_transpositions(sliced_melodies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WecGocO5XGAf",
        "outputId": "81e6ba2e-01d5-42c3-982b-962e12399071"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-815b7824cb2a>:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if melody != ():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seq_and_label(transposed_data, input_length, output_length):\n",
        "\n",
        "  input_data = []\n",
        "  labels = []\n",
        "\n",
        "  for melody in sliced_melodies:\n",
        "    #print(melody)\n",
        "\n",
        "    # Slide the 16-note window every 4 notes\n",
        "    for i in range(0, len(melody[0]) - (input_length + output_length -1), 4): # input length + output length - 1\n",
        "        input_segment = melody[:, i: i+input_length] # input length\n",
        "        label_segment = melody[:, i+input_length: i+ (input_length+output_length)] # input, input + output\n",
        "\n",
        "        input_data.append(input_segment)\n",
        "        labels.append(label_segment)\n",
        "\n",
        "  return input_data, labels"
      ],
      "metadata": {
        "id": "2nbW-ZQuXH4Y"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_length, output_length = 16, 8"
      ],
      "metadata": {
        "id": "89oRl0QHQjmy"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences, labels = get_seq_and_label(transposed, input_length, output_length)"
      ],
      "metadata": {
        "id": "yrk-SGbCXJJm"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSXwO7hwXKkT",
        "outputId": "9ca823f4-a448-4797-c37e-f4b38e0a6ed8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3386"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input data and labels to NumPy arrays\n",
        "seq_arr = np.array(sequences)\n",
        "labels_arr = np.array(labels)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "seq_tensors = torch.from_numpy(seq_arr)\n",
        "labels_tensors = torch.from_numpy(labels_arr)\n",
        "\n",
        "# Create a list of tuples, each containing the input and label tensors\n",
        "tensor_tuples = [(seq_tensors[i], labels_tensors[i]) for i in range(len(seq_tensors))]"
      ],
      "metadata": {
        "id": "ZxIN_CtLXLfY"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_tuples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmWUrMp3wTdO",
        "outputId": "6a972e65-859a-4999-8c3e-42e1a2a71434"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[60, 72, 67, 69, 67, 65, 65, 58, 69, 68, 69, 66, 67, 60, 60, 60],\n",
              "         [88, 68, 84, 76, 88, 89, 89, 86, 86, 86, 84, 84, 83, 84, 84, 84]]),\n",
              " tensor([[67, 66, 62, 67, 64, 69, 69, 69],\n",
              "         [86, 86, 86, 86, 88, 84, 84, 84]]))"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "music_dataset = MusicDataset(tensor_tuples)\n",
        "\n",
        "# Create a DataLoader to iterate through your data in batches\n",
        "batch_size = 256\n",
        "dataloader = DataLoader(music_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ImQJec2eXMof"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, temp):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.temperature = temp\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout = 0.9)\n",
        "        self.fc1 = nn.Linear(hidden_size, 2*output_size)\n",
        "        self.fc2 = nn.Linear(100, 40)\n",
        "        self.fc3 = nn.Linear(40, 2*output_size)\n",
        "        self.fc4 = nn.Linear(120, 2*output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, output_size):\n",
        "        batch_size, seq_length, input_features = x.size()\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Apply FC layers\n",
        "        out = self.fc1(out[:, -1, :])  # First FC layer\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.relu(out)\n",
        "        #out = self.fc2(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.relu(out)\n",
        "        #out = self.fc3(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.relu(out)\n",
        "        #out = self.fc4(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.sigmoid(out)\n",
        "\n",
        "        # Reshape the output to match the desired shape\n",
        "        out_size = (batch_size, 2, output_size)\n",
        "        out = out.view(out_size)\n",
        "\n",
        "        # Adjust logits with temperature\n",
        "        adjusted_logits = out / 3\n",
        "\n",
        "        return adjusted_logits * 128"
      ],
      "metadata": {
        "id": "LSZ-k-S1XNrq"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and training parameters\n",
        "input_size = input_length\n",
        "output_size = output_length\n",
        "\n",
        "hidden_size = 30\n",
        "num_layers = 3\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 500\n",
        "temperature = 0.8"
      ],
      "metadata": {
        "id": "KatrTofRXPjL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(input_size, hidden_size, output_size, num_layers, temperature)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.95)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float(), output_size)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()  # Update the learning rate\n",
        "\n",
        "    # Print the current learning rate (optional)\n",
        "    current_lr = np.round(optimizer.param_groups[0]['lr'], 5)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Learning Rate: {current_lr}')\n",
        "\n",
        "    random_int = random.randint(0, len(sequences)-1)\n",
        "    input_sequence = torch.tensor([sequences[random_int]], dtype=torch.float32)\n",
        "\n",
        "    # Generate a prediction for the single example\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_sequence, output_size)\n",
        "\n",
        "    predicted_notes = prediction.squeeze().tolist()\n",
        "    bass_prediction = predicted_notes[0]\n",
        "    melody_prediction = predicted_notes[1]\n",
        "\n",
        "    int_bass_pred = [int(round(note)) for note in bass_prediction]\n",
        "    int_melody_pred = [int(round(note)) for note in melody_prediction]\n",
        "\n",
        "    melody_input = [int(note) for note in input_sequence.tolist()[0][1]]\n",
        "    bass_input = [int(note) for note in input_sequence.tolist()[0][0]]\n",
        "\n",
        "    full_melody = melody_input + int_melody_pred\n",
        "    full_bass = bass_input + int_bass_pred\n",
        "    print(full_melody)\n",
        "    print(full_bass)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIgerECmXRIy",
        "outputId": "cf1aa52f-d606-40fe-ae5e-c243e9fee70a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 5412.568359375, Learning Rate: 0.0002\n",
            "[93, 91, 91, 89, 89, 88, 84, 86, 86, 88, 89, 91, 88, 86, 86, 84, 8, -5, -6, 4, 3, -1, 6, -1]\n",
            "[65, 65, 65, 62, 62, 60, 72, 71, 67, 72, 69, 67, 69, 65, 67, 60, 0, -8, -5, 0, 0, 3, 9, 4]\n",
            "Epoch [2/500], Loss: 5425.3525390625, Learning Rate: 0.0002\n",
            "[80, 79, 79, 79, 79, 77, 79, 82, 82, 83, 80, 80, 79, 94, 89, 77, 8, -3, -6, 4, 3, 0, 6, -1]\n",
            "[58, 51, 51, 51, 60, 72, 70, 67, 56, 60, 60, 63, 67, 70, 70, 58, 1, -8, -4, -1, 1, 4, 9, 5]\n",
            "Epoch [3/500], Loss: 5305.7177734375, Learning Rate: 0.0002\n",
            "[81, 79, 77, 76, 68, 81, 81, 79, 84, 83, 81, 79, 79, 84, 79, 81, 8, 2, -10, 6, 4, 1, 9, -3]\n",
            "[65, 60, 62, 64, 69, 65, 62, 59, 59, 55, 60, 55, 60, 57, 60, 53, 6, -8, -4, -4, 5, 7, 11, 8]\n",
            "Epoch [4/500], Loss: 5189.5810546875, Learning Rate: 0.00019\n",
            "[84, 87, 86, 86, 84, 84, 84, 84, 79, 79, 80, 79, 77, 77, 75, 75, 10, -4, -5, 6, 1, 2, 7, 1]\n",
            "[63, 60, 67, 55, 60, 60, 60, 60, 60, 63, 56, 60, 57, 58, 51, 51, 3, -7, -4, 0, 2, 6, 10, 7]\n",
            "Epoch [5/500], Loss: 5142.06982421875, Learning Rate: 0.00019\n",
            "[86, 74, 87, 86, 86, 84, 84, 84, 87, 86, 82, 84, 86, 87, 87, 87, 10, -3, -6, 5, 4, 1, 7, 0]\n",
            "[71, 72, 68, 65, 67, 60, 60, 60, 72, 65, 67, 68, 65, 63, 63, 63, 2, -6, -3, 0, 2, 5, 10, 6]\n",
            "Epoch [6/500], Loss: 5050.56982421875, Learning Rate: 0.00019\n",
            "[84, 84, 88, 89, 67, 93, 91, 91, 93, 95, 96, 91, 88, 91, 91, 91, 14, -2, -3, 9, 3, 2, 9, 3]\n",
            "[72, 65, 60, 74, 76, 72, 67, 67, 74, 67, 69, 71, 72, 74, 67, 67, 3, -4, -3, 1, 3, 8, 12, 9]\n",
            "Epoch [7/500], Loss: 5057.97607421875, Learning Rate: 0.00019\n",
            "[84, 83, 81, 79, 79, 81, 83, 84, 86, 84, 83, 81, 83, 81, 81, 79, 13, 2, -3, 7, 5, 0, 8, 0]\n",
            "[60, 67, 62, 55, 60, 65, 62, 60, 55, 57, 59, 62, 67, 62, 64, 55, 2, -3, -3, 1, 5, 8, 11, 10]\n",
            "Epoch [8/500], Loss: 4825.720703125, Learning Rate: 0.00018\n",
            "[87, 86, 82, 84, 86, 87, 87, 87, 82, 84, 82, 80, 80, 81, 88, 86, 16, 2, -7, 5, 11, 2, 11, 3]\n",
            "[72, 65, 67, 68, 65, 63, 63, 63, 63, 68, 65, 65, 68, 72, 72, 72, 2, -3, 2, 0, 4, 10, 14, 6]\n",
            "Epoch [9/500], Loss: 4766.50830078125, Learning Rate: 0.00018\n",
            "[89, 88, 86, 86, 88, 89, 91, 88, 86, 86, 84, 84, 86, 88, 89, 89, 16, -1, -5, 7, 6, 4, 10, 4]\n",
            "[62, 64, 67, 67, 72, 69, 71, 72, 65, 67, 60, 60, 67, 65, 62, 65, 5, -2, 1, 4, 4, 11, 14, 10]\n",
            "Epoch [10/500], Loss: 4477.125, Learning Rate: 0.00018\n",
            "[89, 91, 91, 91, 91, 91, 89, 87, 89, 86, 86, 86, 86, 87, 89, 79, 15, 0, -2, 9, 5, 4, 10, 3]\n",
            "[68, 67, 67, 67, 63, 67, 71, 72, 65, 67, 67, 67, 67, 72, 74, 75, 4, -3, 0, 3, 4, 10, 14, 10]\n",
            "Epoch [11/500], Loss: 4383.84033203125, Learning Rate: 0.00018\n",
            "[79, 79, 84, 86, 88, 89, 88, 81, 84, 84, 84, 84, 88, 89, 91, 89, 36, 4, 4, 22, 8, 10, 17, 18]\n",
            "[67, 67, 69, 71, 72, 72, 76, 79, 72, 72, 72, 72, 60, 62, 64, 65, 11, 12, 14, 12, 9, 27, 30, 23]\n",
            "Epoch [12/500], Loss: 4089.572509765625, Learning Rate: 0.00017\n",
            "[91, 75, 77, 79, 79, 75, 77, 79, 75, 74, 72, 74, 74, 70, 72, 72, 22, 4, 0, 10, 9, 10, 12, 7]\n",
            "[55, 55, 55, 51, 59, 60, 62, 63, 60, 60, 60, 55, 55, 52, 52, 53, 7, 0, 5, 5, 6, 16, 19, 14]\n",
            "Epoch [13/500], Loss: 3802.275390625, Learning Rate: 0.00017\n",
            "[84, 84, 84, 84, 82, 80, 79, 79, 77, 76, 77, 79, 82, 80, 79, 79, 24, 7, 2, 13, 10, 10, 16, 8]\n",
            "[60, 64, 65, 68, 62, 65, 60, 60, 61, 60, 56, 55, 53, 53, 60, 60, 11, 2, 7, 8, 10, 19, 21, 16]\n",
            "Epoch [14/500], Loss: 3376.18798828125, Learning Rate: 0.00017\n",
            "[82, 81, 73, 81, 81, 79, 79, 79, 81, 82, 81, 82, 79, 77, 75, 72, 37, 20, 13, 27, 9, 20, 21, 15]\n",
            "[67, 60, 63, 60, 62, 55, 55, 55, 65, 62, 65, 62, 63, 68, 67, 60, 15, 9, 15, 11, 15, 31, 31, 26]\n",
            "Epoch [15/500], Loss: 3214.739501953125, Learning Rate: 0.00017\n",
            "[84, 84, 84, 84, 79, 79, 77, 75, 74, 74, 72, 72, 79, 81, 82, 65, 34, 11, 6, 22, 12, 16, 19, 16]\n",
            "[60, 60, 60, 60, 60, 58, 56, 53, 53, 55, 48, 48, 60, 65, 62, 63, 15, 9, 13, 15, 14, 27, 30, 23]\n",
            "Epoch [16/500], Loss: 2768.06591796875, Learning Rate: 0.00016\n",
            "[86, 84, 84, 84, 84, 86, 88, 84, 84, 86, 88, 84, 81, 86, 86, 83, 38, 14, 7, 25, 13, 19, 18, 16]\n",
            "[67, 60, 60, 60, 72, 67, 60, 65, 64, 66, 68, 69, 69, 66, 62, 67, 20, 10, 13, 16, 15, 31, 28, 29]\n",
            "Epoch [17/500], Loss: 2422.838134765625, Learning Rate: 0.00016\n",
            "[86, 94, 94, 90, 90, 91, 91, 91, 94, 91, 89, 88, 88, 89, 94, 96, 54, 27, 20, 36, 22, 17, 29, 26]\n",
            "[66, 67, 72, 74, 62, 67, 67, 67, 67, 70, 73, 72, 72, 74, 75, 77, 23, 20, 30, 24, 28, 50, 43, 37]\n",
            "Epoch [18/500], Loss: 1966.7613525390625, Learning Rate: 0.00016\n",
            "[76, 76, 79, 79, 77, 77, 76, 74, 74, 74, 72, 72, 84, 84, 83, 83, 48, 22, 12, 31, 21, 24, 32, 29]\n",
            "[60, 60, 64, 61, 62, 59, 60, 53, 55, 55, 60, 60, 60, 57, 55, 59, 32, 19, 32, 27, 28, 36, 44, 40]\n",
            "Epoch [19/500], Loss: 1826.902587890625, Learning Rate: 0.00016\n",
            "[86, 86, 84, 84, 83, 84, 84, 84, 86, 86, 86, 86, 88, 84, 84, 84, 34, 12, 10, 24, 15, 17, 22, 17]\n",
            "[69, 68, 69, 66, 67, 60, 60, 60, 67, 66, 62, 67, 64, 69, 69, 69, 18, 13, 14, 18, 17, 28, 30, 24]\n",
            "Epoch [20/500], Loss: 1497.8671875, Learning Rate: 0.00015\n",
            "[67, 87, 89, 91, 91, 94, 67, 87, 89, 92, 92, 91, 91, 87, 86, 87, 53, 28, 21, 39, 25, 30, 37, 31]\n",
            "[75, 72, 68, 67, 72, 74, 75, 68, 68, 65, 65, 60, 71, 72, 72, 72, 34, 29, 31, 29, 31, 46, 47, 41]\n",
            "Epoch [21/500], Loss: 1260.56787109375, Learning Rate: 0.00015\n",
            "[89, 86, 87, 84, 84, 82, 82, 87, 76, 87, 89, 89, 87, 86, 84, 87, 91, 51, 42, 67, 40, 57, 57, 58]\n",
            "[62, 58, 67, 63, 65, 58, 63, 60, 71, 72, 75, 69, 72, 67, 72, 69, 58, 49, 63, 52, 48, 82, 75, 70]\n",
            "Epoch [22/500], Loss: 1176.4593505859375, Learning Rate: 0.00015\n",
            "[84, 66, 83, 86, 81, 81, 86, 86, 84, 83, 81, 81, 67, 90, 84, 84, 45, 21, 21, 33, 21, 27, 33, 26]\n",
            "[69, 69, 64, 59, 62, 62, 59, 64, 64, 67, 60, 62, 59, 59, 64, 64, 29, 24, 27, 26, 28, 39, 41, 37]\n",
            "Epoch [23/500], Loss: 937.2117309570312, Learning Rate: 0.00015\n",
            "[84, 84, 84, 90, 83, 84, 81, 79, 84, 83, 81, 79, 81, 93, 80, 72, 64, 38, 37, 52, 31, 51, 46, 46]\n",
            "[72, 69, 67, 64, 62, 60, 62, 55, 57, 59, 62, 64, 65, 69, 67, 60, 46, 35, 44, 40, 42, 60, 56, 53]\n",
            "Epoch [24/500], Loss: 787.9244384765625, Learning Rate: 0.00015\n",
            "[83, 84, 86, 84, 81, 84, 84, 94, 88, 90, 86, 94, 84, 84, 83, 84, 107, 67, 66, 82, 50, 71, 71, 69]\n",
            "[67, 64, 60, 60, 62, 64, 66, 67, 68, 69, 71, 72, 69, 65, 67, 60, 69, 66, 70, 68, 64, 86, 80, 76]\n",
            "Epoch [25/500], Loss: 680.1959838867188, Learning Rate: 0.00015\n",
            "[79, 84, 86, 88, 86, 88, 89, 88, 86, 89, 88, 86, 86, 84, 84, 84, 70, 45, 48, 57, 35, 54, 56, 48]\n",
            "[67, 69, 67, 60, 67, 64, 57, 60, 59, 57, 67, 67, 67, 60, 60, 60, 44, 38, 43, 40, 35, 57, 55, 46]\n",
            "Epoch [26/500], Loss: 703.2359619140625, Learning Rate: 0.00015\n",
            "[72, 88, 86, 89, 86, 71, 86, 84, 84, 86, 88, 89, 88, 86, 88, 86, 106, 79, 68, 87, 57, 74, 73, 76]\n",
            "[72, 69, 65, 62, 59, 64, 67, 60, 60, 67, 65, 62, 67, 70, 69, 62, 74, 62, 76, 66, 70, 90, 83, 76]\n",
            "Epoch [27/500], Loss: 486.6693420410156, Learning Rate: 0.00015\n",
            "[88, 86, 84, 84, 83, 81, 81, 81, 79, 79, 79, 84, 84, 86, 85, 88, 94, 71, 69, 78, 54, 75, 72, 72]\n",
            "[60, 62, 64, 66, 67, 60, 62, 62, 55, 55, 55, 64, 64, 67, 72, 71, 67, 57, 68, 64, 64, 80, 74, 70]\n",
            "Epoch [28/500], Loss: 409.6878662109375, Learning Rate: 0.00014\n",
            "[80, 79, 77, 77, 75, 75, 75, 77, 73, 86, 80, 79, 77, 81, 75, 75, 127, 95, 96, 104, 77, 109, 99, 99]\n",
            "[48, 51, 56, 58, 51, 51, 60, 62, 63, 64, 65, 67, 68, 70, 63, 63, 94, 91, 93, 91, 85, 101, 100, 88]\n",
            "Epoch [29/500], Loss: 429.83441162109375, Learning Rate: 0.00014\n",
            "[79, 84, 83, 81, 83, 84, 81, 79, 79, 84, 79, 81, 76, 79, 77, 76, 116, 88, 88, 97, 76, 101, 92, 99]\n",
            "[60, 57, 59, 62, 55, 52, 50, 55, 60, 48, 52, 53, 57, 59, 62, 57, 88, 81, 88, 87, 78, 85, 92, 80]\n",
            "Epoch [30/500], Loss: 496.8851623535156, Learning Rate: 0.00014\n",
            "[89, 81, 86, 86, 84, 86, 86, 86, 88, 88, 88, 81, 81, 83, 84, 84, 79, 59, 60, 70, 54, 71, 72, 71]\n",
            "[67, 72, 66, 67, 69, 67, 67, 67, 60, 64, 60, 62, 60, 59, 57, 57, 57, 59, 62, 59, 54, 57, 69, 58]\n",
            "Epoch [31/500], Loss: 472.22332763671875, Learning Rate: 0.00014\n",
            "[91, 89, 89, 88, 88, 89, 88, 86, 84, 79, 88, 84, 88, 91, 91, 88, 75, 59, 57, 65, 54, 57, 65, 62]\n",
            "[64, 62, 67, 60, 60, 62, 64, 67, 69, 72, 60, 65, 72, 76, 71, 72, 52, 54, 60, 50, 51, 58, 62, 52]\n",
            "Epoch [32/500], Loss: 441.2696228027344, Learning Rate: 0.00013\n",
            "[85, 86, 80, 88, 89, 89, 88, 88, 88, 89, 91, 89, 88, 86, 88, 88, 96, 76, 80, 86, 65, 87, 82, 82]\n",
            "[69, 67, 72, 71, 69, 71, 72, 72, 72, 69, 64, 65, 67, 67, 60, 60, 67, 66, 69, 66, 64, 73, 71, 66]\n",
            "Epoch [33/500], Loss: 338.6722106933594, Learning Rate: 0.00013\n",
            "[84, 86, 86, 92, 79, 91, 86, 83, 86, 87, 89, 91, 90, 94, 89, 86, 74, 59, 63, 64, 51, 63, 61, 61]\n",
            "[72, 72, 71, 72, 72, 69, 69, 67, 67, 72, 74, 75, 75, 72, 69, 72, 49, 48, 52, 54, 50, 53, 57, 52]\n",
            "Epoch [34/500], Loss: 341.9645080566406, Learning Rate: 0.00013\n",
            "[79, 79, 84, 84, 86, 95, 88, 89, 88, 86, 86, 84, 84, 84, 77, 93, 80, 67, 67, 73, 63, 68, 70, 68]\n",
            "[55, 55, 64, 64, 67, 72, 71, 69, 67, 65, 67, 60, 60, 60, 72, 72, 57, 52, 63, 56, 58, 64, 58, 58]\n",
            "Epoch [35/500], Loss: 395.4739685058594, Learning Rate: 0.00013\n",
            "[79, 84, 86, 87, 89, 93, 86, 82, 82, 84, 84, 86, 86, 79, 79, 79, 78, 53, 59, 72, 52, 71, 65, 73]\n",
            "[60, 72, 71, 72, 69, 70, 66, 67, 63, 68, 65, 59, 59, 60, 60, 60, 48, 47, 42, 48, 44, 46, 49, 48]\n",
            "Epoch [36/500], Loss: 327.9085998535156, Learning Rate: 0.00013\n",
            "[81, 79, 79, 79, 84, 84, 86, 81, 81, 79, 86, 88, 88, 86, 86, 88, 85, 74, 74, 80, 66, 80, 84, 80]\n",
            "[62, 55, 55, 67, 64, 64, 67, 61, 61, 64, 60, 60, 60, 55, 55, 60, 64, 60, 66, 65, 64, 59, 66, 63]\n",
            "Epoch [37/500], Loss: 290.318359375, Learning Rate: 0.00013\n",
            "[84, 84, 77, 93, 89, 91, 91, 89, 88, 86, 86, 88, 88, 88, 91, 91, 75, 66, 64, 72, 61, 71, 72, 68]\n",
            "[60, 60, 72, 72, 69, 64, 64, 65, 67, 65, 67, 60, 60, 60, 60, 72, 56, 56, 55, 56, 55, 56, 57, 55]\n",
            "Epoch [38/500], Loss: 391.77484130859375, Learning Rate: 0.00013\n",
            "[77, 92, 75, 77, 77, 79, 79, 80, 80, 79, 77, 77, 77, 75, 75, 75, 85, 74, 85, 81, 71, 83, 80, 84]\n",
            "[62, 63, 60, 56, 58, 51, 51, 53, 53, 55, 56, 58, 58, 51, 51, 51, 67, 68, 66, 67, 65, 59, 65, 60]\n",
            "Epoch [39/500], Loss: 293.006591796875, Learning Rate: 0.00013\n",
            "[84, 89, 91, 89, 89, 87, 91, 93, 91, 88, 86, 84, 84, 89, 88, 91, 106, 94, 101, 97, 90, 99, 96, 102]\n",
            "[70, 69, 72, 65, 65, 72, 70, 69, 71, 72, 67, 60, 65, 62, 69, 64, 79, 75, 83, 79, 76, 80, 76, 72]\n",
            "Epoch [40/500], Loss: 337.9512939453125, Learning Rate: 0.00012\n",
            "[83, 79, 84, 79, 76, 81, 70, 79, 77, 79, 76, 74, 94, 74, 79, 76, 90, 90, 82, 91, 84, 87, 88, 94]\n",
            "[55, 55, 52, 52, 48, 53, 65, 64, 62, 59, 60, 55, 67, 65, 64, 60, 70, 73, 70, 66, 72, 65, 69, 70]\n",
            "Epoch [41/500], Loss: 267.56695556640625, Learning Rate: 0.00012\n",
            "[88, 89, 91, 91, 89, 88, 86, 86, 88, 89, 91, 88, 86, 86, 84, 84, 73, 70, 59, 70, 63, 65, 66, 58]\n",
            "[72, 69, 64, 69, 62, 60, 67, 67, 60, 62, 64, 69, 65, 67, 60, 60, 54, 57, 50, 52, 59, 55, 52, 58]\n",
            "Epoch [42/500], Loss: 284.2254638671875, Learning Rate: 0.00012\n",
            "[74, 76, 76, 84, 76, 76, 78, 79, 79, 79, 83, 83, 84, 81, 81, 84, 103, 97, 96, 100, 91, 102, 95, 101]\n",
            "[50, 48, 60, 59, 60, 59, 57, 55, 55, 55, 67, 65, 64, 65, 53, 64, 76, 83, 78, 76, 77, 73, 75, 73]\n",
            "Epoch [43/500], Loss: 239.98675537109375, Learning Rate: 0.00012\n",
            "[93, 91, 91, 89, 89, 88, 84, 86, 86, 88, 89, 91, 88, 86, 86, 84, 70, 67, 64, 70, 60, 66, 67, 62]\n",
            "[65, 65, 65, 62, 62, 60, 72, 71, 67, 72, 69, 67, 69, 65, 67, 60, 50, 54, 48, 51, 54, 56, 52, 55]\n",
            "Epoch [44/500], Loss: 304.8483581542969, Learning Rate: 0.00011\n",
            "[83, 83, 84, 84, 86, 86, 84, 84, 93, 81, 81, 81, 79, 79, 79, 79, 95, 96, 89, 91, 91, 90, 91, 91]\n",
            "[62, 55, 60, 60, 67, 71, 69, 66, 67, 60, 62, 62, 55, 55, 60, 64, 70, 84, 74, 66, 72, 68, 71, 69]\n",
            "Epoch [45/500], Loss: 234.9083709716797, Learning Rate: 0.00011\n",
            "[91, 88, 88, 88, 88, 86, 91, 93, 91, 91, 91, 91, 91, 67, 88, 89, 101, 101, 101, 99, 90, 103, 96, 99]\n",
            "[71, 72, 72, 72, 60, 72, 72, 72, 74, 67, 67, 67, 79, 76, 72, 69, 77, 81, 77, 80, 84, 80, 76, 79]\n",
            "Epoch [46/500], Loss: 260.3760681152344, Learning Rate: 0.00011\n",
            "[79, 84, 86, 74, 86, 84, 86, 83, 83, 79, 82, 82, 80, 84, 84, 84, 91, 92, 89, 90, 82, 86, 88, 82]\n",
            "[60, 72, 71, 72, 67, 68, 65, 67, 67, 55, 67, 62, 60, 63, 68, 65, 66, 70, 71, 68, 71, 76, 68, 71]\n",
            "Epoch [47/500], Loss: 208.7859649658203, Learning Rate: 0.00011\n",
            "[83, 79, 82, 82, 75, 79, 84, 84, 83, 84, 84, 84, 86, 87, 89, 89, 64, 62, 59, 64, 59, 63, 69, 64]\n",
            "[67, 67, 67, 72, 74, 71, 69, 67, 67, 60, 60, 60, 67, 72, 74, 75, 52, 44, 48, 47, 53, 44, 47, 50]\n",
            "Epoch [48/500], Loss: 267.4029541015625, Learning Rate: 0.00011\n",
            "[92, 88, 74, 72, 79, 79, 77, 82, 79, 75, 77, 79, 79, 82, 84, 87, 95, 101, 97, 98, 92, 97, 96, 97]\n",
            "[63, 67, 55, 60, 60, 58, 56, 55, 63, 68, 68, 67, 67, 63, 63, 60, 76, 79, 72, 73, 79, 73, 71, 73]\n",
            "Epoch [49/500], Loss: 268.9071960449219, Learning Rate: 0.00011\n",
            "[86, 87, 87, 86, 87, 86, 84, 82, 79, 82, 82, 84, 82, 80, 80, 65, 53, 57, 45, 56, 51, 49, 57, 49]\n",
            "[60, 60, 63, 67, 60, 67, 68, 62, 63, 67, 67, 68, 63, 65, 68, 72, 48, 41, 38, 38, 48, 39, 42, 47]\n",
            "Epoch [50/500], Loss: 253.61654663085938, Learning Rate: 0.00011\n",
            "[88, 86, 86, 91, 88, 70, 86, 86, 84, 84, 84, 84, 86, 88, 86, 86, 91, 89, 92, 90, 88, 90, 89, 88]\n",
            "[60, 72, 72, 69, 72, 76, 67, 67, 57, 57, 53, 53, 62, 60, 65, 67, 65, 64, 70, 68, 65, 72, 67, 66]\n",
            "Epoch [51/500], Loss: 298.989013671875, Learning Rate: 0.00011\n",
            "[89, 73, 72, 72, 73, 75, 77, 77, 75, 68, 72, 72, 77, 76, 77, 79, 87, 85, 84, 87, 80, 84, 81, 85]\n",
            "[57, 58, 53, 53, 58, 54, 50, 50, 51, 52, 53, 53, 53, 55, 56, 58, 66, 64, 69, 65, 68, 64, 63, 66]\n",
            "Epoch [52/500], Loss: 201.98342895507812, Learning Rate: 0.0001\n",
            "[68, 75, 86, 73, 70, 70, 70, 77, 77, 75, 77, 79, 83, 79, 77, 77, 104, 107, 100, 103, 95, 96, 101, 96]\n",
            "[61, 55, 58, 51, 46, 46, 58, 61, 54, 56, 56, 58, 60, 63, 56, 58, 79, 82, 80, 76, 82, 85, 76, 75]\n",
            "Epoch [53/500], Loss: 267.4803771972656, Learning Rate: 0.0001\n",
            "[83, 84, 86, 86, 91, 91, 86, 87, 89, 87, 86, 84, 69, 87, 86, 84, 87, 89, 88, 86, 86, 91, 89, 86]\n",
            "[62, 63, 67, 67, 60, 63, 67, 72, 68, 67, 67, 60, 72, 68, 65, 68, 66, 67, 67, 67, 65, 68, 66, 63]\n",
            "Epoch [54/500], Loss: 182.87203979492188, Learning Rate: 0.0001\n",
            "[88, 84, 84, 83, 79, 84, 86, 88, 88, 86, 86, 86, 88, 84, 86, 69, 73, 64, 68, 73, 68, 70, 72, 72]\n",
            "[64, 69, 69, 67, 65, 64, 62, 60, 64, 67, 67, 67, 68, 69, 71, 72, 56, 58, 53, 59, 54, 57, 56, 58]\n",
            "Epoch [55/500], Loss: 198.00888061523438, Learning Rate: 0.0001\n",
            "[87, 86, 84, 82, 86, 86, 84, 82, 81, 82, 81, 79, 86, 74, 89, 91, 96, 97, 93, 95, 87, 91, 93, 91]\n",
            "[67, 63, 65, 58, 70, 67, 63, 60, 62, 58, 62, 55, 67, 72, 68, 65, 75, 74, 73, 73, 73, 71, 70, 69]\n",
            "Epoch [56/500], Loss: 289.140380859375, Learning Rate: 0.0001\n",
            "[86, 88, 89, 89, 88, 88, 86, 86, 95, 91, 93, 93, 91, 91, 89, 89, 92, 100, 92, 89, 86, 91, 86, 82]\n",
            "[67, 64, 62, 65, 69, 57, 62, 62, 74, 72, 69, 65, 72, 60, 65, 65, 64, 71, 63, 65, 70, 73, 65, 70]\n",
            "Epoch [57/500], Loss: 178.70933532714844, Learning Rate: 0.0001\n",
            "[84, 84, 86, 86, 65, 84, 88, 88, 93, 89, 89, 91, 84, 89, 86, 86, 73, 75, 71, 73, 76, 75, 74, 73]\n",
            "[64, 64, 55, 55, 72, 64, 62, 60, 65, 69, 69, 72, 69, 65, 67, 67, 59, 59, 54, 54, 58, 59, 59, 57]\n",
            "Epoch [58/500], Loss: 157.51060485839844, Learning Rate: 0.0001\n",
            "[72, 79, 86, 72, 79, 81, 81, 79, 79, 81, 83, 84, 83, 81, 81, 79, 70, 69, 62, 71, 67, 67, 68, 63]\n",
            "[60, 59, 60, 57, 52, 53, 57, 60, 59, 60, 62, 64, 67, 60, 62, 55, 53, 50, 52, 51, 52, 56, 51, 58]\n",
            "Epoch [59/500], Loss: 182.12118530273438, Learning Rate: 0.0001\n",
            "[79, 81, 83, 84, 88, 86, 86, 84, 84, 83, 81, 79, 76, 81, 81, 78, 87, 87, 83, 86, 84, 87, 85, 82]\n",
            "[60, 65, 64, 69, 69, 65, 67, 60, 60, 67, 69, 64, 60, 65, 69, 72, 63, 64, 65, 61, 63, 70, 62, 66]\n",
            "Epoch [60/500], Loss: 200.24095153808594, Learning Rate: 9e-05\n",
            "[83, 83, 83, 83, 67, 95, 86, 86, 84, 84, 83, 83, 83, 81, 84, 84, 78, 75, 78, 80, 76, 81, 80, 80]\n",
            "[67, 67, 67, 67, 72, 72, 65, 65, 68, 68, 67, 67, 65, 65, 63, 63, 59, 62, 56, 60, 60, 58, 60, 60]\n",
            "Epoch [61/500], Loss: 232.2757568359375, Learning Rate: 9e-05\n",
            "[79, 81, 79, 77, 77, 76, 76, 76, 84, 83, 86, 84, 83, 81, 81, 83, 59, 53, 54, 59, 55, 53, 61, 57]\n",
            "[60, 65, 61, 62, 50, 57, 57, 57, 54, 55, 67, 64, 62, 60, 62, 55, 45, 46, 42, 43, 46, 43, 46, 47]\n",
            "Epoch [62/500], Loss: 173.63882446289062, Learning Rate: 9e-05\n",
            "[84, 79, 88, 84, 88, 91, 91, 88, 91, 89, 89, 88, 88, 89, 88, 86, 94, 95, 93, 88, 97, 98, 95, 94]\n",
            "[69, 72, 60, 65, 72, 76, 71, 72, 73, 74, 62, 69, 68, 69, 72, 65, 72, 69, 73, 76, 69, 69, 73, 65]\n",
            "Epoch [63/500], Loss: 264.9829406738281, Learning Rate: 9e-05\n",
            "[88, 88, 86, 84, 83, 84, 88, 86, 86, 84, 83, 84, 76, 84, 86, 88, 96, 103, 102, 100, 91, 100, 95, 92]\n",
            "[60, 64, 67, 69, 71, 69, 64, 67, 68, 69, 64, 69, 72, 69, 65, 64, 73, 77, 70, 72, 76, 76, 69, 75]\n",
            "Epoch [64/500], Loss: 172.2353057861328, Learning Rate: 9e-05\n",
            "[79, 77, 77, 75, 79, 89, 79, 77, 79, 82, 80, 67, 77, 77, 80, 81, 84, 76, 82, 81, 79, 75, 81, 83]\n",
            "[63, 56, 58, 51, 63, 62, 63, 58, 58, 55, 60, 65, 53, 53, 65, 67, 60, 63, 65, 61, 59, 62, 65, 60]\n",
            "Epoch [65/500], Loss: 264.14691162109375, Learning Rate: 9e-05\n",
            "[79, 82, 84, 82, 80, 79, 79, 79, 82, 80, 79, 80, 77, 75, 73, 72, 81, 77, 81, 80, 80, 81, 84, 82]\n",
            "[61, 58, 58, 58, 53, 60, 60, 60, 58, 65, 63, 60, 61, 58, 58, 60, 61, 64, 62, 65, 61, 60, 65, 60]\n",
            "Epoch [66/500], Loss: 262.9014587402344, Learning Rate: 9e-05\n",
            "[78, 80, 79, 82, 82, 77, 77, 75, 75, 79, 79, 75, 77, 75, 74, 78, 79, 76, 73, 77, 73, 71, 75, 71]\n",
            "[58, 58, 60, 62, 63, 56, 58, 51, 51, 51, 55, 56, 56, 60, 62, 65, 57, 56, 57, 53, 56, 63, 55, 61]\n",
            "Epoch [67/500], Loss: 203.99905395507812, Learning Rate: 9e-05\n",
            "[91, 89, 89, 88, 77, 88, 89, 91, 89, 86, 86, 84, 91, 93, 91, 88, 99, 105, 103, 97, 102, 108, 101, 101]\n",
            "[71, 74, 67, 72, 71, 72, 69, 64, 65, 67, 67, 60, 72, 65, 67, 69, 78, 80, 79, 78, 80, 76, 77, 75]\n",
            "Epoch [68/500], Loss: 174.4482421875, Learning Rate: 8e-05\n",
            "[83, 84, 81, 79, 84, 83, 81, 79, 81, 93, 80, 72, 84, 84, 84, 87, 80, 80, 83, 82, 88, 89, 89, 87]\n",
            "[62, 60, 62, 55, 57, 59, 62, 64, 65, 69, 67, 60, 72, 69, 67, 64, 66, 67, 62, 64, 65, 60, 67, 63]\n",
            "Epoch [69/500], Loss: 198.0487823486328, Learning Rate: 8e-05\n",
            "[84, 88, 89, 91, 89, 88, 86, 88, 72, 88, 86, 89, 86, 71, 86, 84, 83, 79, 82, 82, 78, 79, 78, 81]\n",
            "[60, 72, 69, 67, 62, 64, 67, 60, 72, 69, 65, 62, 59, 64, 67, 60, 61, 54, 64, 62, 59, 64, 61, 61]\n",
            "Epoch [70/500], Loss: 161.65557861328125, Learning Rate: 8e-05\n",
            "[79, 84, 82, 90, 79, 77, 77, 75, 79, 75, 77, 79, 79, 65, 74, 75, 84, 83, 80, 83, 83, 80, 83, 82]\n",
            "[60, 56, 56, 65, 63, 56, 58, 51, 60, 60, 60, 60, 63, 67, 55, 60, 61, 63, 66, 59, 63, 61, 62, 57]\n",
            "Epoch [71/500], Loss: 138.5415802001953, Learning Rate: 8e-05\n",
            "[76, 74, 74, 72, 72, 79, 79, 76, 76, 79, 79, 76, 76, 76, 76, 94, 66, 63, 65, 59, 69, 66, 66, 62]\n",
            "[60, 55, 55, 60, 60, 59, 59, 60, 60, 55, 55, 60, 60, 60, 64, 67, 48, 43, 50, 51, 48, 48, 50, 45]\n",
            "Epoch [72/500], Loss: 156.02609252929688, Learning Rate: 8e-05\n",
            "[79, 84, 86, 65, 89, 86, 86, 82, 82, 84, 84, 75, 86, 73, 80, 87, 100, 104, 112, 103, 100, 105, 105, 102]\n",
            "[60, 72, 71, 72, 69, 70, 66, 67, 63, 68, 69, 70, 71, 72, 72, 72, 80, 80, 81, 80, 76, 85, 79, 76]\n",
            "Epoch [73/500], Loss: 129.64691162109375, Learning Rate: 8e-05\n",
            "[86, 86, 86, 86, 87, 89, 83, 84, 82, 80, 79, 79, 86, 86, 67, 86, 84, 85, 88, 85, 86, 86, 87, 88]\n",
            "[67, 67, 67, 67, 72, 70, 67, 68, 63, 65, 67, 67, 65, 65, 63, 62, 67, 69, 64, 67, 67, 71, 67, 66]\n",
            "Epoch [74/500], Loss: 219.38714599609375, Learning Rate: 8e-05\n",
            "[79, 80, 79, 77, 75, 79, 95, 76, 79, 80, 79, 77, 75, 79, 74, 73, 100, 104, 100, 98, 97, 99, 98, 97]\n",
            "[60, 60, 63, 57, 60, 63, 67, 56, 55, 53, 55, 58, 60, 51, 55, 48, 77, 78, 76, 75, 77, 78, 75, 74]\n",
            "Epoch [75/500], Loss: 171.0973663330078, Learning Rate: 8e-05\n",
            "[86, 86, 84, 84, 84, 87, 89, 87, 89, 91, 87, 87, 87, 89, 91, 84, 82, 88, 79, 80, 82, 83, 80, 82]\n",
            "[71, 67, 60, 60, 60, 72, 70, 67, 65, 63, 68, 68, 68, 65, 71, 72, 68, 58, 62, 60, 69, 62, 62, 62]\n",
            "Epoch [76/500], Loss: 165.94004821777344, Learning Rate: 8e-05\n",
            "[77, 77, 75, 75, 79, 81, 82, 79, 84, 82, 81, 81, 79, 79, 79, 79, 89, 94, 93, 91, 90, 90, 94, 93]\n",
            "[56, 58, 51, 51, 63, 62, 62, 63, 57, 58, 62, 62, 55, 55, 60, 63, 72, 73, 69, 70, 72, 70, 71, 71]\n",
            "Epoch [77/500], Loss: 167.82598876953125, Learning Rate: 8e-05\n",
            "[76, 74, 72, 74, 85, 79, 77, 77, 76, 76, 79, 76, 77, 76, 70, 76, 88, 91, 87, 88, 88, 84, 86, 85]\n",
            "[60, 55, 57, 59, 60, 52, 55, 55, 48, 48, 59, 60, 57, 60, 53, 52, 67, 66, 66, 66, 65, 70, 65, 67]\n",
            "Epoch [78/500], Loss: 182.7751922607422, Learning Rate: 8e-05\n",
            "[79, 75, 74, 72, 79, 79, 89, 82, 79, 87, 77, 79, 79, 77, 84, 87, 79, 79, 72, 76, 80, 71, 81, 78]\n",
            "[63, 65, 62, 60, 60, 63, 62, 67, 63, 68, 56, 55, 60, 67, 65, 63, 57, 62, 65, 57, 59, 59, 64, 55]\n",
            "Epoch [79/500], Loss: 204.16952514648438, Learning Rate: 8e-05\n",
            "[69, 84, 84, 84, 88, 91, 93, 91, 88, 86, 81, 84, 86, 84, 84, 88, 97, 100, 98, 94, 99, 98, 96, 96]\n",
            "[67, 60, 60, 60, 72, 71, 69, 71, 72, 64, 65, 69, 65, 60, 60, 72, 76, 77, 76, 79, 79, 71, 72, 73]\n",
            "Epoch [80/500], Loss: 150.85179138183594, Learning Rate: 7e-05\n",
            "[88, 88, 86, 89, 86, 77, 86, 84, 84, 86, 88, 89, 88, 86, 86, 86, 88, 85, 81, 85, 84, 79, 81, 80]\n",
            "[69, 64, 67, 62, 65, 69, 65, 60, 72, 71, 71, 69, 67, 65, 67, 62, 61, 64, 64, 63, 65, 66, 60, 64]\n",
            "Epoch [81/500], Loss: 151.5435028076172, Learning Rate: 7e-05\n",
            "[84, 83, 79, 81, 83, 84, 84, 84, 79, 81, 79, 77, 77, 85, 72, 71, 88, 91, 91, 90, 91, 91, 96, 91]\n",
            "[69, 62, 64, 65, 62, 60, 60, 60, 60, 65, 60, 62, 65, 69, 69, 69, 73, 72, 70, 73, 73, 68, 69, 70]\n",
            "Epoch [82/500], Loss: 191.42005920410156, Learning Rate: 7e-05\n",
            "[84, 84, 66, 87, 86, 76, 89, 86, 84, 84, 87, 87, 86, 87, 89, 82, 75, 76, 73, 76, 78, 71, 75, 74]\n",
            "[72, 72, 74, 72, 70, 68, 65, 67, 60, 60, 72, 72, 67, 65, 62, 63, 57, 60, 53, 55, 61, 56, 56, 59]\n",
            "Epoch [83/500], Loss: 197.6250457763672, Learning Rate: 7e-05\n",
            "[79, 79, 77, 77, 76, 76, 79, 79, 77, 76, 74, 74, 72, 72, 80, 74, 90, 95, 95, 94, 89, 94, 96, 94]\n",
            "[55, 59, 62, 59, 60, 60, 67, 64, 60, 60, 53, 55, 48, 48, 67, 64, 74, 71, 71, 70, 69, 71, 69, 70]\n",
            "Epoch [84/500], Loss: 187.26095581054688, Learning Rate: 7e-05\n",
            "[83, 89, 89, 84, 88, 91, 91, 86, 88, 84, 84, 83, 79, 84, 86, 88, 103, 105, 106, 105, 103, 106, 108, 108]\n",
            "[72, 69, 71, 72, 69, 64, 64, 67, 64, 69, 69, 67, 65, 64, 62, 60, 81, 81, 82, 84, 78, 80, 80, 75]\n",
            "Epoch [85/500], Loss: 166.11827087402344, Learning Rate: 7e-05\n",
            "[86, 72, 86, 86, 87, 89, 91, 87, 87, 91, 92, 91, 89, 89, 87, 87, 78, 76, 72, 76, 83, 70, 76, 77]\n",
            "[67, 68, 67, 67, 67, 67, 63, 75, 75, 75, 72, 70, 68, 70, 63, 63, 55, 63, 56, 57, 58, 60, 57, 60]\n",
            "Epoch [86/500], Loss: 216.81297302246094, Learning Rate: 7e-05\n",
            "[86, 77, 82, 82, 81, 82, 82, 84, 86, 87, 86, 84, 84, 86, 86, 86, 85, 84, 84, 86, 83, 83, 86, 84]\n",
            "[67, 70, 67, 63, 65, 58, 70, 69, 68, 67, 65, 63, 63, 67, 67, 67, 61, 66, 63, 60, 60, 63, 62, 60]\n",
            "Epoch [87/500], Loss: 156.73306274414062, Learning Rate: 7e-05\n",
            "[91, 89, 87, 86, 84, 87, 86, 87, 91, 89, 87, 86, 91, 91, 91, 89, 87, 88, 91, 86, 90, 90, 92, 86]\n",
            "[65, 62, 60, 67, 72, 69, 70, 67, 63, 60, 60, 67, 67, 72, 68, 65, 63, 72, 67, 69, 66, 68, 71, 64]\n",
            "Epoch [88/500], Loss: 153.6403045654297, Learning Rate: 6e-05\n",
            "[89, 89, 86, 86, 86, 86, 84, 84, 84, 84, 84, 84, 89, 89, 91, 91, 90, 91, 91, 91, 89, 91, 90, 92]\n",
            "[65, 65, 67, 67, 67, 67, 60, 60, 60, 60, 60, 60, 62, 62, 63, 63, 70, 72, 69, 70, 70, 73, 71, 69]\n",
            "Epoch [89/500], Loss: 156.8196258544922, Learning Rate: 6e-05\n",
            "[76, 87, 89, 89, 87, 86, 84, 87, 91, 87, 91, 89, 87, 86, 91, 91, 82, 88, 92, 89, 81, 90, 87, 86]\n",
            "[71, 72, 75, 69, 72, 67, 72, 69, 70, 67, 63, 60, 60, 67, 59, 60, 71, 67, 63, 67, 71, 66, 65, 71]\n",
            "Epoch [90/500], Loss: 154.77967834472656, Learning Rate: 6e-05\n",
            "[91, 91, 91, 95, 72, 74, 91, 89, 91, 91, 84, 88, 90, 91, 93, 91, 89, 95, 93, 91, 92, 93, 95, 94]\n",
            "[72, 71, 67, 72, 74, 77, 72, 74, 71, 72, 72, 69, 72, 71, 72, 67, 71, 75, 68, 70, 76, 71, 69, 73]\n",
            "Epoch [91/500], Loss: 158.32199096679688, Learning Rate: 6e-05\n",
            "[79, 81, 84, 84, 83, 79, 81, 81, 79, 79, 79, 81, 84, 84, 83, 65, 81, 83, 85, 83, 85, 82, 82, 84]\n",
            "[60, 59, 57, 60, 64, 64, 60, 62, 55, 55, 60, 53, 52, 52, 55, 59, 66, 60, 61, 61, 64, 63, 62, 61]\n",
            "Epoch [92/500], Loss: 109.26995849609375, Learning Rate: 6e-05\n",
            "[85, 83, 84, 86, 84, 91, 81, 79, 86, 88, 88, 84, 88, 89, 88, 86, 92, 92, 86, 93, 89, 90, 96, 90]\n",
            "[74, 71, 71, 67, 71, 74, 62, 67, 67, 72, 69, 69, 65, 62, 64, 67, 72, 76, 70, 72, 71, 65, 69, 68]\n",
            "Epoch [93/500], Loss: 91.58031463623047, Learning Rate: 6e-05\n",
            "[84, 79, 79, 85, 79, 81, 67, 84, 83, 84, 81, 79, 81, 81, 79, 81, 95, 94, 95, 95, 94, 91, 93, 96]\n",
            "[62, 55, 55, 64, 62, 65, 67, 60, 64, 57, 60, 55, 62, 65, 64, 61, 72, 76, 76, 69, 74, 69, 69, 71]\n",
            "Epoch [94/500], Loss: 125.69375610351562, Learning Rate: 6e-05\n",
            "[86, 86, 67, 91, 91, 89, 89, 88, 84, 86, 86, 69, 90, 88, 84, 84, 84, 82, 82, 87, 87, 92, 87, 88]\n",
            "[67, 67, 72, 72, 64, 65, 65, 67, 69, 71, 67, 72, 72, 60, 65, 65, 63, 67, 61, 62, 61, 61, 64, 60]\n",
            "Epoch [95/500], Loss: 152.98326110839844, Learning Rate: 6e-05\n",
            "[84, 84, 82, 84, 80, 79, 80, 77, 77, 80, 82, 74, 82, 87, 86, 84, 91, 92, 93, 83, 95, 93, 89, 89]\n",
            "[60, 65, 70, 64, 65, 58, 60, 65, 65, 65, 67, 68, 63, 60, 67, 60, 69, 73, 67, 68, 72, 72, 69, 66]\n",
            "Epoch [96/500], Loss: 133.55039978027344, Learning Rate: 6e-05\n",
            "[91, 89, 87, 86, 87, 84, 84, 89, 89, 86, 87, 84, 84, 82, 82, 87, 104, 108, 107, 106, 101, 105, 102, 101]\n",
            "[72, 69, 72, 67, 60, 72, 69, 65, 62, 58, 67, 63, 65, 58, 63, 60, 81, 82, 78, 81, 83, 81, 77, 77]\n",
            "Epoch [97/500], Loss: 135.50762939453125, Learning Rate: 6e-05\n",
            "[67, 87, 89, 91, 91, 94, 67, 87, 89, 92, 92, 91, 91, 87, 86, 87, 89, 89, 90, 85, 92, 90, 87, 93]\n",
            "[75, 72, 68, 67, 72, 74, 75, 68, 68, 65, 65, 60, 71, 72, 72, 72, 66, 66, 71, 67, 71, 65, 69, 65]\n",
            "Epoch [98/500], Loss: 187.83920288085938, Learning Rate: 6e-05\n",
            "[84, 93, 89, 91, 93, 88, 90, 91, 71, 91, 69, 91, 94, 89, 89, 88, 64, 68, 61, 61, 72, 61, 59, 59]\n",
            "[65, 72, 69, 65, 65, 69, 62, 67, 72, 72, 77, 71, 73, 74, 62, 69, 46, 48, 44, 42, 48, 41, 46, 48]\n",
            "Epoch [99/500], Loss: 129.89447021484375, Learning Rate: 6e-05\n",
            "[77, 79, 79, 79, 82, 81, 79, 79, 78, 79, 79, 79, 84, 75, 74, 75, 84, 84, 83, 84, 89, 83, 86, 88]\n",
            "[56, 51, 51, 51, 62, 60, 61, 62, 50, 55, 55, 55, 63, 60, 60, 60, 64, 68, 69, 63, 62, 62, 67, 61]\n",
            "Epoch [100/500], Loss: 147.65179443359375, Learning Rate: 6e-05\n",
            "[86, 86, 84, 84, 84, 84, 86, 88, 86, 86, 88, 88, 83, 83, 81, 81, 96, 95, 102, 93, 94, 91, 93, 95]\n",
            "[65, 67, 60, 60, 64, 65, 65, 62, 67, 67, 68, 69, 62, 64, 57, 57, 72, 66, 77, 73, 69, 74, 70, 68]\n",
            "Epoch [101/500], Loss: 143.9309539794922, Learning Rate: 6e-05\n",
            "[84, 84, 84, 83, 87, 86, 84, 83, 83, 83, 83, 84, 81, 83, 83, 84, 81, 81, 79, 82, 79, 75, 78, 76]\n",
            "[72, 72, 68, 67, 63, 65, 66, 67, 67, 68, 67, 65, 65, 62, 67, 60, 58, 55, 61, 58, 61, 65, 58, 64]\n",
            "Epoch [102/500], Loss: 122.30339813232422, Learning Rate: 6e-05\n",
            "[86, 87, 89, 66, 91, 89, 86, 86, 84, 87, 89, 91, 91, 89, 86, 86, 89, 89, 95, 93, 89, 90, 90, 92]\n",
            "[67, 72, 75, 75, 72, 69, 65, 70, 71, 72, 68, 68, 65, 65, 68, 67, 71, 72, 69, 65, 68, 66, 68, 67]\n",
            "Epoch [103/500], Loss: 117.12704467773438, Learning Rate: 6e-05\n",
            "[81, 79, 79, 84, 84, 86, 86, 88, 88, 84, 84, 89, 89, 88, 88, 86, 81, 79, 82, 79, 84, 78, 86, 84]\n",
            "[62, 55, 67, 64, 69, 65, 67, 60, 60, 65, 64, 62, 74, 67, 69, 62, 61, 67, 64, 64, 63, 63, 66, 63]\n",
            "Epoch [104/500], Loss: 145.8237762451172, Learning Rate: 5e-05\n",
            "[79, 81, 79, 77, 77, 85, 72, 71, 84, 75, 86, 84, 83, 81, 81, 83, 61, 57, 59, 61, 59, 58, 62, 58]\n",
            "[60, 65, 60, 62, 65, 69, 69, 69, 66, 67, 59, 62, 65, 60, 62, 55, 48, 45, 46, 47, 47, 47, 48, 48]\n",
            "Epoch [105/500], Loss: 132.82522583007812, Learning Rate: 5e-05\n",
            "[85, 86, 86, 86, 74, 84, 86, 88, 89, 91, 89, 86, 89, 88, 86, 91, 84, 84, 84, 85, 82, 82, 86, 83]\n",
            "[69, 62, 62, 62, 64, 69, 67, 64, 62, 60, 64, 67, 62, 69, 72, 71, 64, 66, 64, 66, 64, 62, 65, 62]\n",
            "Epoch [106/500], Loss: 140.7320098876953, Learning Rate: 5e-05\n",
            "[72, 92, 90, 76, 77, 77, 76, 76, 76, 77, 79, 77, 76, 74, 76, 76, 93, 91, 97, 95, 86, 91, 90, 91]\n",
            "[57, 67, 64, 60, 57, 59, 60, 60, 57, 55, 52, 52, 62, 62, 57, 57, 68, 70, 72, 68, 66, 71, 69, 68]\n",
            "Epoch [107/500], Loss: 127.9660415649414, Learning Rate: 5e-05\n",
            "[84, 84, 84, 84, 79, 79, 77, 84, 69, 74, 72, 72, 79, 79, 81, 83, 98, 94, 101, 100, 93, 93, 95, 98]\n",
            "[60, 60, 60, 60, 60, 63, 68, 67, 67, 55, 60, 60, 60, 63, 65, 63, 72, 76, 76, 71, 72, 75, 71, 71]\n",
            "Epoch [108/500], Loss: 107.54668426513672, Learning Rate: 5e-05\n",
            "[92, 77, 91, 95, 68, 84, 89, 89, 89, 89, 91, 87, 87, 86, 89, 92, 90, 89, 89, 91, 91, 86, 89, 94]\n",
            "[65, 77, 77, 77, 75, 72, 68, 70, 67, 71, 67, 67, 72, 72, 68, 65, 69, 70, 74, 67, 71, 70, 67, 71]\n",
            "Epoch [109/500], Loss: 137.93247985839844, Learning Rate: 5e-05\n",
            "[91, 93, 91, 88, 91, 89, 89, 88, 68, 88, 89, 91, 89, 86, 86, 84, 89, 93, 92, 91, 89, 92, 92, 91]\n",
            "[72, 65, 67, 69, 71, 74, 67, 72, 71, 72, 69, 64, 65, 67, 67, 60, 72, 72, 69, 67, 69, 68, 68, 70]\n",
            "Epoch [110/500], Loss: 149.6888427734375, Learning Rate: 5e-05\n",
            "[87, 86, 84, 82, 86, 86, 84, 82, 81, 82, 81, 79, 86, 74, 89, 91, 82, 81, 85, 82, 82, 85, 82, 82]\n",
            "[67, 63, 65, 58, 70, 67, 63, 60, 62, 58, 62, 55, 67, 72, 68, 65, 62, 63, 62, 61, 61, 62, 62, 60]\n",
            "Epoch [111/500], Loss: 165.90780639648438, Learning Rate: 5e-05\n",
            "[84, 82, 86, 79, 79, 75, 77, 76, 75, 80, 74, 72, 79, 78, 79, 82, 94, 92, 97, 98, 90, 98, 93, 101]\n",
            "[55, 67, 60, 55, 59, 60, 62, 63, 63, 67, 55, 60, 60, 60, 58, 55, 70, 74, 69, 65, 70, 67, 68, 69]\n",
            "Epoch [112/500], Loss: 132.23094177246094, Learning Rate: 5e-05\n",
            "[89, 72, 76, 89, 91, 71, 84, 86, 86, 84, 84, 84, 91, 89, 87, 86, 83, 83, 79, 79, 85, 82, 79, 79]\n",
            "[69, 70, 70, 58, 71, 72, 68, 65, 67, 72, 72, 72, 60, 62, 63, 65, 60, 59, 62, 60, 62, 64, 60, 60]\n",
            "Epoch [113/500], Loss: 157.91827392578125, Learning Rate: 5e-05\n",
            "[75, 77, 77, 75, 74, 74, 72, 72, 79, 79, 87, 80, 77, 90, 79, 79, 92, 92, 93, 89, 93, 87, 85, 88]\n",
            "[56, 55, 55, 56, 53, 55, 48, 48, 60, 63, 65, 65, 58, 62, 63, 63, 66, 69, 70, 68, 65, 67, 66, 66]\n",
            "Epoch [114/500], Loss: 121.68750762939453, Learning Rate: 5e-05\n",
            "[89, 90, 89, 87, 86, 87, 87, 87, 80, 85, 87, 89, 90, 92, 90, 87, 62, 58, 63, 64, 61, 65, 60, 60]\n",
            "[65, 63, 68, 71, 70, 63, 63, 63, 66, 65, 63, 61, 63, 65, 66, 68, 48, 43, 47, 47, 45, 48, 48, 48]\n",
            "Epoch [115/500], Loss: 106.27714538574219, Learning Rate: 5e-05\n",
            "[91, 91, 91, 91, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 93, 93, 94, 94, 94, 91, 91, 92, 90, 87]\n",
            "[71, 72, 74, 74, 62, 62, 67, 67, 67, 67, 67, 67, 72, 72, 65, 65, 67, 73, 73, 72, 68, 75, 69, 68]\n",
            "Epoch [116/500], Loss: 123.7783203125, Learning Rate: 5e-05\n",
            "[86, 84, 84, 84, 91, 89, 87, 86, 73, 86, 86, 86, 86, 87, 89, 66, 85, 85, 94, 91, 84, 87, 86, 87]\n",
            "[67, 60, 60, 60, 60, 62, 63, 67, 68, 67, 67, 67, 67, 72, 75, 75, 69, 65, 69, 64, 65, 69, 65, 67]\n",
            "Epoch [117/500], Loss: 158.88876342773438, Learning Rate: 5e-05\n",
            "[76, 77, 74, 76, 74, 72, 72, 72, 72, 72, 76, 74, 72, 71, 71, 69, 85, 90, 86, 84, 89, 88, 91, 88]\n",
            "[57, 62, 65, 64, 55, 48, 48, 48, 60, 53, 60, 55, 57, 50, 52, 57, 69, 66, 70, 69, 70, 66, 68, 67]\n",
            "Epoch [118/500], Loss: 160.5172119140625, Learning Rate: 5e-05\n",
            "[79, 76, 72, 76, 78, 79, 81, 79, 79, 79, 79, 79, 82, 81, 79, 77, 68, 64, 64, 68, 69, 64, 68, 66]\n",
            "[59, 60, 60, 57, 60, 59, 62, 55, 60, 60, 59, 60, 52, 53, 55, 58, 52, 50, 52, 51, 52, 51, 51, 51]\n",
            "Epoch [119/500], Loss: 93.53633117675781, Learning Rate: 5e-05\n",
            "[88, 81, 88, 86, 88, 84, 86, 89, 88, 78, 88, 89, 88, 86, 90, 91, 83, 80, 81, 81, 81, 81, 77, 78]\n",
            "[67, 64, 60, 67, 60, 65, 65, 62, 69, 70, 67, 65, 60, 67, 67, 67, 60, 60, 63, 62, 60, 61, 61, 61]\n",
            "Epoch [120/500], Loss: 101.46663665771484, Learning Rate: 4e-05\n",
            "[87, 89, 91, 91, 86, 87, 89, 86, 87, 86, 84, 82, 86, 86, 84, 82, 64, 58, 63, 63, 65, 62, 64, 59]\n",
            "[60, 56, 55, 55, 67, 72, 69, 70, 67, 63, 65, 58, 70, 67, 63, 60, 49, 49, 46, 53, 48, 49, 52, 51]\n",
            "Epoch [121/500], Loss: 129.4539794921875, Learning Rate: 4e-05\n",
            "[69, 88, 88, 84, 86, 88, 89, 88, 88, 86, 86, 86, 84, 86, 88, 86, 66, 63, 63, 65, 65, 63, 67, 63]\n",
            "[72, 68, 64, 69, 69, 57, 62, 64, 65, 67, 67, 67, 64, 62, 60, 65, 50, 49, 49, 50, 49, 48, 52, 50]\n",
            "Epoch [122/500], Loss: 147.4098358154297, Learning Rate: 4e-05\n",
            "[74, 74, 72, 72, 79, 81, 82, 79, 84, 84, 83, 83, 84, 87, 86, 86, 66, 66, 64, 60, 71, 70, 64, 61]\n",
            "[53, 55, 48, 48, 60, 63, 62, 65, 64, 66, 67, 67, 68, 67, 67, 55, 51, 47, 50, 48, 49, 53, 51, 48]\n",
            "Epoch [123/500], Loss: 134.31040954589844, Learning Rate: 4e-05\n",
            "[88, 89, 88, 86, 84, 79, 88, 84, 88, 91, 91, 88, 91, 89, 89, 88, 68, 68, 66, 69, 71, 68, 78, 70]\n",
            "[60, 62, 64, 67, 69, 72, 60, 65, 72, 76, 71, 72, 73, 74, 62, 69, 60, 50, 53, 56, 60, 50, 53, 57]\n",
            "Epoch [124/500], Loss: 141.2211151123047, Learning Rate: 4e-05\n",
            "[79, 81, 83, 84, 84, 91, 71, 71, 79, 79, 81, 79, 77, 76, 76, 76, 90, 95, 95, 88, 90, 90, 92, 90]\n",
            "[67, 66, 62, 64, 66, 67, 67, 67, 65, 64, 62, 59, 55, 57, 57, 57, 69, 68, 71, 68, 69, 70, 69, 65]\n",
            "Epoch [125/500], Loss: 110.8781509399414, Learning Rate: 4e-05\n",
            "[80, 79, 77, 75, 79, 68, 84, 84, 77, 77, 79, 80, 82, 82, 80, 79, 80, 83, 81, 82, 82, 85, 80, 80]\n",
            "[60, 56, 58, 51, 63, 60, 56, 57, 58, 58, 63, 62, 62, 62, 60, 63, 64, 61, 62, 59, 63, 65, 61, 65]\n",
            "Epoch [126/500], Loss: 171.08670043945312, Learning Rate: 4e-05\n",
            "[86, 86, 88, 88, 88, 88, 86, 88, 88, 88, 86, 77, 86, 86, 84, 84, 70, 70, 69, 70, 71, 69, 70, 70]\n",
            "[65, 67, 60, 60, 72, 68, 68, 64, 69, 67, 65, 64, 65, 67, 60, 60, 55, 47, 54, 51, 55, 56, 54, 54]\n",
            "Epoch [127/500], Loss: 116.0801773071289, Learning Rate: 4e-05\n",
            "[86, 84, 84, 84, 73, 84, 82, 80, 79, 77, 77, 79, 86, 74, 87, 86, 84, 81, 83, 82, 87, 87, 86, 84]\n",
            "[67, 60, 60, 60, 67, 68, 70, 72, 70, 68, 68, 67, 67, 72, 68, 65, 65, 68, 63, 65, 65, 62, 66, 61]\n",
            "Epoch [128/500], Loss: 113.19806671142578, Learning Rate: 4e-05\n",
            "[83, 83, 81, 81, 86, 86, 84, 83, 84, 81, 81, 79, 79, 79, 84, 84, 81, 86, 81, 81, 87, 84, 82, 83]\n",
            "[62, 64, 57, 57, 66, 67, 67, 65, 64, 60, 62, 67, 67, 64, 69, 67, 65, 63, 61, 61, 65, 62, 62, 64]\n",
            "Epoch [129/500], Loss: 165.38674926757812, Learning Rate: 4e-05\n",
            "[86, 93, 86, 83, 83, 79, 82, 82, 75, 79, 84, 84, 83, 84, 84, 84, 90, 93, 96, 89, 95, 95, 94, 92]\n",
            "[62, 63, 65, 67, 67, 67, 67, 72, 74, 71, 69, 67, 67, 60, 60, 60, 70, 71, 72, 73, 70, 72, 71, 69]\n",
            "Epoch [130/500], Loss: 135.37330627441406, Learning Rate: 4e-05\n",
            "[83, 83, 83, 84, 81, 83, 83, 84, 86, 86, 89, 86, 77, 84, 86, 87, 95, 96, 95, 93, 96, 99, 94, 94]\n",
            "[67, 68, 67, 65, 65, 62, 67, 60, 70, 70, 70, 70, 68, 68, 70, 72, 71, 74, 73, 73, 73, 72, 73, 70]\n",
            "Epoch [131/500], Loss: 121.47032928466797, Learning Rate: 4e-05\n",
            "[79, 78, 79, 72, 82, 82, 79, 77, 75, 75, 75, 92, 82, 65, 84, 86, 70, 69, 66, 70, 70, 69, 71, 68]\n",
            "[67, 70, 67, 72, 70, 60, 63, 58, 51, 51, 51, 67, 67, 67, 68, 59, 54, 52, 54, 51, 52, 57, 54, 56]\n",
            "Epoch [132/500], Loss: 95.32592010498047, Learning Rate: 4e-05\n",
            "[84, 82, 81, 81, 79, 79, 79, 81, 82, 84, 82, 81, 79, 79, 79, 84, 54, 48, 49, 53, 52, 51, 54, 51]\n",
            "[69, 67, 60, 62, 55, 55, 67, 66, 67, 60, 62, 50, 55, 55, 60, 64, 42, 37, 38, 41, 40, 40, 43, 42]\n",
            "Epoch [133/500], Loss: 132.0558624267578, Learning Rate: 4e-05\n",
            "[75, 75, 75, 65, 77, 75, 75, 73, 72, 72, 72, 72, 79, 79, 80, 80, 76, 78, 71, 79, 81, 73, 77, 76]\n",
            "[51, 51, 56, 53, 49, 54, 56, 58, 58, 56, 60, 60, 48, 60, 60, 58, 59, 60, 56, 55, 60, 60, 57, 62]\n",
            "Epoch [134/500], Loss: 132.24868774414062, Learning Rate: 4e-05\n",
            "[84, 88, 84, 81, 86, 84, 84, 83, 67, 88, 88, 93, 91, 91, 69, 91, 91, 93, 93, 90, 88, 93, 91, 88]\n",
            "[60, 64, 60, 65, 62, 64, 66, 67, 72, 72, 69, 66, 67, 71, 74, 67, 69, 72, 66, 69, 68, 67, 68, 65]\n",
            "Epoch [135/500], Loss: 118.18809509277344, Learning Rate: 4e-05\n",
            "[87, 86, 80, 87, 89, 86, 86, 84, 89, 91, 91, 86, 89, 87, 87, 86, 59, 55, 56, 59, 57, 57, 58, 55]\n",
            "[60, 67, 68, 68, 67, 67, 67, 60, 70, 63, 67, 70, 71, 72, 63, 67, 45, 43, 42, 45, 45, 45, 47, 47]\n",
            "Epoch [136/500], Loss: 123.25682067871094, Learning Rate: 3e-05\n",
            "[84, 88, 88, 86, 86, 84, 84, 84, 88, 86, 88, 84, 67, 83, 84, 86, 96, 99, 99, 97, 96, 97, 94, 94]\n",
            "[69, 72, 64, 67, 67, 60, 60, 60, 60, 67, 68, 69, 74, 71, 69, 67, 72, 71, 77, 73, 75, 79, 70, 75]\n",
            "Epoch [137/500], Loss: 96.57049560546875, Learning Rate: 3e-05\n",
            "[84, 88, 91, 88, 66, 88, 90, 91, 93, 91, 88, 86, 93, 88, 86, 84, 81, 88, 82, 84, 79, 83, 86, 78]\n",
            "[60, 60, 59, 60, 67, 72, 69, 67, 74, 71, 72, 66, 69, 72, 67, 60, 66, 67, 58, 58, 63, 65, 62, 63]\n",
            "Epoch [138/500], Loss: 110.66328430175781, Learning Rate: 3e-05\n",
            "[86, 87, 86, 87, 86, 84, 84, 84, 73, 86, 82, 84, 86, 87, 87, 87, 77, 78, 78, 79, 82, 78, 78, 81]\n",
            "[59, 60, 65, 67, 67, 60, 60, 60, 72, 66, 67, 66, 65, 67, 68, 63, 61, 60, 57, 58, 60, 58, 60, 59]\n",
            "Epoch [139/500], Loss: 138.2545623779297, Learning Rate: 3e-05\n",
            "[79, 84, 86, 88, 89, 86, 86, 86, 88, 84, 86, 70, 91, 86, 89, 88, 77, 80, 79, 72, 85, 82, 74, 76]\n",
            "[65, 64, 62, 60, 65, 67, 67, 67, 68, 69, 71, 72, 73, 74, 71, 69, 59, 57, 63, 58, 60, 64, 61, 59]\n",
            "Epoch [140/500], Loss: 123.00249481201172, Learning Rate: 3e-05\n",
            "[88, 89, 91, 89, 88, 94, 88, 88, 91, 91, 93, 91, 89, 88, 86, 86, 86, 88, 87, 85, 87, 87, 84, 81]\n",
            "[60, 62, 64, 65, 69, 69, 69, 69, 64, 60, 65, 67, 69, 72, 67, 67, 63, 61, 69, 65, 63, 70, 64, 65]\n",
            "Epoch [141/500], Loss: 106.36141967773438, Learning Rate: 3e-05\n",
            "[84, 84, 87, 86, 91, 89, 87, 86, 87, 75, 91, 89, 91, 84, 88, 89, 91, 90, 85, 88, 91, 84, 86, 85]\n",
            "[60, 72, 72, 67, 63, 62, 60, 67, 60, 72, 72, 68, 65, 65, 60, 53, 68, 67, 73, 68, 66, 65, 67, 64]\n",
            "Epoch [142/500], Loss: 135.2157745361328, Learning Rate: 3e-05\n",
            "[83, 72, 81, 79, 84, 83, 84, 86, 86, 88, 84, 86, 73, 81, 83, 84, 95, 97, 88, 97, 94, 86, 94, 95]\n",
            "[69, 74, 62, 67, 64, 62, 60, 67, 67, 60, 65, 58, 59, 61, 63, 64, 69, 73, 80, 70, 72, 76, 70, 73]\n",
            "Epoch [143/500], Loss: 93.08576965332031, Learning Rate: 3e-05\n",
            "[84, 84, 89, 86, 84, 94, 81, 81, 74, 81, 82, 81, 67, 67, 82, 84, 74, 72, 78, 73, 78, 77, 75, 74]\n",
            "[65, 65, 62, 65, 69, 67, 65, 65, 72, 69, 67, 60, 63, 63, 67, 65, 58, 57, 59, 60, 57, 58, 60, 57]\n",
            "Epoch [144/500], Loss: 118.29194641113281, Learning Rate: 3e-05\n",
            "[79, 79, 75, 74, 75, 77, 79, 77, 75, 75, 91, 74, 75, 75, 77, 79, 92, 92, 93, 93, 95, 92, 95, 101]\n",
            "[55, 55, 60, 58, 55, 53, 51, 58, 60, 56, 53, 58, 51, 51, 58, 63, 66, 69, 75, 70, 69, 70, 74, 66]\n",
            "Epoch [145/500], Loss: 95.98921203613281, Learning Rate: 3e-05\n",
            "[93, 91, 89, 88, 86, 88, 86, 84, 91, 93, 93, 86, 86, 91, 91, 84, 82, 85, 83, 83, 82, 82, 85, 83]\n",
            "[65, 67, 69, 72, 65, 67, 67, 60, 72, 65, 66, 67, 67, 64, 64, 65, 64, 64, 62, 66, 67, 66, 63, 65]\n",
            "Epoch [146/500], Loss: 106.6171875, Learning Rate: 3e-05\n",
            "[80, 79, 75, 79, 79, 77, 77, 77, 77, 77, 77, 77, 77, 79, 79, 79, 68, 71, 64, 71, 71, 65, 69, 67]\n",
            "[53, 58, 61, 58, 60, 53, 53, 53, 53, 56, 53, 49, 49, 48, 48, 48, 55, 55, 50, 49, 56, 52, 53, 57]\n",
            "Epoch [147/500], Loss: 105.40662384033203, Learning Rate: 3e-05\n",
            "[79, 84, 84, 83, 84, 84, 87, 89, 91, 91, 89, 87, 89, 89, 87, 87, 84, 83, 84, 85, 82, 78, 80, 81]\n",
            "[63, 60, 65, 67, 60, 60, 72, 70, 67, 63, 63, 63, 68, 70, 63, 63, 62, 57, 67, 63, 63, 67, 60, 62]\n",
            "Epoch [148/500], Loss: 121.11216735839844, Learning Rate: 3e-05\n",
            "[87, 91, 91, 87, 91, 89, 87, 87, 86, 87, 87, 87, 85, 94, 94, 88, 90, 91, 94, 90, 92, 93, 89, 93]\n",
            "[72, 71, 71, 72, 72, 68, 67, 65, 65, 63, 63, 63, 75, 74, 74, 75, 70, 74, 67, 68, 71, 70, 69, 69]\n",
            "Epoch [149/500], Loss: 81.29620361328125, Learning Rate: 3e-05\n",
            "[91, 91, 89, 89, 69, 86, 87, 87, 89, 89, 91, 91, 91, 91, 92, 91, 82, 82, 81, 83, 86, 82, 82, 84]\n",
            "[70, 72, 65, 65, 77, 71, 72, 67, 68, 70, 63, 63, 59, 60, 65, 67, 64, 65, 62, 65, 62, 61, 63, 64]\n",
            "Epoch [150/500], Loss: 118.0634994506836, Learning Rate: 3e-05\n",
            "[68, 84, 82, 80, 79, 77, 77, 79, 86, 87, 86, 87, 86, 84, 84, 84, 89, 95, 89, 90, 96, 89, 92, 92]\n",
            "[72, 68, 67, 65, 63, 57, 58, 63, 59, 60, 65, 67, 67, 60, 60, 60, 71, 71, 70, 69, 70, 67, 69, 69]\n",
            "Epoch [151/500], Loss: 82.60316467285156, Learning Rate: 3e-05\n",
            "[86, 91, 87, 89, 92, 91, 91, 94, 91, 92, 91, 91, 91, 92, 91, 94, 81, 83, 80, 81, 82, 83, 79, 78]\n",
            "[75, 71, 72, 68, 65, 58, 58, 62, 63, 63, 63, 63, 63, 63, 63, 67, 63, 62, 60, 60, 63, 59, 61, 61]\n",
            "Epoch [152/500], Loss: 142.1770782470703, Learning Rate: 3e-05\n",
            "[86, 86, 84, 84, 89, 89, 89, 89, 91, 75, 89, 87, 86, 84, 82, 82, 74, 74, 75, 76, 74, 73, 81, 74]\n",
            "[67, 55, 60, 60, 69, 70, 72, 74, 75, 75, 70, 67, 63, 65, 58, 58, 62, 58, 58, 61, 62, 57, 58, 61]\n",
            "Epoch [153/500], Loss: 94.92349243164062, Learning Rate: 3e-05\n",
            "[84, 81, 81, 79, 79, 84, 83, 81, 79, 84, 86, 88, 86, 88, 89, 88, 83, 89, 85, 83, 85, 86, 86, 82]\n",
            "[64, 60, 62, 55, 64, 57, 59, 62, 65, 64, 67, 60, 67, 64, 57, 60, 67, 65, 66, 67, 68, 66, 66, 67]\n",
            "Epoch [154/500], Loss: 115.65901947021484, Learning Rate: 3e-05\n",
            "[91, 75, 77, 79, 79, 75, 77, 79, 75, 74, 72, 74, 74, 70, 72, 72, 67, 62, 65, 70, 65, 68, 68, 69]\n",
            "[55, 55, 55, 51, 59, 60, 62, 63, 60, 60, 60, 55, 55, 52, 52, 53, 55, 49, 51, 53, 48, 54, 53, 54]\n",
            "Epoch [155/500], Loss: 87.92236328125, Learning Rate: 3e-05\n",
            "[81, 81, 79, 79, 77, 76, 74, 72, 72, 72, 72, 72, 79, 79, 79, 79, 91, 87, 89, 94, 87, 96, 90, 95]\n",
            "[65, 53, 60, 60, 62, 64, 65, 67, 60, 60, 60, 60, 60, 55, 59, 60, 66, 70, 65, 67, 65, 65, 66, 65]\n",
            "Epoch [156/500], Loss: 96.64332580566406, Learning Rate: 3e-05\n",
            "[84, 75, 81, 83, 84, 84, 84, 83, 84, 82, 81, 81, 79, 79, 79, 81, 86, 82, 82, 78, 85, 83, 82, 80]\n",
            "[72, 71, 69, 67, 64, 65, 60, 72, 69, 67, 60, 62, 55, 55, 67, 66, 60, 66, 59, 65, 61, 62, 65, 55]\n",
            "Epoch [157/500], Loss: 100.36143493652344, Learning Rate: 3e-05\n",
            "[77, 79, 88, 77, 89, 73, 72, 72, 73, 75, 77, 77, 75, 68, 72, 72, 77, 75, 74, 77, 78, 79, 81, 78]\n",
            "[65, 64, 65, 61, 57, 58, 53, 53, 58, 54, 50, 50, 51, 52, 53, 53, 56, 58, 56, 59, 57, 55, 58, 55]\n",
            "Epoch [158/500], Loss: 154.63070678710938, Learning Rate: 3e-05\n",
            "[89, 89, 88, 88, 88, 88, 86, 88, 89, 88, 86, 86, 84, 84, 84, 84, 90, 90, 89, 89, 95, 90, 96, 94]\n",
            "[62, 67, 60, 60, 60, 64, 67, 72, 72, 72, 65, 67, 60, 60, 60, 60, 67, 71, 69, 69, 69, 66, 67, 67]\n",
            "Epoch [159/500], Loss: 104.70938873291016, Learning Rate: 3e-05\n",
            "[91, 89, 89, 91, 91, 91, 91, 91, 89, 87, 87, 95, 91, 91, 94, 92, 93, 95, 90, 94, 92, 91, 92, 88]\n",
            "[63, 68, 70, 63, 60, 63, 67, 71, 74, 72, 75, 79, 75, 72, 68, 65, 71, 71, 72, 70, 70, 69, 68, 68]\n",
            "Epoch [160/500], Loss: 119.97390747070312, Learning Rate: 3e-05\n",
            "[91, 88, 86, 84, 84, 89, 88, 91, 86, 84, 81, 79, 77, 86, 84, 86, 87, 84, 82, 80, 86, 81, 85, 81]\n",
            "[71, 72, 67, 60, 65, 62, 69, 64, 67, 66, 62, 67, 72, 69, 71, 67, 60, 68, 62, 66, 61, 63, 66, 58]\n",
            "Epoch [161/500], Loss: 139.5382537841797, Learning Rate: 3e-05\n",
            "[86, 86, 84, 84, 84, 84, 86, 88, 86, 86, 88, 88, 83, 83, 81, 81, 93, 94, 96, 92, 95, 94, 95, 98]\n",
            "[65, 67, 60, 60, 64, 65, 65, 62, 67, 67, 68, 69, 62, 64, 57, 57, 71, 75, 73, 72, 72, 70, 71, 71]\n",
            "Epoch [162/500], Loss: 130.72982788085938, Learning Rate: 3e-05\n",
            "[75, 73, 72, 72, 70, 70, 81, 77, 77, 75, 77, 79, 65, 79, 77, 77, 91, 90, 87, 86, 87, 86, 85, 82]\n",
            "[61, 61, 63, 60, 58, 58, 58, 58, 58, 58, 61, 61, 65, 65, 56, 58, 63, 74, 64, 63, 66, 68, 65, 65]\n",
            "Epoch [163/500], Loss: 152.56912231445312, Learning Rate: 3e-05\n",
            "[91, 89, 86, 65, 87, 91, 89, 87, 86, 87, 87, 87, 94, 96, 94, 91, 88, 89, 89, 90, 88, 89, 90, 88]\n",
            "[60, 72, 71, 72, 72, 75, 68, 70, 58, 63, 63, 63, 75, 68, 70, 71, 70, 69, 70, 69, 69, 68, 67, 68]\n",
            "Epoch [164/500], Loss: 128.32321166992188, Learning Rate: 2e-05\n",
            "[84, 84, 83, 84, 86, 87, 86, 84, 84, 86, 86, 91, 89, 87, 87, 86, 94, 96, 100, 95, 94, 94, 99, 97]\n",
            "[72, 68, 67, 63, 62, 60, 55, 60, 72, 70, 67, 63, 63, 60, 63, 67, 72, 74, 73, 78, 68, 76, 74, 68]\n",
            "Epoch [165/500], Loss: 100.47308349609375, Learning Rate: 2e-05\n",
            "[89, 89, 87, 86, 86, 86, 84, 84, 84, 86, 86, 84, 82, 82, 79, 82, 66, 59, 61, 66, 64, 62, 67, 64]\n",
            "[68, 68, 67, 65, 67, 55, 60, 60, 72, 70, 70, 65, 67, 67, 63, 62, 51, 49, 49, 51, 49, 50, 52, 52]\n",
            "Epoch [166/500], Loss: 124.1771011352539, Learning Rate: 2e-05\n",
            "[84, 86, 91, 84, 84, 91, 89, 87, 86, 72, 86, 86, 87, 89, 91, 87, 93, 93, 90, 91, 92, 90, 89, 93]\n",
            "[72, 72, 72, 60, 60, 60, 62, 63, 67, 68, 67, 67, 67, 67, 63, 75, 69, 74, 71, 68, 75, 74, 68, 73]\n",
            "Epoch [167/500], Loss: 122.72418975830078, Learning Rate: 2e-05\n",
            "[86, 86, 86, 86, 88, 84, 84, 84, 84, 83, 84, 88, 86, 84, 84, 84, 68, 62, 67, 62, 65, 63, 65, 61]\n",
            "[67, 71, 74, 68, 68, 69, 65, 65, 66, 67, 69, 67, 67, 60, 60, 60, 45, 41, 46, 46, 43, 50, 49, 45]\n",
            "Epoch [168/500], Loss: 102.770263671875, Learning Rate: 2e-05\n",
            "[86, 84, 84, 84, 88, 86, 88, 84, 67, 83, 84, 86, 88, 84, 84, 84, 94, 95, 95, 92, 93, 92, 92, 90]\n",
            "[67, 60, 60, 60, 60, 67, 68, 69, 74, 71, 69, 67, 64, 69, 69, 72, 71, 75, 68, 72, 74, 72, 70, 68]\n",
            "Epoch [169/500], Loss: 90.88335418701172, Learning Rate: 2e-05\n",
            "[79, 79, 77, 77, 77, 75, 75, 75, 74, 74, 74, 83, 74, 71, 72, 72, 82, 79, 79, 81, 84, 77, 81, 79]\n",
            "[62, 60, 60, 59, 55, 60, 51, 48, 55, 55, 55, 56, 53, 55, 60, 60, 60, 62, 64, 65, 62, 61, 62, 60]\n",
            "Epoch [170/500], Loss: 137.90383911132812, Learning Rate: 2e-05\n",
            "[84, 79, 88, 84, 86, 80, 86, 84, 87, 86, 84, 86, 87, 89, 91, 91, 87, 85, 86, 85, 87, 87, 86, 85]\n",
            "[60, 63, 67, 69, 71, 72, 67, 60, 60, 60, 72, 65, 60, 56, 55, 55, 66, 67, 66, 68, 66, 65, 68, 63]\n",
            "Epoch [171/500], Loss: 65.58287048339844, Learning Rate: 2e-05\n",
            "[76, 77, 79, 81, 81, 93, 79, 79, 79, 77, 77, 77, 79, 76, 79, 81, 93, 93, 93, 86, 95, 95, 90, 91]\n",
            "[60, 57, 55, 53, 55, 57, 58, 55, 60, 53, 53, 53, 59, 60, 52, 53, 70, 79, 67, 69, 68, 65, 70, 64]\n",
            "Epoch [172/500], Loss: 108.67579650878906, Learning Rate: 2e-05\n",
            "[83, 84, 83, 81, 81, 79, 79, 79, 79, 81, 83, 84, 79, 74, 79, 77, 85, 86, 81, 81, 87, 83, 80, 80]\n",
            "[55, 64, 62, 60, 62, 55, 55, 55, 64, 65, 56, 57, 60, 65, 65, 62, 65, 64, 65, 60, 64, 64, 63, 62]\n",
            "Epoch [173/500], Loss: 80.13363647460938, Learning Rate: 2e-05\n",
            "[91, 91, 94, 92, 91, 89, 89, 91, 91, 91, 91, 91, 89, 87, 87, 95, 90, 94, 92, 91, 93, 92, 89, 89]\n",
            "[75, 72, 68, 65, 63, 68, 70, 63, 60, 63, 67, 71, 74, 72, 75, 79, 66, 66, 72, 71, 68, 75, 66, 69]\n",
            "Epoch [174/500], Loss: 97.32364654541016, Learning Rate: 2e-05\n",
            "[88, 88, 86, 86, 89, 91, 93, 93, 91, 91, 89, 89, 66, 89, 91, 91, 68, 67, 69, 68, 68, 67, 67, 66]\n",
            "[69, 69, 62, 62, 74, 72, 69, 65, 72, 60, 65, 65, 72, 69, 71, 76, 55, 45, 52, 50, 54, 54, 52, 53]\n",
            "Epoch [175/500], Loss: 118.77041625976562, Learning Rate: 2e-05\n",
            "[77, 79, 88, 77, 89, 73, 72, 72, 73, 75, 77, 77, 75, 68, 72, 72, 85, 83, 88, 80, 85, 79, 81, 86]\n",
            "[65, 64, 65, 61, 57, 58, 53, 53, 58, 54, 50, 50, 51, 52, 53, 53, 63, 59, 68, 65, 65, 58, 65, 60]\n",
            "Epoch [176/500], Loss: 96.484619140625, Learning Rate: 2e-05\n",
            "[84, 88, 84, 81, 86, 84, 84, 83, 67, 88, 88, 93, 91, 91, 69, 91, 98, 99, 95, 92, 95, 97, 92, 91]\n",
            "[60, 64, 60, 65, 62, 64, 66, 67, 72, 72, 69, 66, 67, 71, 74, 67, 70, 67, 75, 75, 69, 74, 70, 66]\n",
            "Epoch [177/500], Loss: 96.4286117553711, Learning Rate: 2e-05\n",
            "[75, 75, 75, 65, 77, 75, 75, 73, 72, 72, 72, 72, 79, 79, 80, 80, 85, 84, 84, 85, 83, 85, 84, 84]\n",
            "[51, 51, 56, 53, 49, 54, 56, 58, 58, 56, 60, 60, 48, 60, 60, 58, 66, 65, 66, 64, 63, 64, 64, 64]\n",
            "Epoch [178/500], Loss: 107.64144897460938, Learning Rate: 2e-05\n",
            "[86, 91, 91, 91, 90, 91, 91, 91, 91, 91, 91, 91, 94, 91, 89, 86, 75, 77, 76, 71, 81, 80, 72, 71]\n",
            "[67, 72, 71, 74, 62, 67, 67, 67, 67, 64, 67, 71, 72, 64, 65, 67, 59, 55, 60, 57, 58, 63, 58, 58]\n",
            "Epoch [179/500], Loss: 96.62335205078125, Learning Rate: 2e-05\n",
            "[74, 74, 74, 83, 74, 71, 72, 72, 72, 77, 77, 77, 79, 79, 79, 75, 90, 90, 89, 87, 93, 87, 93, 94]\n",
            "[55, 55, 55, 56, 53, 55, 60, 60, 60, 57, 58, 55, 51, 63, 60, 60, 64, 68, 71, 73, 69, 68, 68, 66]\n",
            "Epoch [180/500], Loss: 97.63243865966797, Learning Rate: 2e-05\n",
            "[88, 88, 91, 89, 88, 81, 88, 86, 88, 84, 86, 89, 88, 78, 88, 89, 85, 84, 83, 85, 87, 80, 84, 86]\n",
            "[72, 69, 65, 62, 67, 64, 60, 67, 60, 65, 65, 62, 69, 70, 67, 65, 60, 64, 66, 64, 62, 65, 65, 61]\n",
            "Epoch [181/500], Loss: 101.06067657470703, Learning Rate: 2e-05\n",
            "[79, 79, 77, 75, 72, 71, 79, 79, 75, 82, 88, 67, 77, 79, 77, 75, 92, 95, 97, 96, 97, 95, 95, 97]\n",
            "[59, 59, 57, 60, 60, 63, 63, 59, 60, 62, 63, 65, 68, 67, 59, 60, 73, 76, 75, 72, 71, 71, 71, 70]\n",
            "Epoch [182/500], Loss: 103.90509796142578, Learning Rate: 2e-05\n",
            "[79, 79, 83, 83, 84, 81, 81, 84, 83, 83, 81, 80, 80, 76, 81, 81, 93, 89, 86, 84, 90, 83, 82, 85]\n",
            "[55, 55, 67, 65, 64, 65, 53, 64, 62, 60, 62, 64, 52, 62, 60, 59, 67, 65, 69, 65, 67, 65, 66, 62]\n",
            "Epoch [183/500], Loss: 99.01801300048828, Learning Rate: 2e-05\n",
            "[85, 87, 86, 84, 83, 84, 86, 86, 91, 91, 86, 87, 89, 87, 86, 84, 92, 94, 92, 91, 90, 94, 90, 87]\n",
            "[72, 68, 65, 68, 62, 63, 67, 67, 60, 63, 67, 72, 68, 67, 67, 60, 68, 75, 66, 66, 70, 69, 67, 65]\n",
            "Epoch [184/500], Loss: 93.46762084960938, Learning Rate: 2e-05\n",
            "[76, 72, 72, 84, 76, 74, 74, 72, 72, 76, 77, 91, 79, 81, 93, 84, 69, 60, 62, 68, 66, 71, 64, 70]\n",
            "[56, 57, 60, 62, 57, 53, 55, 48, 60, 60, 62, 64, 60, 65, 67, 60, 50, 47, 50, 51, 46, 49, 51, 49]\n",
            "Epoch [185/500], Loss: 124.84676361083984, Learning Rate: 2e-05\n",
            "[77, 84, 75, 74, 92, 75, 77, 79, 75, 77, 80, 79, 80, 77, 76, 77, 76, 72, 74, 78, 74, 78, 76, 76]\n",
            "[62, 63, 60, 67, 63, 60, 56, 55, 60, 61, 56, 60, 53, 56, 60, 53, 56, 57, 55, 56, 55, 59, 59, 55]\n",
            "Epoch [186/500], Loss: 127.56690216064453, Learning Rate: 2e-05\n",
            "[86, 84, 83, 89, 81, 72, 65, 89, 85, 84, 86, 88, 91, 91, 89, 88, 80, 80, 85, 85, 80, 86, 81, 83]\n",
            "[67, 69, 71, 74, 62, 67, 67, 67, 64, 69, 67, 72, 72, 69, 62, 64, 66, 62, 63, 64, 61, 65, 61, 64]\n",
            "Epoch [187/500], Loss: 112.05640411376953, Learning Rate: 2e-05\n",
            "[79, 75, 77, 79, 77, 67, 74, 72, 72, 72, 75, 77, 87, 75, 77, 79, 76, 72, 78, 77, 74, 76, 76, 74]\n",
            "[59, 60, 62, 59, 60, 67, 55, 60, 60, 60, 60, 56, 53, 60, 56, 51, 59, 57, 60, 59, 57, 58, 59, 58]\n",
            "Epoch [188/500], Loss: 134.2400360107422, Learning Rate: 2e-05\n",
            "[78, 79, 81, 79, 79, 79, 79, 79, 82, 81, 79, 77, 79, 76, 72, 76, 80, 75, 79, 79, 79, 73, 78, 80]\n",
            "[60, 59, 62, 55, 60, 60, 59, 60, 52, 53, 55, 58, 59, 60, 60, 57, 56, 61, 63, 57, 56, 59, 62, 57]\n",
            "Epoch [189/500], Loss: 101.09489440917969, Learning Rate: 2e-05\n",
            "[82, 81, 79, 77, 79, 70, 72, 76, 78, 79, 81, 79, 79, 79, 79, 79, 90, 96, 90, 86, 89, 91, 84, 82]\n",
            "[55, 57, 60, 62, 59, 60, 48, 60, 57, 55, 50, 55, 60, 52, 55, 48, 69, 62, 67, 67, 70, 72, 65, 68]\n",
            "Epoch [190/500], Loss: 106.34058380126953, Learning Rate: 2e-05\n",
            "[84, 84, 84, 86, 82, 82, 81, 82, 82, 84, 86, 87, 86, 84, 84, 86, 94, 97, 94, 94, 96, 93, 95, 95]\n",
            "[72, 69, 65, 70, 67, 64, 65, 58, 63, 72, 74, 72, 70, 68, 68, 67, 69, 74, 73, 75, 73, 74, 70, 72]\n",
            "Epoch [191/500], Loss: 88.71569061279297, Learning Rate: 2e-05\n",
            "[88, 91, 91, 86, 88, 84, 84, 83, 79, 84, 86, 88, 88, 86, 86, 86, 87, 90, 87, 83, 87, 90, 84, 86]\n",
            "[69, 64, 64, 67, 64, 69, 69, 67, 65, 64, 62, 60, 64, 67, 67, 67, 66, 62, 65, 62, 66, 67, 65, 62]\n",
            "Epoch [192/500], Loss: 122.57292938232422, Learning Rate: 2e-05\n",
            "[89, 88, 86, 86, 84, 84, 84, 84, 84, 86, 88, 88, 86, 84, 84, 83, 91, 94, 90, 83, 100, 86, 88, 91]\n",
            "[69, 67, 65, 67, 60, 60, 60, 72, 72, 67, 60, 60, 62, 64, 66, 67, 68, 74, 72, 67, 70, 69, 70, 68]\n",
            "Epoch [193/500], Loss: 124.44662475585938, Learning Rate: 2e-05\n",
            "[83, 83, 81, 81, 81, 81, 79, 81, 84, 79, 79, 76, 77, 93, 77, 76, 74, 69, 72, 75, 73, 73, 75, 75]\n",
            "[64, 67, 62, 69, 57, 62, 64, 65, 64, 64, 67, 60, 57, 59, 55, 60, 59, 57, 57, 58, 56, 57, 58, 58]\n",
            "Epoch [194/500], Loss: 136.27500915527344, Learning Rate: 2e-05\n",
            "[79, 79, 78, 79, 88, 86, 84, 83, 88, 89, 86, 94, 84, 84, 83, 90, 87, 82, 83, 81, 83, 84, 84, 83]\n",
            "[64, 61, 62, 55, 60, 59, 57, 64, 68, 69, 71, 72, 64, 66, 68, 69, 60, 69, 59, 64, 63, 58, 66, 54]\n",
            "Epoch [195/500], Loss: 79.54395294189453, Learning Rate: 2e-05\n",
            "[84, 84, 82, 80, 79, 79, 84, 82, 84, 86, 87, 84, 73, 78, 87, 86, 91, 92, 95, 87, 95, 93, 92, 94]\n",
            "[68, 65, 62, 60, 63, 63, 63, 67, 68, 65, 63, 65, 67, 67, 67, 70, 70, 74, 71, 71, 74, 71, 72, 70]\n",
            "Epoch [196/500], Loss: 93.26006317138672, Learning Rate: 2e-05\n",
            "[69, 74, 72, 72, 79, 79, 81, 83, 84, 84, 83, 83, 84, 87, 86, 86, 89, 90, 91, 90, 89, 89, 86, 88]\n",
            "[67, 55, 60, 60, 60, 63, 65, 63, 63, 68, 67, 67, 63, 60, 65, 67, 68, 69, 68, 69, 70, 69, 66, 69]\n",
            "Epoch [197/500], Loss: 104.09075164794922, Learning Rate: 2e-05\n",
            "[84, 87, 80, 79, 74, 82, 79, 79, 87, 86, 91, 89, 86, 87, 83, 83, 77, 74, 74, 77, 75, 75, 75, 74]\n",
            "[63, 60, 65, 67, 70, 58, 63, 63, 60, 62, 63, 60, 65, 67, 67, 67, 58, 58, 58, 60, 57, 55, 59, 56]\n",
            "Epoch [198/500], Loss: 87.67300415039062, Learning Rate: 2e-05\n",
            "[88, 91, 91, 86, 88, 84, 84, 83, 79, 84, 86, 88, 89, 86, 86, 86, 81, 82, 83, 83, 80, 81, 83, 84]\n",
            "[69, 64, 64, 67, 64, 69, 62, 67, 65, 64, 62, 60, 65, 67, 67, 67, 67, 66, 64, 63, 68, 61, 63, 67]\n",
            "Epoch [199/500], Loss: 76.82577514648438, Learning Rate: 2e-05\n",
            "[83, 83, 84, 84, 86, 86, 84, 84, 93, 81, 81, 81, 79, 79, 79, 79, 80, 85, 83, 80, 82, 85, 80, 80]\n",
            "[62, 55, 60, 60, 67, 71, 69, 66, 67, 60, 62, 62, 55, 55, 60, 64, 64, 62, 58, 60, 65, 59, 62, 61]\n",
            "Epoch [200/500], Loss: 105.47223663330078, Learning Rate: 2e-05\n",
            "[83, 83, 83, 83, 67, 95, 86, 86, 84, 84, 83, 83, 83, 81, 84, 84, 85, 87, 91, 88, 89, 92, 92, 91]\n",
            "[67, 67, 67, 67, 72, 72, 65, 65, 68, 68, 67, 67, 65, 65, 63, 63, 68, 68, 66, 69, 68, 67, 69, 65]\n",
            "Epoch [201/500], Loss: 69.29869842529297, Learning Rate: 2e-05\n",
            "[91, 91, 89, 87, 89, 89, 87, 87, 87, 89, 91, 71, 89, 87, 89, 89, 82, 83, 86, 80, 82, 79, 81, 78]\n",
            "[67, 63, 63, 63, 68, 70, 63, 63, 63, 63, 63, 75, 70, 72, 68, 70, 60, 61, 58, 61, 60, 60, 61, 57]\n",
            "Epoch [202/500], Loss: 108.7933120727539, Learning Rate: 2e-05\n",
            "[84, 66, 81, 83, 84, 86, 75, 84, 66, 91, 91, 66, 91, 70, 89, 88, 86, 87, 80, 82, 84, 82, 80, 78]\n",
            "[60, 64, 66, 68, 69, 71, 72, 65, 72, 71, 72, 72, 73, 74, 62, 69, 66, 67, 61, 59, 63, 62, 60, 62]\n",
            "Epoch [203/500], Loss: 96.00849914550781, Learning Rate: 2e-05\n",
            "[84, 84, 84, 84, 89, 89, 91, 91, 91, 91, 86, 86, 89, 89, 87, 92, 86, 87, 90, 83, 88, 87, 88, 84]\n",
            "[60, 60, 60, 60, 62, 62, 63, 63, 63, 63, 70, 70, 71, 71, 72, 72, 66, 67, 65, 69, 67, 69, 68, 63]\n",
            "Epoch [204/500], Loss: 92.06735229492188, Learning Rate: 1e-05\n",
            "[79, 92, 77, 76, 76, 74, 76, 87, 74, 72, 72, 72, 74, 74, 76, 77, 89, 92, 89, 93, 94, 93, 89, 94]\n",
            "[55, 57, 59, 60, 60, 65, 62, 67, 55, 60, 60, 60, 59, 55, 60, 62, 70, 71, 69, 64, 71, 67, 64, 72]\n",
            "Epoch [205/500], Loss: 117.53058624267578, Learning Rate: 1e-05\n",
            "[79, 78, 76, 72, 76, 74, 74, 74, 76, 94, 74, 77, 77, 77, 76, 74, 75, 74, 71, 73, 74, 68, 65, 63]\n",
            "[52, 53, 55, 57, 52, 55, 55, 55, 60, 65, 64, 62, 62, 62, 57, 58, 52, 45, 56, 51, 53, 62, 53, 53]\n",
            "Epoch [206/500], Loss: 112.49433135986328, Learning Rate: 1e-05\n",
            "[87, 87, 87, 86, 86, 86, 91, 91, 89, 87, 87, 86, 82, 86, 88, 89, 72, 67, 70, 71, 71, 70, 69, 69]\n",
            "[60, 60, 63, 67, 67, 67, 67, 71, 71, 72, 72, 72, 68, 68, 68, 65, 52, 52, 57, 52, 50, 55, 56, 51]\n",
            "Epoch [207/500], Loss: 115.49786376953125, Learning Rate: 1e-05\n",
            "[83, 83, 83, 83, 83, 84, 84, 84, 88, 77, 86, 84, 86, 86, 88, 89, 72, 72, 72, 67, 75, 72, 69, 64]\n",
            "[62, 64, 67, 67, 64, 60, 60, 60, 60, 72, 71, 69, 71, 69, 67, 69, 54, 58, 52, 53, 57, 56, 56, 56]\n",
            "Epoch [208/500], Loss: 81.98310852050781, Learning Rate: 1e-05\n",
            "[91, 89, 89, 87, 87, 83, 91, 87, 89, 87, 86, 70, 90, 82, 84, 87, 73, 77, 69, 70, 83, 69, 72, 72]\n",
            "[67, 70, 58, 63, 63, 75, 72, 72, 68, 68, 65, 68, 68, 63, 68, 67, 59, 54, 56, 49, 60, 55, 57, 56]\n",
            "Epoch [209/500], Loss: 79.79426574707031, Learning Rate: 1e-05\n",
            "[76, 74, 76, 76, 76, 80, 79, 77, 76, 76, 74, 72, 72, 71, 72, 72, 91, 95, 93, 90, 93, 96, 94, 91]\n",
            "[57, 53, 52, 52, 57, 64, 64, 60, 60, 60, 57, 57, 53, 55, 57, 57, 70, 74, 70, 71, 70, 72, 70, 66]\n",
            "Epoch [210/500], Loss: 122.90613555908203, Learning Rate: 1e-05\n",
            "[88, 91, 91, 88, 91, 89, 89, 88, 88, 89, 88, 86, 84, 79, 88, 84, 85, 82, 83, 83, 85, 81, 86, 84]\n",
            "[69, 71, 67, 72, 64, 62, 67, 60, 60, 62, 64, 67, 69, 72, 60, 65, 62, 63, 66, 65, 64, 62, 65, 60]\n",
            "Epoch [211/500], Loss: 101.0329818725586, Learning Rate: 1e-05\n",
            "[89, 88, 91, 79, 91, 88, 86, 84, 84, 89, 88, 91, 86, 84, 81, 79, 88, 90, 91, 87, 95, 91, 92, 95]\n",
            "[74, 69, 76, 77, 74, 72, 67, 72, 69, 74, 72, 64, 67, 60, 62, 67, 69, 71, 69, 71, 69, 65, 70, 68]\n",
            "Epoch [212/500], Loss: 87.37765502929688, Learning Rate: 1e-05\n",
            "[81, 81, 83, 84, 81, 81, 79, 79, 88, 88, 91, 92, 86, 86, 86, 86, 77, 77, 79, 76, 78, 79, 71, 72]\n",
            "[62, 62, 63, 64, 60, 62, 67, 67, 68, 69, 71, 72, 72, 72, 71, 71, 61, 54, 59, 56, 58, 63, 57, 59]\n",
            "Epoch [213/500], Loss: 129.8668975830078, Learning Rate: 1e-05\n",
            "[85, 86, 86, 86, 74, 84, 86, 88, 89, 91, 89, 86, 89, 88, 86, 91, 77, 75, 79, 77, 75, 80, 81, 82]\n",
            "[69, 62, 62, 62, 64, 69, 67, 64, 62, 60, 64, 67, 62, 69, 72, 71, 62, 56, 56, 58, 61, 55, 57, 60]\n",
            "Epoch [214/500], Loss: 110.23949432373047, Learning Rate: 1e-05\n",
            "[83, 84, 84, 84, 86, 86, 88, 89, 89, 89, 88, 88, 88, 86, 86, 86, 88, 86, 85, 80, 89, 86, 84, 86]\n",
            "[67, 60, 60, 60, 67, 67, 64, 62, 62, 65, 69, 69, 69, 62, 62, 62, 68, 74, 69, 65, 65, 65, 68, 62]\n",
            "Epoch [215/500], Loss: 106.73263549804688, Learning Rate: 1e-05\n",
            "[82, 81, 81, 79, 76, 81, 79, 77, 78, 74, 74, 72, 72, 79, 79, 76, 77, 81, 79, 80, 80, 76, 80, 78]\n",
            "[67, 62, 50, 55, 60, 53, 55, 57, 60, 55, 55, 60, 60, 59, 59, 60, 61, 63, 60, 58, 63, 59, 60, 63]\n",
            "Epoch [216/500], Loss: 84.79315948486328, Learning Rate: 1e-05\n",
            "[82, 80, 79, 77, 77, 80, 82, 84, 82, 87, 86, 84, 82, 86, 86, 84, 88, 88, 82, 85, 93, 86, 86, 91]\n",
            "[67, 65, 60, 53, 65, 65, 63, 68, 63, 60, 65, 72, 67, 72, 70, 68, 64, 66, 67, 67, 63, 65, 65, 64]\n",
            "Epoch [217/500], Loss: 107.34204864501953, Learning Rate: 1e-05\n",
            "[79, 94, 75, 81, 75, 74, 72, 72, 75, 75, 75, 72, 75, 77, 79, 77, 87, 86, 82, 88, 84, 78, 87, 83]\n",
            "[55, 63, 60, 56, 54, 55, 48, 48, 60, 56, 55, 56, 56, 53, 51, 53, 65, 69, 67, 66, 66, 68, 66, 66]\n",
            "Epoch [218/500], Loss: 90.85699462890625, Learning Rate: 1e-05\n",
            "[87, 86, 86, 84, 91, 92, 94, 87, 89, 91, 89, 89, 89, 87, 87, 87, 72, 75, 68, 73, 77, 71, 70, 69]\n",
            "[72, 65, 67, 60, 63, 65, 67, 60, 62, 63, 68, 65, 70, 63, 63, 63, 55, 55, 52, 53, 56, 55, 56, 58]\n",
            "Epoch [219/500], Loss: 98.0107650756836, Learning Rate: 1e-05\n",
            "[81, 79, 79, 84, 84, 86, 86, 88, 88, 84, 84, 89, 89, 88, 88, 86, 89, 86, 84, 83, 93, 85, 82, 90]\n",
            "[62, 55, 67, 64, 69, 65, 67, 60, 60, 65, 64, 62, 74, 67, 69, 62, 65, 69, 67, 63, 68, 64, 65, 67]\n",
            "Epoch [220/500], Loss: 103.93793487548828, Learning Rate: 1e-05\n",
            "[86, 88, 89, 89, 88, 88, 84, 86, 88, 71, 89, 88, 89, 89, 91, 91, 64, 64, 62, 65, 62, 64, 69, 61]\n",
            "[65, 62, 62, 65, 69, 69, 69, 70, 70, 74, 70, 72, 65, 65, 64, 67, 53, 47, 48, 49, 47, 50, 50, 50]\n",
            "Epoch [221/500], Loss: 70.84709167480469, Learning Rate: 1e-05\n",
            "[79, 82, 83, 84, 86, 87, 84, 87, 86, 84, 84, 83, 79, 84, 87, 86, 91, 92, 91, 94, 89, 92, 92, 92]\n",
            "[63, 63, 62, 60, 59, 60, 60, 60, 67, 63, 68, 67, 67, 63, 60, 65, 68, 72, 69, 69, 71, 69, 69, 65]\n",
            "Epoch [222/500], Loss: 83.66279602050781, Learning Rate: 1e-05\n",
            "[92, 92, 91, 89, 91, 91, 89, 89, 69, 86, 87, 87, 89, 89, 91, 91, 82, 80, 83, 82, 79, 79, 77, 75]\n",
            "[65, 68, 72, 73, 70, 72, 65, 65, 77, 71, 72, 67, 68, 70, 63, 63, 58, 58, 64, 59, 59, 67, 59, 63]\n",
            "Epoch [223/500], Loss: 88.60836791992188, Learning Rate: 1e-05\n",
            "[86, 84, 86, 88, 86, 84, 84, 86, 81, 88, 86, 84, 84, 83, 83, 84, 89, 88, 84, 87, 87, 81, 84, 86]\n",
            "[67, 69, 71, 72, 67, 69, 69, 67, 71, 72, 67, 69, 64, 67, 67, 60, 65, 69, 70, 64, 72, 69, 63, 71]\n",
            "Epoch [224/500], Loss: 86.94378662109375, Learning Rate: 1e-05\n",
            "[87, 87, 86, 87, 87, 86, 84, 89, 89, 86, 87, 86, 86, 84, 84, 84, 83, 85, 86, 82, 84, 81, 81, 84]\n",
            "[67, 70, 58, 63, 63, 70, 65, 68, 72, 67, 63, 67, 67, 60, 60, 60, 66, 60, 62, 60, 67, 65, 63, 62]\n",
            "Epoch [225/500], Loss: 83.12117004394531, Learning Rate: 1e-05\n",
            "[87, 87, 86, 86, 84, 84, 87, 87, 89, 89, 86, 86, 86, 86, 84, 84, 87, 86, 87, 89, 85, 85, 87, 87]\n",
            "[60, 60, 67, 67, 68, 68, 67, 67, 65, 65, 67, 67, 67, 67, 60, 60, 69, 67, 70, 68, 68, 66, 65, 66]\n",
            "Epoch [226/500], Loss: 73.97561645507812, Learning Rate: 1e-05\n",
            "[84, 83, 81, 79, 81, 77, 67, 76, 89, 79, 81, 79, 84, 79, 79, 85, 75, 78, 77, 73, 77, 76, 71, 68]\n",
            "[57, 59, 60, 64, 65, 62, 55, 60, 60, 64, 62, 59, 62, 55, 55, 64, 56, 51, 55, 54, 59, 62, 56, 58]\n",
            "Epoch [227/500], Loss: 100.21139526367188, Learning Rate: 1e-05\n",
            "[84, 87, 86, 84, 84, 82, 80, 79, 69, 81, 83, 84, 87, 86, 86, 84, 87, 95, 87, 86, 90, 88, 91, 89]\n",
            "[60, 72, 67, 68, 65, 62, 65, 60, 63, 63, 62, 65, 60, 65, 67, 60, 70, 68, 70, 66, 69, 67, 68, 66]\n",
            "Epoch [228/500], Loss: 128.27684020996094, Learning Rate: 1e-05\n",
            "[79, 78, 79, 82, 84, 82, 81, 79, 79, 75, 77, 86, 77, 74, 74, 72, 76, 77, 74, 78, 73, 75, 77, 74]\n",
            "[60, 62, 58, 55, 54, 55, 62, 55, 59, 60, 62, 63, 60, 53, 55, 60, 63, 61, 57, 58, 63, 57, 58, 62]\n",
            "Epoch [229/500], Loss: 95.43721008300781, Learning Rate: 1e-05\n",
            "[72, 79, 93, 72, 79, 81, 81, 79, 79, 81, 83, 84, 83, 81, 83, 79, 85, 89, 88, 85, 90, 85, 89, 85]\n",
            "[60, 59, 60, 57, 64, 60, 62, 55, 60, 65, 64, 64, 67, 60, 62, 55, 63, 69, 61, 67, 67, 66, 66, 63]\n",
            "Epoch [230/500], Loss: 89.8072280883789, Learning Rate: 1e-05\n",
            "[82, 76, 86, 84, 82, 80, 81, 77, 77, 80, 82, 84, 82, 87, 86, 84, 85, 83, 85, 82, 85, 88, 81, 85]\n",
            "[67, 72, 70, 68, 62, 65, 60, 65, 65, 61, 58, 56, 63, 60, 67, 72, 62, 68, 63, 61, 63, 63, 64, 62]\n",
            "Epoch [231/500], Loss: 87.1827621459961, Learning Rate: 1e-05\n",
            "[75, 87, 74, 72, 72, 72, 77, 75, 77, 92, 75, 77, 77, 79, 79, 80, 96, 94, 94, 93, 91, 92, 92, 90]\n",
            "[63, 67, 55, 60, 60, 63, 62, 60, 62, 63, 60, 56, 58, 51, 51, 53, 69, 77, 70, 69, 69, 70, 69, 66]\n",
            "Epoch [232/500], Loss: 74.56458282470703, Learning Rate: 1e-05\n",
            "[87, 86, 84, 84, 83, 84, 84, 84, 87, 87, 87, 86, 87, 84, 84, 83, 82, 85, 83, 83, 88, 79, 86, 85]\n",
            "[60, 65, 66, 67, 55, 60, 60, 60, 60, 60, 63, 67, 63, 68, 68, 67, 62, 70, 61, 64, 63, 58, 62, 63]\n",
            "Epoch [233/500], Loss: 108.8703384399414, Learning Rate: 1e-05\n",
            "[79, 79, 67, 77, 70, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 85, 86, 87, 85, 89, 86, 87, 87]\n",
            "[58, 58, 56, 56, 68, 68, 67, 67, 67, 67, 67, 67, 60, 60, 63, 63, 65, 68, 66, 66, 62, 65, 65, 64]\n",
            "Epoch [234/500], Loss: 68.81553649902344, Learning Rate: 1e-05\n",
            "[87, 87, 86, 84, 72, 82, 84, 87, 91, 89, 87, 87, 86, 87, 87, 87, 86, 89, 84, 86, 90, 82, 84, 86]\n",
            "[72, 65, 67, 60, 72, 67, 68, 67, 63, 70, 72, 70, 70, 63, 63, 63, 64, 63, 64, 62, 64, 65, 64, 65]\n",
            "Epoch [235/500], Loss: 79.14832305908203, Learning Rate: 1e-05\n",
            "[91, 91, 91, 89, 89, 89, 91, 89, 87, 86, 86, 86, 86, 86, 88, 89, 90, 86, 86, 86, 88, 80, 88, 87]\n",
            "[72, 67, 63, 70, 74, 72, 71, 71, 72, 67, 67, 67, 67, 67, 70, 68, 66, 70, 69, 67, 66, 66, 66, 65]\n",
            "Epoch [236/500], Loss: 84.66453552246094, Learning Rate: 1e-05\n",
            "[85, 84, 86, 88, 91, 91, 89, 88, 86, 88, 88, 88, 88, 91, 93, 91, 86, 91, 90, 90, 90, 88, 89, 89]\n",
            "[64, 69, 67, 72, 72, 69, 62, 64, 65, 64, 64, 64, 69, 76, 74, 71, 67, 70, 66, 67, 68, 66, 66, 67]\n",
            "Epoch [237/500], Loss: 82.96368408203125, Learning Rate: 1e-05\n",
            "[88, 88, 88, 88, 86, 86, 88, 89, 88, 88, 86, 86, 84, 86, 86, 69, 93, 93, 89, 85, 92, 88, 84, 88]\n",
            "[68, 69, 69, 67, 66, 67, 64, 59, 60, 64, 67, 67, 69, 71, 71, 72, 68, 65, 72, 70, 71, 70, 66, 65]\n",
            "Epoch [238/500], Loss: 73.97469329833984, Learning Rate: 1e-05\n",
            "[79, 81, 83, 76, 88, 86, 86, 84, 84, 84, 86, 84, 81, 84, 84, 83, 92, 89, 93, 84, 91, 83, 89, 86]\n",
            "[64, 66, 68, 69, 69, 65, 67, 60, 60, 64, 60, 69, 69, 66, 62, 67, 64, 68, 71, 73, 65, 69, 71, 62]\n",
            "Epoch [239/500], Loss: 67.1911392211914, Learning Rate: 1e-05\n",
            "[72, 72, 75, 75, 77, 78, 79, 79, 79, 82, 80, 79, 77, 77, 75, 75, 60, 53, 58, 61, 57, 59, 61, 58]\n",
            "[60, 63, 60, 56, 53, 50, 51, 63, 51, 55, 53, 51, 56, 58, 51, 51, 49, 43, 45, 48, 46, 46, 49, 48]\n",
            "Epoch [240/500], Loss: 94.14260864257812, Learning Rate: 1e-05\n",
            "[80, 80, 79, 77, 75, 77, 79, 79, 79, 79, 72, 73, 75, 77, 80, 80, 79, 76, 84, 79, 84, 85, 83, 83]\n",
            "[53, 58, 51, 51, 61, 61, 63, 58, 60, 60, 60, 63, 60, 60, 56, 53, 62, 62, 61, 60, 62, 61, 64, 61]\n",
            "Epoch [241/500], Loss: 88.98014068603516, Learning Rate: 1e-05\n",
            "[91, 89, 89, 91, 91, 90, 89, 91, 91, 89, 89, 87, 71, 94, 91, 91, 73, 68, 63, 68, 70, 67, 65, 68]\n",
            "[67, 70, 70, 67, 71, 72, 70, 67, 63, 68, 70, 63, 75, 74, 70, 71, 55, 49, 53, 53, 54, 51, 53, 54]\n",
            "Epoch [242/500], Loss: 62.55696487426758, Learning Rate: 1e-05\n",
            "[84, 84, 79, 82, 81, 81, 79, 79, 79, 82, 82, 79, 81, 77, 74, 75, 88, 91, 89, 87, 89, 89, 90, 87]\n",
            "[63, 62, 60, 62, 60, 62, 55, 55, 60, 62, 58, 63, 57, 53, 58, 55, 68, 68, 66, 69, 69, 69, 68, 66]\n",
            "Epoch [243/500], Loss: 109.43783569335938, Learning Rate: 1e-05\n",
            "[79, 79, 79, 79, 84, 84, 84, 86, 79, 79, 81, 81, 83, 83, 83, 83, 91, 94, 98, 96, 90, 96, 92, 91]\n",
            "[55, 55, 55, 55, 57, 57, 57, 59, 60, 60, 61, 61, 62, 62, 63, 63, 74, 70, 70, 71, 70, 73, 67, 71]\n",
            "Epoch [244/500], Loss: 80.29222869873047, Learning Rate: 1e-05\n",
            "[86, 86, 88, 89, 88, 88, 86, 86, 84, 86, 86, 91, 86, 86, 84, 83, 69, 64, 67, 68, 67, 68, 68, 67]\n",
            "[65, 67, 64, 60, 60, 64, 67, 67, 69, 71, 69, 72, 65, 65, 69, 65, 55, 51, 53, 54, 52, 51, 54, 53]\n",
            "Epoch [245/500], Loss: 75.79644012451172, Learning Rate: 1e-05\n",
            "[72, 72, 79, 79, 82, 79, 79, 77, 80, 80, 79, 79, 77, 77, 75, 75, 82, 82, 82, 78, 86, 85, 80, 82]\n",
            "[60, 60, 63, 60, 62, 63, 68, 65, 65, 58, 63, 63, 57, 58, 60, 62, 63, 68, 64, 60, 61, 59, 64, 59]\n",
            "Epoch [246/500], Loss: 79.87362670898438, Learning Rate: 1e-05\n",
            "[83, 83, 81, 81, 81, 81, 79, 81, 84, 79, 79, 76, 77, 93, 77, 76, 96, 98, 99, 96, 91, 95, 93, 89]\n",
            "[64, 67, 62, 69, 57, 62, 64, 65, 64, 64, 67, 60, 57, 59, 55, 60, 71, 74, 71, 74, 72, 73, 71, 67]\n",
            "Epoch [247/500], Loss: 68.26482391357422, Learning Rate: 1e-05\n",
            "[73, 86, 82, 84, 86, 87, 87, 87, 82, 84, 82, 84, 80, 79, 79, 79, 94, 97, 91, 94, 93, 89, 90, 90]\n",
            "[72, 66, 67, 66, 65, 67, 68, 63, 63, 68, 67, 64, 65, 60, 60, 60, 65, 69, 68, 70, 71, 72, 64, 71]\n",
            "Epoch [248/500], Loss: 105.94996643066406, Learning Rate: 1e-05\n",
            "[75, 70, 72, 72, 76, 77, 79, 79, 77, 76, 84, 94, 76, 76, 74, 89, 86, 87, 85, 86, 88, 84, 86, 89]\n",
            "[69, 69, 57, 57, 64, 67, 64, 61, 62, 64, 67, 67, 60, 60, 65, 65, 66, 69, 70, 64, 68, 69, 65, 71]\n",
            "Epoch [249/500], Loss: 128.63929748535156, Learning Rate: 1e-05\n",
            "[76, 79, 80, 79, 79, 72, 79, 79, 82, 81, 79, 77, 79, 76, 65, 76, 91, 91, 90, 91, 91, 92, 90, 92]\n",
            "[62, 59, 62, 55, 60, 59, 55, 60, 62, 65, 60, 53, 59, 60, 64, 57, 67, 69, 69, 70, 66, 68, 68, 65]\n",
            "Epoch [250/500], Loss: 84.81741333007812, Learning Rate: 1e-05\n",
            "[89, 88, 88, 88, 88, 91, 84, 86, 88, 86, 86, 84, 88, 88, 93, 91, 80, 78, 81, 79, 81, 81, 81, 82]\n",
            "[67, 60, 60, 60, 60, 64, 65, 65, 62, 67, 67, 60, 69, 69, 69, 64, 62, 64, 63, 62, 63, 62, 63, 63]\n",
            "Epoch [251/500], Loss: 138.34671020507812, Learning Rate: 1e-05\n",
            "[83, 81, 81, 81, 76, 78, 79, 77, 76, 74, 74, 76, 83, 84, 84, 83, 94, 94, 91, 91, 93, 94, 90, 92]\n",
            "[52, 57, 57, 57, 60, 65, 64, 62, 60, 53, 55, 60, 56, 57, 60, 64, 69, 75, 69, 66, 70, 68, 68, 67]\n",
            "Epoch [252/500], Loss: 112.39059448242188, Learning Rate: 1e-05\n",
            "[87, 89, 86, 84, 86, 70, 86, 84, 82, 84, 80, 79, 77, 80, 82, 84, 87, 92, 84, 78, 86, 88, 82, 80]\n",
            "[68, 65, 67, 60, 67, 72, 65, 68, 67, 64, 65, 60, 65, 61, 58, 56, 67, 65, 67, 62, 67, 66, 64, 64]\n",
            "Epoch [253/500], Loss: 65.01346588134766, Learning Rate: 1e-05\n",
            "[77, 77, 76, 74, 81, 74, 72, 72, 84, 84, 84, 65, 81, 81, 79, 79, 90, 91, 92, 91, 90, 91, 89, 88]\n",
            "[62, 59, 60, 65, 67, 65, 60, 60, 57, 60, 64, 67, 61, 62, 55, 55, 69, 68, 72, 70, 69, 72, 67, 70]\n",
            "Epoch [254/500], Loss: 81.95077514648438, Learning Rate: 1e-05\n",
            "[87, 86, 89, 75, 79, 82, 84, 86, 84, 84, 83, 84, 91, 91, 91, 86, 65, 62, 63, 65, 64, 62, 65, 64]\n",
            "[60, 67, 68, 62, 63, 63, 68, 65, 65, 67, 67, 60, 60, 63, 67, 68, 51, 46, 52, 49, 51, 48, 50, 50]\n",
            "Epoch [255/500], Loss: 118.71440124511719, Learning Rate: 1e-05\n",
            "[78, 79, 81, 79, 79, 84, 83, 81, 83, 84, 81, 79, 79, 84, 79, 81, 88, 88, 89, 88, 87, 88, 88, 87]\n",
            "[60, 59, 62, 55, 60, 57, 55, 62, 65, 64, 62, 55, 60, 64, 67, 67, 67, 68, 67, 70, 66, 67, 67, 66]\n",
            "Epoch [256/500], Loss: 126.62989044189453, Learning Rate: 1e-05\n",
            "[89, 89, 87, 87, 86, 87, 87, 87, 86, 87, 89, 68, 91, 89, 89, 86, 85, 81, 87, 85, 80, 83, 83, 84]\n",
            "[72, 68, 69, 70, 58, 63, 63, 63, 70, 72, 74, 75, 72, 69, 69, 70, 65, 61, 65, 62, 61, 64, 62, 63]\n",
            "Epoch [257/500], Loss: 78.94935607910156, Learning Rate: 1e-05\n",
            "[95, 79, 77, 76, 68, 79, 77, 76, 66, 77, 77, 72, 79, 79, 79, 81, 81, 84, 80, 78, 80, 79, 80, 77]\n",
            "[70, 67, 62, 57, 64, 67, 62, 64, 67, 65, 67, 60, 59, 60, 57, 53, 61, 68, 58, 58, 66, 56, 62, 59]\n",
            "Epoch [258/500], Loss: 99.09000396728516, Learning Rate: 1e-05\n",
            "[84, 83, 81, 79, 81, 77, 67, 76, 89, 79, 81, 79, 84, 79, 79, 85, 83, 83, 85, 85, 82, 82, 84, 84]\n",
            "[57, 59, 60, 64, 65, 62, 55, 60, 60, 64, 62, 59, 62, 55, 55, 64, 65, 65, 63, 64, 64, 68, 65, 65]\n",
            "Epoch [259/500], Loss: 62.77961730957031, Learning Rate: 1e-05\n",
            "[72, 72, 72, 76, 74, 74, 72, 79, 79, 78, 79, 84, 84, 83, 81, 81, 84, 86, 86, 85, 86, 83, 83, 84]\n",
            "[48, 48, 48, 60, 55, 55, 57, 59, 60, 62, 64, 64, 66, 67, 60, 62, 63, 60, 69, 69, 65, 67, 62, 63]\n",
            "Epoch [260/500], Loss: 67.02568817138672, Learning Rate: 1e-05\n",
            "[79, 79, 75, 79, 77, 75, 74, 74, 72, 72, 72, 72, 84, 90, 84, 84, 83, 95, 85, 89, 91, 86, 87, 82]\n",
            "[55, 55, 60, 63, 57, 60, 53, 55, 60, 60, 60, 60, 60, 60, 57, 57, 66, 62, 63, 61, 71, 74, 61, 69]\n",
            "Epoch [261/500], Loss: 88.7831039428711, Learning Rate: 1e-05\n",
            "[77, 76, 74, 74, 72, 72, 72, 72, 76, 78, 80, 81, 83, 83, 81, 81, 87, 88, 82, 82, 83, 83, 82, 81]\n",
            "[57, 60, 53, 55, 48, 48, 48, 48, 60, 60, 59, 60, 62, 64, 57, 57, 63, 69, 64, 61, 69, 66, 64, 69]\n",
            "Epoch [262/500], Loss: 88.03404998779297, Learning Rate: 1e-05\n",
            "[89, 87, 86, 86, 84, 84, 87, 87, 86, 84, 83, 84, 86, 86, 86, 86, 80, 77, 77, 76, 79, 79, 74, 77]\n",
            "[74, 72, 65, 67, 60, 60, 72, 68, 65, 68, 62, 60, 67, 67, 67, 67, 59, 60, 60, 58, 58, 58, 59, 58]\n",
            "Epoch [263/500], Loss: 78.07172393798828, Learning Rate: 1e-05\n",
            "[84, 79, 79, 84, 86, 85, 86, 84, 87, 86, 84, 86, 87, 89, 91, 91, 91, 89, 92, 87, 90, 90, 92, 89]\n",
            "[60, 63, 67, 69, 71, 72, 67, 60, 60, 60, 72, 65, 60, 56, 55, 55, 66, 71, 68, 71, 66, 68, 71, 61]\n",
            "Epoch [264/500], Loss: 70.53834533691406, Learning Rate: 1e-05\n",
            "[84, 83, 83, 86, 81, 86, 82, 79, 80, 77, 77, 75, 87, 86, 84, 82, 86, 86, 85, 86, 86, 85, 84, 84]\n",
            "[68, 62, 60, 67, 72, 65, 62, 63, 60, 56, 58, 63, 63, 65, 68, 67, 65, 66, 66, 67, 65, 64, 66, 65]\n",
            "Epoch [265/500], Loss: 87.15210723876953, Learning Rate: 1e-05\n",
            "[83, 81, 81, 81, 76, 94, 79, 77, 76, 74, 74, 76, 83, 84, 84, 83, 73, 74, 72, 75, 74, 72, 73, 73]\n",
            "[52, 57, 57, 57, 60, 65, 64, 57, 60, 53, 55, 60, 59, 57, 60, 64, 59, 55, 57, 55, 58, 53, 56, 56]\n",
            "Epoch [266/500], Loss: 86.03466033935547, Learning Rate: 1e-05\n",
            "[84, 87, 86, 84, 84, 82, 80, 79, 86, 81, 83, 84, 87, 86, 86, 84, 88, 90, 94, 87, 91, 93, 93, 90]\n",
            "[60, 72, 67, 68, 65, 62, 65, 60, 63, 63, 62, 65, 60, 65, 67, 60, 69, 68, 69, 72, 68, 70, 70, 64]\n",
            "Epoch [267/500], Loss: 131.48825073242188, Learning Rate: 1e-05\n",
            "[84, 84, 87, 86, 92, 78, 87, 89, 91, 91, 91, 91, 91, 92, 94, 87, 92, 91, 90, 87, 93, 89, 90, 90]\n",
            "[60, 60, 72, 74, 75, 71, 72, 68, 67, 67, 67, 67, 72, 70, 67, 68, 70, 71, 72, 71, 69, 71, 70, 66]\n",
            "Epoch [268/500], Loss: 120.48220825195312, Learning Rate: 1e-05\n",
            "[84, 88, 88, 73, 86, 90, 88, 76, 83, 84, 81, 79, 79, 87, 86, 88, 74, 76, 75, 72, 78, 78, 78, 74]\n",
            "[69, 69, 68, 69, 67, 72, 69, 66, 67, 64, 62, 67, 64, 69, 69, 72, 61, 56, 58, 59, 60, 57, 58, 58]\n",
            "Epoch [269/500], Loss: 64.03488159179688, Learning Rate: 1e-05\n",
            "[84, 86, 75, 84, 66, 91, 91, 66, 91, 70, 89, 88, 88, 89, 88, 86, 84, 86, 84, 85, 87, 81, 84, 85]\n",
            "[69, 71, 72, 65, 72, 71, 72, 72, 73, 74, 62, 69, 72, 72, 72, 72, 61, 64, 66, 66, 66, 67, 62, 65]\n",
            "Epoch [270/500], Loss: 84.44773864746094, Learning Rate: 1e-05\n",
            "[78, 79, 81, 79, 79, 79, 79, 79, 82, 81, 79, 77, 79, 80, 72, 76, 84, 79, 80, 81, 79, 79, 75, 77]\n",
            "[57, 55, 50, 55, 60, 52, 55, 48, 55, 57, 60, 62, 59, 60, 48, 60, 58, 58, 62, 63, 58, 61, 60, 60]\n",
            "Epoch [271/500], Loss: 93.66959381103516, Learning Rate: 1e-05\n",
            "[90, 91, 91, 91, 91, 93, 91, 86, 89, 88, 86, 84, 84, 84, 84, 86, 56, 50, 52, 55, 54, 54, 56, 53]\n",
            "[69, 67, 67, 67, 72, 65, 64, 67, 62, 64, 67, 60, 60, 60, 72, 71, 44, 40, 41, 43, 43, 41, 45, 44]\n",
            "Epoch [272/500], Loss: 98.00005340576172, Learning Rate: 1e-05\n",
            "[88, 75, 86, 88, 91, 89, 89, 72, 88, 91, 91, 86, 88, 84, 84, 86, 79, 79, 85, 84, 80, 84, 86, 84]\n",
            "[72, 77, 74, 72, 71, 69, 71, 72, 60, 64, 64, 67, 64, 69, 69, 67, 68, 61, 65, 69, 66, 62, 62, 66]\n",
            "Epoch [273/500], Loss: 94.19940185546875, Learning Rate: 1e-05\n",
            "[79, 85, 81, 82, 80, 79, 77, 88, 75, 82, 82, 84, 86, 87, 86, 87, 90, 91, 91, 89, 91, 94, 90, 91]\n",
            "[60, 63, 63, 62, 65, 67, 65, 60, 60, 55, 51, 56, 53, 51, 58, 63, 68, 71, 67, 69, 66, 64, 69, 63]\n",
            "Epoch [274/500], Loss: 90.00434875488281, Learning Rate: 1e-05\n",
            "[83, 79, 81, 81, 79, 79, 79, 81, 84, 84, 83, 65, 81, 81, 79, 79, 84, 87, 83, 88, 89, 89, 87, 85]\n",
            "[64, 64, 60, 62, 55, 55, 60, 53, 52, 52, 55, 59, 62, 50, 55, 55, 69, 68, 65, 63, 66, 61, 64, 65]\n",
            "Epoch [275/500], Loss: 76.85401916503906, Learning Rate: 1e-05\n",
            "[76, 81, 77, 76, 67, 81, 81, 79, 68, 77, 85, 72, 72, 79, 79, 81, 90, 94, 90, 90, 91, 94, 87, 89]\n",
            "[57, 61, 62, 57, 69, 65, 66, 67, 69, 65, 67, 60, 60, 59, 60, 57, 69, 65, 69, 67, 70, 70, 66, 70]\n",
            "Epoch [276/500], Loss: 80.86180114746094, Learning Rate: 1e-05\n",
            "[89, 86, 86, 86, 88, 84, 86, 70, 91, 86, 89, 88, 88, 91, 91, 86, 83, 83, 77, 83, 81, 80, 80, 78]\n",
            "[65, 67, 67, 67, 68, 69, 71, 72, 73, 74, 71, 69, 69, 64, 64, 66, 62, 64, 63, 65, 61, 61, 62, 59]\n",
            "Epoch [277/500], Loss: 89.99763488769531, Learning Rate: 1e-05\n",
            "[84, 84, 84, 81, 83, 84, 86, 84, 83, 81, 81, 81, 81, 83, 84, 86, 82, 80, 76, 82, 79, 81, 80, 78]\n",
            "[60, 64, 60, 65, 65, 64, 66, 66, 67, 62, 62, 62, 62, 67, 64, 59, 56, 61, 59, 59, 59, 60, 62, 57]\n",
            "Epoch [278/500], Loss: 74.26457977294922, Learning Rate: 1e-05\n",
            "[86, 86, 67, 91, 91, 89, 89, 88, 84, 86, 86, 69, 90, 88, 84, 84, 92, 90, 86, 84, 92, 83, 90, 87]\n",
            "[67, 67, 72, 72, 64, 65, 65, 67, 69, 71, 67, 72, 72, 60, 65, 65, 63, 70, 69, 69, 67, 67, 71, 62]\n",
            "Epoch [279/500], Loss: 88.10999298095703, Learning Rate: 1e-05\n",
            "[86, 84, 84, 84, 76, 84, 82, 80, 79, 77, 77, 79, 86, 87, 87, 86, 76, 73, 70, 67, 77, 70, 70, 70]\n",
            "[67, 60, 60, 60, 72, 68, 67, 65, 63, 68, 68, 67, 67, 60, 63, 67, 53, 60, 57, 54, 52, 55, 59, 51]\n",
            "Epoch [280/500], Loss: 65.0674819946289, Learning Rate: 1e-05\n",
            "[91, 91, 89, 88, 86, 88, 88, 88, 88, 84, 93, 91, 86, 88, 90, 91, 89, 91, 91, 88, 93, 91, 90, 92]\n",
            "[72, 69, 62, 64, 65, 64, 64, 64, 69, 76, 74, 71, 67, 72, 69, 67, 68, 68, 71, 73, 68, 71, 68, 67]\n",
            "Epoch [281/500], Loss: 66.53721618652344, Learning Rate: 1e-05\n",
            "[77, 79, 79, 79, 79, 79, 92, 79, 79, 65, 79, 81, 81, 82, 82, 82, 95, 97, 95, 93, 94, 94, 92, 92]\n",
            "[55, 51, 51, 51, 51, 58, 63, 63, 67, 70, 63, 60, 63, 58, 58, 58, 72, 71, 74, 74, 72, 71, 71, 69]\n",
            "Epoch [282/500], Loss: 88.13581085205078, Learning Rate: 1e-05\n",
            "[89, 84, 93, 68, 79, 93, 93, 95, 96, 96, 88, 89, 89, 88, 86, 86, 89, 93, 90, 91, 87, 90, 87, 86]\n",
            "[74, 77, 74, 76, 76, 69, 69, 67, 72, 72, 72, 65, 65, 60, 67, 67, 68, 69, 70, 68, 72, 74, 65, 74]\n",
            "Epoch [283/500], Loss: 99.2626953125, Learning Rate: 1e-05\n",
            "[84, 86, 86, 86, 79, 82, 84, 82, 87, 87, 86, 87, 87, 86, 84, 89, 89, 89, 86, 85, 88, 87, 84, 83]\n",
            "[63, 67, 67, 67, 55, 67, 66, 67, 72, 69, 70, 63, 60, 58, 65, 62, 63, 58, 69, 68, 61, 71, 65, 61]\n",
            "Epoch [284/500], Loss: 128.93630981445312, Learning Rate: 1e-05\n",
            "[86, 87, 89, 91, 91, 93, 93, 90, 91, 91, 91, 91, 83, 84, 86, 87, 83, 83, 80, 77, 83, 81, 78, 80]\n",
            "[70, 67, 63, 63, 63, 60, 61, 62, 67, 62, 62, 67, 67, 63, 60, 60, 61, 61, 59, 56, 64, 63, 60, 63]\n",
            "Epoch [285/500], Loss: 90.93367004394531, Learning Rate: 1e-05\n",
            "[77, 77, 79, 80, 82, 82, 80, 79, 77, 77, 79, 79, 82, 79, 80, 79, 74, 71, 73, 74, 74, 74, 78, 75]\n",
            "[58, 58, 63, 62, 62, 62, 60, 63, 58, 58, 63, 60, 60, 60, 53, 55, 62, 56, 58, 60, 60, 54, 57, 59]\n",
            "Epoch [286/500], Loss: 84.84042358398438, Learning Rate: 1e-05\n",
            "[74, 76, 79, 81, 78, 79, 79, 79, 79, 76, 79, 81, 79, 77, 67, 67, 85, 84, 86, 86, 83, 86, 85, 86]\n",
            "[55, 60, 57, 60, 62, 55, 55, 55, 55, 60, 60, 53, 62, 62, 65, 69, 67, 65, 66, 65, 65, 64, 65, 64]\n",
            "Epoch [287/500], Loss: 86.49790954589844, Learning Rate: 1e-05\n",
            "[91, 89, 87, 86, 89, 87, 86, 84, 73, 86, 84, 82, 79, 82, 84, 86, 89, 84, 87, 89, 83, 88, 86, 87]\n",
            "[63, 65, 67, 70, 71, 72, 67, 60, 72, 70, 67, 67, 67, 63, 60, 58, 62, 66, 64, 63, 61, 63, 65, 59]\n",
            "Epoch [288/500], Loss: 73.82632446289062, Learning Rate: 0.0\n",
            "[77, 79, 79, 79, 79, 79, 82, 75, 79, 77, 77, 75, 79, 77, 79, 75, 86, 87, 87, 85, 87, 88, 87, 86]\n",
            "[53, 51, 51, 51, 63, 60, 55, 56, 55, 58, 58, 51, 63, 58, 55, 60, 67, 68, 64, 66, 68, 65, 67, 64]\n",
            "Epoch [289/500], Loss: 87.60997772216797, Learning Rate: 0.0\n",
            "[89, 86, 86, 86, 88, 84, 86, 78, 91, 89, 75, 88, 85, 91, 91, 86, 70, 62, 65, 70, 69, 67, 69, 70]\n",
            "[65, 67, 67, 67, 68, 69, 71, 72, 73, 74, 74, 69, 72, 71, 69, 66, 55, 54, 53, 54, 52, 54, 54, 56]\n",
            "Epoch [290/500], Loss: 71.09313201904297, Learning Rate: 0.0\n",
            "[87, 86, 91, 89, 86, 87, 83, 83, 84, 87, 80, 79, 74, 82, 79, 79, 91, 88, 84, 79, 90, 87, 80, 83]\n",
            "[60, 62, 63, 60, 65, 67, 67, 67, 63, 60, 65, 67, 70, 58, 63, 63, 63, 67, 67, 62, 64, 64, 65, 60]\n",
            "Epoch [291/500], Loss: 100.06851959228516, Learning Rate: 0.0\n",
            "[91, 89, 88, 86, 86, 88, 88, 88, 91, 91, 91, 85, 93, 91, 89, 88, 82, 82, 82, 82, 83, 82, 83, 81]\n",
            "[69, 71, 72, 65, 67, 60, 60, 60, 72, 74, 76, 77, 65, 67, 69, 72, 62, 63, 65, 64, 63, 65, 62, 64]\n",
            "Epoch [292/500], Loss: 82.45736694335938, Learning Rate: 0.0\n",
            "[88, 89, 88, 86, 88, 86, 86, 84, 84, 84, 86, 88, 91, 86, 90, 91, 75, 77, 70, 77, 78, 72, 74, 73]\n",
            "[68, 69, 72, 65, 62, 67, 55, 60, 72, 69, 67, 72, 71, 74, 62, 67, 60, 60, 56, 57, 59, 56, 56, 60]\n",
            "Epoch [293/500], Loss: 73.51892852783203, Learning Rate: 0.0\n",
            "[74, 72, 72, 72, 72, 74, 79, 77, 93, 79, 78, 79, 79, 79, 84, 76, 79, 86, 85, 86, 92, 82, 82, 86]\n",
            "[55, 48, 48, 48, 60, 59, 60, 57, 59, 55, 62, 55, 55, 55, 60, 57, 64, 62, 60, 58, 64, 61, 60, 64]\n",
            "Epoch [294/500], Loss: 87.65740203857422, Learning Rate: 0.0\n",
            "[75, 78, 84, 91, 68, 84, 88, 90, 91, 91, 91, 91, 90, 90, 91, 91, 83, 82, 85, 82, 84, 82, 87, 84]\n",
            "[67, 67, 67, 67, 71, 71, 72, 72, 71, 72, 74, 74, 62, 62, 67, 67, 67, 62, 66, 69, 66, 62, 63, 63]\n",
            "Epoch [295/500], Loss: 52.88895797729492, Learning Rate: 0.0\n",
            "[86, 84, 84, 84, 91, 89, 87, 86, 88, 86, 86, 86, 86, 87, 89, 76, 73, 69, 68, 73, 70, 70, 74, 70]\n",
            "[67, 72, 72, 72, 60, 62, 63, 65, 68, 67, 67, 67, 67, 72, 74, 75, 57, 57, 55, 55, 55, 51, 57, 54]\n",
            "Epoch [296/500], Loss: 69.61492919921875, Learning Rate: 0.0\n",
            "[79, 81, 83, 84, 83, 84, 83, 81, 81, 79, 81, 81, 79, 81, 77, 76, 68, 70, 69, 66, 70, 75, 68, 67]\n",
            "[64, 65, 62, 60, 64, 57, 59, 60, 62, 55, 62, 65, 60, 61, 62, 57, 57, 51, 52, 50, 55, 53, 55, 53]\n",
            "Epoch [297/500], Loss: 71.97393035888672, Learning Rate: 0.0\n",
            "[79, 79, 89, 82, 79, 87, 77, 79, 79, 77, 84, 87, 86, 84, 83, 84, 82, 79, 78, 83, 80, 85, 80, 82]\n",
            "[60, 63, 62, 67, 63, 68, 56, 55, 60, 67, 65, 63, 65, 68, 67, 60, 59, 63, 62, 59, 61, 61, 62, 58]\n",
            "Epoch [298/500], Loss: 94.24024200439453, Learning Rate: 0.0\n",
            "[91, 89, 87, 84, 84, 89, 89, 87, 86, 84, 82, 82, 87, 89, 91, 87, 80, 86, 83, 84, 87, 85, 88, 84]\n",
            "[63, 63, 72, 60, 60, 69, 70, 67, 65, 65, 58, 58, 72, 70, 67, 68, 65, 65, 63, 65, 65, 66, 63, 67]\n",
            "Epoch [299/500], Loss: 76.97498321533203, Learning Rate: 0.0\n",
            "[84, 84, 86, 84, 81, 84, 84, 83, 83, 84, 86, 84, 81, 84, 86, 83, 78, 73, 76, 77, 79, 74, 78, 82]\n",
            "[60, 65, 62, 57, 69, 66, 62, 67, 67, 64, 60, 60, 62, 64, 66, 67, 60, 62, 62, 60, 58, 59, 62, 59]\n",
            "Epoch [300/500], Loss: 114.09090423583984, Learning Rate: 0.0\n",
            "[91, 94, 67, 87, 89, 92, 92, 91, 91, 87, 86, 87, 93, 87, 89, 91, 88, 91, 94, 91, 92, 92, 95, 96]\n",
            "[72, 74, 75, 68, 68, 65, 65, 60, 71, 72, 72, 72, 75, 72, 68, 67, 72, 73, 71, 75, 74, 69, 70, 72]\n",
            "Epoch [301/500], Loss: 79.27967834472656, Learning Rate: 0.0\n",
            "[91, 91, 90, 93, 93, 91, 91, 91, 88, 89, 88, 86, 88, 86, 86, 84, 90, 88, 88, 90, 88, 89, 89, 89]\n",
            "[60, 72, 76, 65, 69, 72, 72, 72, 72, 71, 72, 67, 64, 67, 67, 60, 67, 68, 71, 68, 67, 67, 67, 65]\n",
            "Epoch [302/500], Loss: 103.98503112792969, Learning Rate: 0.0\n",
            "[86, 86, 84, 91, 89, 67, 93, 93, 93, 88, 89, 91, 91, 93, 89, 86, 93, 95, 89, 88, 91, 90, 87, 89]\n",
            "[67, 67, 60, 60, 62, 64, 65, 65, 65, 72, 71, 67, 64, 64, 65, 67, 73, 69, 71, 67, 75, 72, 69, 70]\n",
            "Epoch [303/500], Loss: 122.95054626464844, Learning Rate: 0.0\n",
            "[84, 87, 86, 87, 91, 89, 87, 86, 87, 84, 84, 89, 86, 84, 84, 82, 79, 81, 79, 76, 80, 82, 78, 77]\n",
            "[63, 60, 67, 72, 75, 71, 72, 67, 72, 69, 65, 62, 67, 65, 65, 58, 62, 61, 61, 58, 62, 59, 61, 59]\n",
            "Epoch [304/500], Loss: 99.23324584960938, Learning Rate: 0.0\n",
            "[77, 76, 84, 94, 76, 76, 74, 89, 77, 76, 74, 74, 72, 72, 72, 72, 91, 94, 91, 94, 89, 90, 89, 82]\n",
            "[62, 64, 67, 67, 60, 60, 65, 65, 62, 60, 57, 59, 48, 48, 48, 48, 70, 70, 69, 69, 70, 70, 66, 69]\n",
            "Epoch [305/500], Loss: 71.10547637939453, Learning Rate: 0.0\n",
            "[88, 89, 88, 86, 86, 84, 84, 84, 84, 84, 86, 81, 93, 86, 84, 83, 74, 75, 67, 77, 73, 70, 75, 68]\n",
            "[71, 69, 67, 65, 67, 60, 60, 60, 60, 64, 67, 72, 72, 71, 66, 67, 61, 57, 56, 57, 59, 53, 55, 59]\n",
            "Epoch [306/500], Loss: 112.43006134033203, Learning Rate: 0.0\n",
            "[84, 88, 88, 86, 86, 84, 84, 84, 88, 86, 88, 84, 86, 83, 84, 86, 65, 67, 66, 65, 70, 69, 69, 66]\n",
            "[69, 72, 64, 67, 67, 60, 60, 60, 60, 67, 68, 69, 74, 71, 69, 67, 54, 47, 51, 51, 52, 55, 55, 50]\n",
            "Epoch [307/500], Loss: 62.42206954956055, Learning Rate: 0.0\n",
            "[86, 86, 86, 85, 86, 88, 89, 89, 88, 88, 84, 86, 88, 71, 89, 88, 87, 85, 85, 85, 86, 84, 83, 86]\n",
            "[62, 62, 62, 69, 65, 62, 62, 65, 69, 69, 69, 70, 70, 74, 70, 72, 63, 64, 65, 62, 65, 69, 63, 64]\n",
            "Epoch [308/500], Loss: 62.33570861816406, Learning Rate: 0.0\n",
            "[91, 89, 90, 86, 86, 71, 87, 89, 89, 91, 91, 89, 87, 86, 86, 84, 87, 87, 90, 88, 86, 90, 88, 87]\n",
            "[63, 70, 72, 67, 67, 72, 72, 68, 70, 67, 63, 60, 60, 55, 55, 60, 69, 69, 68, 68, 68, 67, 67, 67]\n",
            "Epoch [309/500], Loss: 72.1992416381836, Learning Rate: 0.0\n",
            "[93, 91, 89, 88, 88, 86, 86, 84, 79, 84, 86, 88, 89, 91, 91, 88, 87, 91, 82, 85, 86, 88, 83, 80]\n",
            "[65, 67, 69, 72, 64, 67, 67, 60, 60, 72, 71, 69, 67, 64, 67, 60, 62, 59, 67, 66, 61, 71, 63, 63]\n",
            "Epoch [310/500], Loss: 74.19963073730469, Learning Rate: 0.0\n",
            "[84, 80, 86, 89, 86, 86, 86, 84, 82, 84, 86, 84, 82, 91, 79, 82, 80, 82, 79, 81, 85, 79, 80, 81]\n",
            "[65, 60, 67, 68, 67, 67, 67, 60, 67, 65, 70, 67, 67, 62, 55, 67, 63, 61, 59, 62, 61, 61, 62, 61]\n",
            "Epoch [311/500], Loss: 89.28618621826172, Learning Rate: 0.0\n",
            "[76, 79, 79, 79, 78, 79, 79, 76, 77, 77, 76, 71, 74, 68, 74, 90, 89, 96, 87, 91, 88, 87, 90, 87]\n",
            "[60, 64, 60, 62, 62, 55, 55, 60, 62, 64, 64, 67, 64, 64, 65, 67, 68, 73, 68, 63, 74, 73, 65, 74]\n",
            "Epoch [312/500], Loss: 75.91886901855469, Learning Rate: 0.0\n",
            "[75, 75, 77, 77, 82, 73, 77, 75, 79, 75, 74, 72, 75, 77, 77, 91, 89, 98, 87, 91, 91, 84, 96, 91]\n",
            "[63, 60, 62, 63, 62, 63, 58, 51, 59, 60, 55, 48, 60, 62, 62, 65, 67, 67, 74, 68, 72, 68, 65, 65]\n",
            "Epoch [313/500], Loss: 80.36616516113281, Learning Rate: 0.0\n",
            "[72, 72, 79, 81, 79, 79, 84, 84, 83, 83, 81, 81, 79, 79, 79, 81, 93, 94, 94, 93, 92, 92, 94, 93]\n",
            "[48, 48, 59, 62, 55, 55, 57, 54, 55, 64, 60, 62, 55, 55, 60, 53, 71, 67, 73, 72, 72, 72, 67, 70]\n",
            "Epoch [314/500], Loss: 79.72772216796875, Learning Rate: 0.0\n",
            "[86, 86, 86, 86, 89, 86, 86, 86, 85, 86, 71, 86, 88, 88, 88, 80, 82, 83, 80, 83, 86, 80, 81, 84]\n",
            "[66, 67, 71, 69, 67, 69, 62, 62, 69, 70, 71, 71, 72, 72, 72, 74, 61, 66, 63, 60, 65, 65, 61, 68]\n",
            "Epoch [315/500], Loss: 102.3508071899414, Learning Rate: 0.0\n",
            "[84, 84, 84, 84, 91, 91, 94, 94, 91, 85, 91, 91, 92, 86, 91, 91, 83, 84, 86, 86, 84, 86, 87, 85]\n",
            "[60, 60, 60, 60, 72, 72, 70, 70, 75, 77, 75, 74, 75, 77, 70, 67, 68, 65, 66, 67, 66, 65, 65, 65]\n",
            "Epoch [316/500], Loss: 96.09343719482422, Learning Rate: 0.0\n",
            "[74, 76, 75, 79, 79, 76, 79, 77, 79, 76, 73, 72, 79, 81, 81, 76, 66, 69, 67, 68, 67, 69, 69, 68]\n",
            "[55, 60, 62, 55, 59, 60, 59, 57, 59, 60, 55, 48, 60, 53, 57, 60, 57, 49, 51, 50, 55, 52, 55, 51]\n",
            "Epoch [317/500], Loss: 74.63151550292969, Learning Rate: 0.0\n",
            "[88, 88, 91, 88, 85, 86, 93, 89, 88, 86, 84, 84, 84, 88, 88, 86, 87, 90, 89, 85, 90, 88, 87, 87]\n",
            "[60, 72, 71, 72, 69, 70, 72, 65, 60, 67, 69, 65, 69, 72, 64, 67, 67, 68, 65, 64, 70, 67, 67, 68]\n",
            "Epoch [318/500], Loss: 65.97333526611328, Learning Rate: 0.0\n",
            "[68, 72, 72, 71, 71, 72, 74, 74, 77, 74, 74, 74, 76, 72, 74, 78, 69, 68, 70, 69, 70, 69, 72, 66]\n",
            "[52, 57, 57, 55, 55, 57, 59, 60, 53, 55, 55, 55, 56, 57, 59, 60, 56, 52, 55, 54, 55, 55, 55, 55]\n",
            "Epoch [319/500], Loss: 95.75762176513672, Learning Rate: 0.0\n",
            "[91, 88, 66, 84, 84, 89, 88, 91, 88, 78, 82, 79, 95, 67, 86, 86, 85, 85, 86, 87, 88, 85, 86, 89]\n",
            "[71, 72, 65, 72, 65, 69, 72, 76, 79, 76, 74, 67, 72, 71, 67, 67, 68, 67, 65, 67, 65, 64, 65, 66]\n",
            "Epoch [320/500], Loss: 70.59012603759766, Learning Rate: 0.0\n",
            "[82, 87, 87, 89, 87, 87, 86, 87, 87, 87, 87, 87, 87, 86, 86, 84, 80, 81, 83, 84, 81, 82, 81, 83]\n",
            "[63, 63, 75, 74, 72, 69, 70, 63, 63, 75, 72, 69, 65, 70, 67, 64, 65, 60, 65, 62, 63, 63, 62, 63]\n",
            "Epoch [321/500], Loss: 85.47259521484375, Learning Rate: 0.0\n",
            "[84, 81, 81, 81, 83, 84, 84, 84, 88, 91, 89, 91, 91, 88, 84, 81, 84, 81, 81, 79, 84, 82, 84, 80]\n",
            "[60, 65, 64, 62, 67, 60, 60, 60, 72, 76, 77, 71, 67, 72, 64, 65, 63, 66, 61, 63, 61, 61, 64, 58]\n",
            "Epoch [322/500], Loss: 84.84259033203125, Learning Rate: 0.0\n",
            "[88, 95, 91, 71, 90, 91, 91, 81, 89, 89, 88, 86, 86, 86, 67, 83, 81, 82, 85, 84, 87, 86, 87, 85]\n",
            "[72, 76, 72, 74, 62, 67, 67, 72, 65, 65, 60, 67, 67, 67, 69, 67, 64, 64, 62, 63, 67, 62, 65, 62]\n",
            "Epoch [323/500], Loss: 89.04601287841797, Learning Rate: 0.0\n",
            "[84, 72, 84, 70, 84, 94, 88, 88, 84, 69, 91, 95, 93, 93, 91, 91, 87, 85, 89, 85, 90, 88, 87, 90]\n",
            "[72, 76, 72, 72, 69, 65, 60, 60, 72, 72, 71, 67, 72, 74, 67, 67, 67, 69, 68, 67, 68, 68, 68, 67]\n",
            "Epoch [324/500], Loss: 70.00717163085938, Learning Rate: 0.0\n",
            "[79, 79, 77, 82, 79, 75, 77, 79, 79, 82, 84, 87, 86, 84, 72, 84, 78, 81, 82, 80, 79, 81, 82, 79]\n",
            "[60, 58, 56, 55, 63, 68, 68, 67, 67, 63, 63, 60, 65, 66, 67, 60, 64, 60, 60, 60, 65, 60, 61, 63]\n",
            "Epoch [325/500], Loss: 69.44878387451172, Learning Rate: 0.0\n",
            "[84, 88, 89, 91, 88, 86, 86, 84, 88, 88, 86, 84, 83, 84, 88, 86, 81, 81, 79, 81, 84, 79, 79, 81]\n",
            "[60, 72, 71, 71, 72, 65, 67, 60, 60, 64, 67, 69, 71, 69, 64, 67, 61, 61, 58, 59, 60, 63, 61, 60]\n",
            "Epoch [326/500], Loss: 83.30835723876953, Learning Rate: 0.0\n",
            "[89, 88, 86, 84, 84, 86, 89, 88, 86, 81, 83, 84, 84, 89, 88, 86, 91, 95, 90, 92, 91, 90, 88, 86]\n",
            "[62, 60, 67, 60, 69, 67, 62, 64, 67, 65, 62, 60, 72, 69, 72, 72, 71, 65, 72, 72, 72, 74, 66, 71]\n",
            "Epoch [327/500], Loss: 90.27699279785156, Learning Rate: 0.0\n",
            "[86, 86, 86, 84, 88, 88, 75, 89, 89, 91, 91, 91, 84, 84, 70, 86, 93, 94, 96, 91, 89, 93, 90, 89]\n",
            "[62, 62, 67, 72, 71, 69, 74, 72, 70, 64, 64, 64, 65, 65, 72, 66, 70, 74, 69, 71, 70, 71, 71, 68]\n",
            "Epoch [328/500], Loss: 71.58120727539062, Learning Rate: 0.0\n",
            "[89, 88, 88, 88, 86, 86, 86, 86, 89, 86, 86, 86, 85, 86, 71, 86, 82, 81, 81, 82, 78, 84, 75, 74]\n",
            "[69, 68, 69, 67, 66, 67, 71, 69, 67, 69, 62, 62, 69, 70, 71, 71, 55, 59, 58, 59, 58, 63, 60, 59]\n",
            "Epoch [329/500], Loss: 85.48714447021484, Learning Rate: 0.0\n",
            "[83, 83, 83, 83, 67, 95, 86, 86, 84, 84, 83, 83, 83, 81, 84, 84, 87, 91, 90, 86, 92, 88, 86, 89]\n",
            "[67, 67, 67, 67, 72, 72, 65, 65, 68, 68, 67, 67, 65, 65, 63, 63, 69, 69, 65, 64, 71, 66, 65, 68]\n",
            "Epoch [330/500], Loss: 79.24524688720703, Learning Rate: 0.0\n",
            "[91, 91, 91, 95, 72, 74, 91, 89, 91, 91, 84, 88, 90, 91, 93, 91, 96, 96, 97, 92, 94, 91, 89, 92]\n",
            "[72, 71, 67, 72, 74, 77, 72, 74, 71, 72, 72, 69, 72, 71, 72, 67, 70, 65, 73, 69, 69, 72, 68, 67]\n",
            "Epoch [331/500], Loss: 79.62784576416016, Learning Rate: 0.0\n",
            "[88, 88, 91, 88, 87, 86, 79, 89, 88, 86, 84, 84, 84, 88, 88, 86, 86, 90, 94, 87, 87, 91, 89, 91]\n",
            "[60, 72, 71, 72, 69, 70, 72, 65, 60, 67, 69, 65, 69, 72, 64, 67, 70, 73, 66, 67, 74, 64, 68, 71]\n",
            "Epoch [332/500], Loss: 89.19387817382812, Learning Rate: 0.0\n",
            "[86, 88, 86, 65, 83, 70, 81, 79, 84, 84, 84, 84, 86, 82, 81, 79, 92, 92, 87, 86, 89, 86, 85, 84]\n",
            "[67, 72, 74, 76, 74, 74, 62, 67, 72, 69, 64, 65, 66, 67, 62, 55, 66, 65, 71, 69, 67, 70, 67, 66]\n",
            "Epoch [333/500], Loss: 92.65398406982422, Learning Rate: 0.0\n",
            "[91, 91, 84, 88, 90, 91, 93, 91, 91, 91, 91, 72, 66, 70, 91, 89, 65, 64, 62, 66, 65, 60, 65, 59]\n",
            "[71, 72, 72, 69, 72, 71, 72, 67, 72, 71, 67, 72, 74, 77, 72, 74, 51, 42, 51, 49, 50, 50, 50, 50]\n",
            "Epoch [334/500], Loss: 60.56987380981445, Learning Rate: 0.0\n",
            "[79, 79, 80, 79, 79, 77, 79, 79, 82, 82, 84, 84, 74, 82, 79, 79, 90, 93, 92, 93, 92, 91, 93, 94]\n",
            "[48, 60, 60, 60, 56, 56, 55, 55, 51, 63, 63, 63, 62, 58, 63, 51, 71, 73, 70, 69, 67, 70, 69, 69]\n",
            "Epoch [335/500], Loss: 94.93883514404297, Learning Rate: 0.0\n",
            "[91, 91, 93, 91, 89, 88, 86, 86, 88, 91, 89, 88, 86, 86, 88, 88, 92, 95, 94, 93, 93, 95, 91, 91]\n",
            "[72, 69, 65, 67, 69, 72, 67, 67, 72, 60, 62, 64, 65, 62, 60, 64, 68, 72, 69, 67, 72, 72, 68, 69]\n",
            "Epoch [336/500], Loss: 71.82250213623047, Learning Rate: 0.0\n",
            "[77, 80, 82, 74, 82, 87, 86, 84, 84, 89, 91, 89, 84, 86, 84, 82, 89, 88, 89, 81, 92, 86, 86, 85]\n",
            "[65, 65, 67, 68, 63, 60, 67, 60, 65, 62, 63, 70, 69, 70, 69, 67, 66, 67, 66, 68, 69, 65, 69, 60]\n",
            "Epoch [337/500], Loss: 80.46394348144531, Learning Rate: 0.0\n",
            "[84, 86, 86, 86, 86, 89, 89, 89, 89, 91, 68, 86, 86, 84, 84, 84, 90, 88, 91, 87, 89, 85, 88, 88]\n",
            "[60, 67, 67, 67, 67, 72, 70, 69, 71, 67, 72, 65, 67, 60, 60, 60, 66, 67, 71, 70, 65, 70, 70, 65]\n",
            "Epoch [338/500], Loss: 84.84635162353516, Learning Rate: 0.0\n",
            "[84, 83, 79, 81, 83, 84, 84, 84, 79, 81, 79, 77, 77, 85, 72, 71, 89, 87, 89, 84, 90, 88, 88, 91]\n",
            "[69, 62, 64, 65, 62, 60, 60, 60, 60, 65, 60, 62, 65, 69, 69, 69, 67, 67, 69, 66, 70, 67, 69, 67]\n",
            "Epoch [339/500], Loss: 118.41830444335938, Learning Rate: 0.0\n",
            "[86, 86, 86, 86, 88, 84, 84, 84, 81, 83, 84, 86, 86, 84, 84, 84, 71, 73, 70, 73, 70, 69, 81, 73]\n",
            "[67, 66, 62, 67, 64, 69, 69, 69, 69, 62, 60, 67, 55, 60, 60, 60, 61, 54, 55, 57, 57, 54, 56, 58]\n",
            "Epoch [340/500], Loss: 73.49467468261719, Learning Rate: 0.0\n",
            "[84, 79, 79, 76, 77, 93, 77, 76, 78, 76, 78, 79, 79, 79, 79, 81, 91, 89, 91, 92, 88, 92, 90, 91]\n",
            "[64, 64, 67, 60, 57, 59, 55, 60, 59, 60, 57, 55, 55, 60, 64, 65, 66, 71, 68, 69, 66, 68, 69, 63]\n",
            "Epoch [341/500], Loss: 70.39712524414062, Learning Rate: 0.0\n",
            "[88, 89, 88, 86, 86, 86, 84, 84, 83, 84, 84, 84, 86, 86, 86, 86, 85, 83, 89, 86, 80, 83, 84, 83]\n",
            "[65, 62, 64, 67, 65, 64, 69, 66, 67, 60, 60, 60, 67, 71, 74, 68, 64, 65, 61, 63, 63, 62, 64, 61]\n",
            "Epoch [342/500], Loss: 94.01714324951172, Learning Rate: 0.0\n",
            "[91, 71, 84, 86, 86, 84, 84, 84, 91, 89, 87, 86, 88, 86, 86, 86, 87, 93, 91, 87, 90, 90, 90, 89]\n",
            "[71, 72, 68, 65, 67, 72, 72, 72, 60, 62, 63, 65, 68, 67, 67, 67, 69, 63, 68, 69, 71, 66, 65, 66]\n",
            "Epoch [343/500], Loss: 109.00995635986328, Learning Rate: 0.0\n",
            "[91, 91, 86, 87, 89, 87, 86, 84, 69, 87, 86, 84, 83, 84, 86, 86, 92, 92, 88, 89, 90, 88, 92, 90]\n",
            "[60, 63, 67, 72, 68, 67, 67, 60, 72, 68, 65, 68, 62, 63, 67, 67, 66, 73, 68, 68, 68, 67, 69, 62]\n",
            "Epoch [344/500], Loss: 83.06505584716797, Learning Rate: 0.0\n",
            "[84, 84, 82, 82, 81, 81, 79, 79, 79, 79, 80, 79, 77, 77, 75, 75, 87, 91, 88, 89, 87, 85, 88, 87]\n",
            "[48, 51, 55, 58, 62, 50, 55, 55, 60, 56, 53, 55, 58, 58, 51, 51, 68, 65, 69, 64, 65, 72, 63, 69]\n",
            "Epoch [345/500], Loss: 69.75298309326172, Learning Rate: 0.0\n",
            "[79, 81, 71, 79, 79, 80, 79, 86, 76, 77, 77, 79, 79, 79, 84, 81, 73, 73, 76, 76, 73, 79, 74, 75]\n",
            "[60, 61, 61, 60, 60, 53, 55, 56, 60, 56, 56, 60, 60, 60, 60, 65, 61, 55, 57, 56, 58, 57, 57, 59]\n",
            "Epoch [346/500], Loss: 82.04399108886719, Learning Rate: 0.0\n",
            "[86, 87, 86, 84, 84, 86, 86, 91, 89, 87, 87, 86, 91, 89, 87, 86, 80, 79, 79, 80, 79, 80, 80, 80]\n",
            "[67, 67, 67, 60, 72, 72, 71, 70, 68, 67, 66, 67, 75, 75, 72, 72, 64, 61, 61, 63, 62, 62, 63, 61]\n",
            "Epoch [347/500], Loss: 74.85104370117188, Learning Rate: 0.0\n",
            "[87, 86, 84, 84, 82, 82, 87, 89, 82, 87, 92, 91, 89, 89, 87, 87, 91, 91, 90, 87, 91, 89, 87, 90]\n",
            "[67, 65, 65, 65, 58, 58, 67, 70, 75, 75, 72, 75, 68, 70, 63, 63, 70, 72, 69, 66, 73, 70, 69, 69]\n",
            "Epoch [348/500], Loss: 83.73744201660156, Learning Rate: 0.0\n",
            "[86, 87, 86, 84, 87, 87, 87, 84, 80, 84, 82, 95, 84, 84, 84, 82, 73, 74, 67, 69, 78, 68, 65, 67]\n",
            "[62, 67, 67, 60, 72, 67, 63, 68, 65, 60, 63, 56, 68, 64, 65, 61, 57, 59, 54, 48, 59, 50, 55, 57]\n",
            "Epoch [349/500], Loss: 91.8799057006836, Learning Rate: 0.0\n",
            "[88, 88, 86, 88, 91, 89, 89, 88, 88, 91, 91, 86, 88, 84, 84, 83, 82, 86, 80, 81, 84, 78, 87, 84]\n",
            "[72, 77, 76, 72, 72, 69, 71, 68, 69, 64, 64, 67, 64, 69, 62, 67, 66, 67, 67, 61, 63, 65, 65, 62]\n",
            "Epoch [350/500], Loss: 73.12055206298828, Learning Rate: 0.0\n",
            "[82, 80, 79, 77, 77, 80, 82, 84, 82, 87, 86, 84, 82, 86, 86, 84, 90, 90, 88, 90, 86, 86, 89, 88]\n",
            "[67, 65, 60, 53, 65, 65, 63, 68, 63, 60, 65, 72, 67, 72, 70, 68, 65, 73, 65, 67, 69, 68, 64, 68]\n",
            "Epoch [351/500], Loss: 93.23464965820312, Learning Rate: 0.0\n",
            "[85, 86, 80, 88, 89, 89, 88, 88, 88, 89, 91, 89, 88, 86, 88, 88, 87, 86, 87, 89, 87, 90, 89, 88]\n",
            "[69, 67, 72, 71, 69, 71, 72, 72, 72, 69, 64, 65, 67, 67, 60, 60, 63, 68, 61, 65, 63, 62, 66, 61]\n",
            "Epoch [352/500], Loss: 96.462890625, Learning Rate: 0.0\n",
            "[86, 72, 86, 86, 87, 89, 91, 87, 87, 91, 92, 91, 89, 89, 87, 87, 78, 79, 73, 76, 86, 73, 75, 75]\n",
            "[67, 68, 67, 67, 67, 67, 63, 75, 75, 75, 72, 70, 68, 70, 63, 63, 59, 59, 60, 58, 61, 59, 58, 60]\n",
            "Epoch [353/500], Loss: 84.57066345214844, Learning Rate: 0.0\n",
            "[86, 84, 84, 83, 67, 88, 88, 93, 91, 91, 69, 91, 88, 88, 88, 89, 81, 78, 81, 81, 79, 83, 80, 81]\n",
            "[62, 64, 66, 67, 72, 72, 69, 66, 67, 71, 74, 67, 68, 69, 70, 69, 64, 61, 64, 63, 61, 62, 62, 62]\n",
            "Epoch [354/500], Loss: 83.2868881225586, Learning Rate: 0.0\n",
            "[84, 84, 81, 81, 83, 83, 83, 83, 84, 84, 86, 86, 86, 86, 86, 86, 78, 75, 75, 74, 80, 76, 76, 78]\n",
            "[64, 64, 65, 65, 64, 64, 62, 62, 60, 60, 67, 67, 67, 67, 67, 67, 56, 55, 61, 58, 57, 61, 58, 59]\n",
            "Epoch [355/500], Loss: 71.52670288085938, Learning Rate: 0.0\n",
            "[79, 82, 84, 82, 80, 79, 79, 79, 82, 80, 79, 80, 77, 75, 73, 72, 83, 88, 84, 82, 86, 86, 83, 77]\n",
            "[61, 58, 58, 58, 53, 60, 60, 60, 58, 65, 63, 60, 61, 58, 58, 60, 64, 64, 64, 63, 65, 69, 62, 63]\n",
            "Epoch [356/500], Loss: 116.37806701660156, Learning Rate: 0.0\n",
            "[88, 86, 86, 84, 88, 88, 93, 91, 89, 88, 88, 88, 88, 91, 84, 86, 78, 77, 83, 83, 80, 80, 79, 84]\n",
            "[62, 67, 67, 60, 69, 69, 69, 64, 67, 60, 60, 60, 60, 64, 65, 65, 65, 60, 64, 63, 61, 59, 61, 64]\n",
            "Epoch [357/500], Loss: 100.06494140625, Learning Rate: 0.0\n",
            "[84, 86, 88, 91, 84, 83, 81, 79, 81, 83, 84, 86, 88, 86, 84, 84, 75, 79, 73, 77, 79, 72, 84, 82]\n",
            "[72, 71, 69, 64, 64, 67, 62, 67, 62, 62, 64, 60, 60, 67, 60, 60, 62, 61, 61, 56, 61, 54, 58, 60]\n",
            "Epoch [358/500], Loss: 83.63665008544922, Learning Rate: 0.0\n",
            "[93, 75, 75, 77, 75, 85, 72, 72, 77, 79, 77, 66, 75, 87, 74, 72, 82, 80, 80, 85, 78, 83, 81, 80]\n",
            "[65, 60, 56, 53, 55, 56, 55, 60, 58, 63, 62, 65, 63, 67, 55, 60, 62, 65, 61, 61, 63, 65, 61, 66]\n",
            "Epoch [359/500], Loss: 63.403541564941406, Learning Rate: 0.0\n",
            "[67, 82, 84, 81, 81, 79, 79, 79, 82, 81, 79, 77, 95, 79, 77, 75, 69, 66, 69, 70, 68, 71, 70, 69]\n",
            "[55, 55, 63, 60, 62, 55, 55, 55, 55, 60, 63, 63, 62, 63, 58, 60, 56, 53, 54, 55, 54, 53, 56, 55]\n",
            "Epoch [360/500], Loss: 75.90435028076172, Learning Rate: 0.0\n",
            "[91, 79, 86, 89, 87, 86, 86, 84, 89, 89, 89, 91, 89, 91, 93, 94, 81, 82, 84, 84, 82, 87, 84, 84]\n",
            "[60, 63, 62, 59, 60, 65, 67, 60, 57, 58, 62, 63, 63, 72, 69, 67, 67, 64, 62, 62, 65, 61, 63, 64]\n",
            "Epoch [361/500], Loss: 71.6744613647461, Learning Rate: 0.0\n",
            "[88, 86, 88, 84, 84, 84, 89, 89, 89, 88, 88, 88, 86, 86, 86, 91, 84, 84, 84, 85, 84, 85, 86, 83]\n",
            "[60, 60, 60, 65, 65, 65, 62, 64, 67, 70, 67, 69, 62, 62, 67, 72, 66, 68, 65, 66, 68, 65, 65, 66]\n",
            "Epoch [362/500], Loss: 124.89856719970703, Learning Rate: 0.0\n",
            "[79, 72, 74, 95, 77, 76, 74, 72, 76, 76, 76, 74, 76, 72, 86, 67, 84, 81, 82, 86, 81, 80, 82, 81]\n",
            "[52, 57, 59, 60, 57, 55, 43, 48, 60, 60, 57, 54, 56, 57, 60, 55, 64, 64, 65, 62, 63, 64, 62, 65]\n",
            "Epoch [363/500], Loss: 77.06133270263672, Learning Rate: 0.0\n",
            "[86, 87, 87, 87, 82, 84, 82, 84, 80, 79, 79, 79, 87, 86, 89, 87, 85, 89, 87, 85, 87, 89, 85, 86]\n",
            "[65, 67, 68, 63, 63, 68, 67, 64, 65, 60, 60, 60, 69, 70, 74, 67, 65, 72, 61, 61, 67, 64, 64, 65]\n",
            "Epoch [364/500], Loss: 85.64488220214844, Learning Rate: 0.0\n",
            "[84, 84, 83, 83, 90, 84, 79, 76, 77, 76, 74, 74, 72, 72, 72, 72, 82, 84, 84, 78, 87, 80, 82, 78]\n",
            "[57, 60, 64, 64, 65, 64, 59, 60, 57, 60, 53, 55, 48, 48, 48, 48, 60, 58, 65, 64, 61, 68, 64, 61]\n",
            "Epoch [365/500], Loss: 82.3656234741211, Learning Rate: 0.0\n",
            "[89, 91, 91, 88, 84, 84, 81, 86, 84, 83, 84, 81, 81, 79, 79, 79, 87, 89, 86, 79, 87, 84, 87, 83]\n",
            "[59, 64, 62, 60, 65, 65, 72, 66, 66, 67, 64, 60, 62, 55, 55, 67, 66, 69, 68, 69, 67, 65, 67, 62]\n",
            "Epoch [366/500], Loss: 82.98762512207031, Learning Rate: 0.0\n",
            "[70, 75, 75, 77, 77, 79, 79, 77, 75, 74, 74, 72, 79, 75, 77, 74, 90, 94, 90, 89, 90, 91, 92, 86]\n",
            "[63, 60, 56, 53, 50, 51, 55, 58, 60, 53, 55, 48, 59, 60, 53, 55, 68, 74, 66, 69, 67, 67, 68, 62]\n",
            "Epoch [367/500], Loss: 93.51424407958984, Learning Rate: 0.0\n",
            "[84, 84, 84, 84, 84, 84, 94, 74, 81, 81, 84, 84, 86, 86, 88, 88, 72, 74, 69, 73, 74, 73, 71, 71]\n",
            "[60, 60, 60, 60, 60, 60, 64, 64, 65, 65, 64, 64, 62, 62, 60, 60, 58, 53, 55, 54, 57, 51, 55, 55]\n",
            "Epoch [368/500], Loss: 69.56501770019531, Learning Rate: 0.0\n",
            "[86, 86, 86, 86, 87, 87, 89, 89, 84, 84, 87, 87, 86, 86, 84, 84, 79, 82, 83, 81, 82, 83, 80, 80]\n",
            "[67, 67, 67, 67, 63, 63, 62, 62, 65, 65, 57, 57, 58, 58, 63, 63, 62, 60, 63, 62, 64, 65, 62, 64]\n",
            "Epoch [369/500], Loss: 79.08477783203125, Learning Rate: 0.0\n",
            "[79, 79, 91, 81, 83, 83, 84, 84, 86, 86, 84, 84, 83, 83, 81, 81, 89, 89, 86, 87, 92, 91, 87, 89]\n",
            "[60, 64, 65, 62, 67, 64, 60, 60, 59, 62, 66, 69, 67, 64, 60, 62, 68, 65, 72, 70, 64, 72, 68, 66]\n",
            "Epoch [370/500], Loss: 97.59510803222656, Learning Rate: 0.0\n",
            "[84, 86, 86, 87, 87, 86, 87, 89, 85, 87, 87, 86, 87, 86, 84, 84, 92, 98, 95, 90, 91, 96, 90, 86]\n",
            "[68, 65, 67, 60, 72, 67, 72, 69, 71, 72, 66, 67, 60, 65, 66, 67, 70, 74, 68, 66, 70, 72, 67, 67]\n",
            "Epoch [371/500], Loss: 85.19557189941406, Learning Rate: 0.0\n",
            "[84, 86, 87, 89, 88, 86, 84, 84, 88, 89, 91, 89, 88, 86, 88, 88, 84, 83, 88, 84, 85, 85, 85, 81]\n",
            "[69, 71, 72, 69, 67, 67, 60, 60, 60, 62, 64, 65, 69, 65, 64, 64, 62, 66, 63, 67, 65, 67, 67, 64]\n",
            "Epoch [372/500], Loss: 83.09423065185547, Learning Rate: 0.0\n",
            "[84, 84, 84, 79, 83, 84, 81, 79, 84, 83, 81, 79, 81, 77, 67, 76, 78, 75, 81, 80, 74, 76, 78, 74]\n",
            "[72, 69, 67, 64, 62, 60, 62, 55, 57, 59, 60, 64, 65, 62, 55, 60, 59, 58, 59, 58, 57, 60, 59, 59]\n",
            "Epoch [373/500], Loss: 75.37464141845703, Learning Rate: 0.0\n",
            "[77, 76, 76, 76, 79, 81, 83, 84, 84, 83, 83, 81, 76, 79, 94, 79, 81, 79, 80, 83, 79, 81, 79, 77]\n",
            "[55, 60, 60, 60, 60, 60, 59, 57, 60, 64, 64, 57, 60, 64, 65, 64, 63, 61, 63, 62, 61, 60, 61, 61]\n",
            "Epoch [374/500], Loss: 116.40620422363281, Learning Rate: 0.0\n",
            "[76, 74, 79, 79, 79, 79, 81, 83, 84, 83, 81, 83, 83, 79, 84, 79, 67, 68, 71, 67, 67, 72, 69, 68]\n",
            "[57, 50, 59, 55, 60, 59, 57, 55, 57, 62, 50, 55, 55, 55, 52, 52, 55, 48, 50, 52, 55, 51, 54, 53]\n",
            "Epoch [375/500], Loss: 80.05986785888672, Learning Rate: 0.0\n",
            "[93, 91, 89, 69, 89, 86, 86, 84, 91, 93, 93, 86, 86, 91, 91, 84, 93, 100, 102, 99, 95, 99, 93, 93]\n",
            "[65, 67, 69, 72, 69, 65, 67, 60, 72, 65, 66, 67, 67, 71, 64, 69, 74, 73, 67, 70, 75, 75, 67, 74]\n",
            "Epoch [376/500], Loss: 71.55897521972656, Learning Rate: 0.0\n",
            "[79, 79, 79, 79, 80, 80, 82, 82, 79, 79, 79, 79, 84, 84, 83, 83, 90, 93, 93, 90, 90, 93, 91, 89]\n",
            "[60, 60, 63, 63, 65, 65, 62, 62, 63, 63, 63, 63, 63, 63, 62, 62, 71, 69, 68, 73, 71, 70, 68, 67]\n",
            "Epoch [377/500], Loss: 74.41907501220703, Learning Rate: 0.0\n",
            "[89, 91, 91, 86, 89, 87, 87, 86, 87, 89, 84, 87, 86, 84, 84, 82, 94, 98, 95, 91, 94, 95, 90, 91]\n",
            "[58, 63, 67, 70, 68, 60, 63, 67, 72, 69, 65, 67, 70, 63, 65, 58, 72, 67, 73, 71, 72, 73, 68, 70]\n",
            "Epoch [378/500], Loss: 57.738765716552734, Learning Rate: 0.0\n",
            "[68, 84, 84, 81, 84, 86, 83, 81, 85, 83, 84, 86, 84, 91, 81, 79, 91, 88, 89, 94, 82, 86, 86, 89]\n",
            "[72, 60, 64, 65, 69, 66, 67, 62, 74, 71, 71, 67, 71, 74, 62, 67, 65, 68, 70, 63, 66, 67, 64, 65]\n",
            "Epoch [379/500], Loss: 70.26184844970703, Learning Rate: 0.0\n",
            "[71, 76, 78, 79, 81, 83, 81, 79, 79, 79, 79, 84, 82, 81, 81, 81, 86, 92, 87, 87, 83, 86, 92, 87]\n",
            "[59, 60, 57, 59, 60, 62, 62, 55, 59, 60, 60, 64, 62, 65, 65, 65, 71, 71, 64, 65, 69, 69, 68, 63]\n",
            "Epoch [380/500], Loss: 101.5285873413086, Learning Rate: 0.0\n",
            "[72, 79, 79, 77, 84, 82, 81, 79, 82, 81, 73, 81, 81, 79, 79, 79, 86, 84, 85, 85, 84, 82, 82, 84]\n",
            "[60, 63, 63, 62, 58, 55, 60, 55, 67, 60, 63, 60, 62, 55, 55, 55, 66, 65, 66, 64, 67, 63, 63, 64]\n",
            "Epoch [381/500], Loss: 65.71333312988281, Learning Rate: 0.0\n",
            "[88, 84, 84, 83, 79, 84, 86, 88, 88, 86, 86, 86, 88, 84, 86, 69, 81, 79, 80, 83, 77, 84, 80, 78]\n",
            "[64, 69, 69, 67, 65, 64, 62, 60, 64, 67, 67, 67, 68, 69, 71, 72, 60, 61, 60, 60, 57, 63, 61, 59]\n",
            "Epoch [382/500], Loss: 112.46183013916016, Learning Rate: 0.0\n",
            "[83, 83, 83, 83, 83, 84, 84, 84, 88, 77, 86, 84, 86, 86, 88, 89, 72, 64, 66, 69, 70, 68, 68, 71]\n",
            "[62, 64, 67, 67, 64, 60, 60, 60, 60, 72, 71, 69, 71, 69, 67, 69, 52, 53, 54, 56, 51, 53, 57, 52]\n",
            "Epoch [383/500], Loss: 112.01249694824219, Learning Rate: 0.0\n",
            "[83, 84, 81, 79, 84, 83, 81, 79, 81, 71, 86, 72, 79, 79, 81, 79, 80, 82, 80, 83, 80, 80, 80, 79]\n",
            "[62, 60, 62, 55, 57, 59, 62, 64, 65, 69, 67, 60, 60, 64, 66, 67, 63, 60, 64, 60, 64, 66, 59, 66]\n",
            "Epoch [384/500], Loss: 85.62141418457031, Learning Rate: 0.0\n",
            "[79, 82, 83, 84, 86, 87, 84, 87, 86, 84, 84, 83, 79, 84, 87, 86, 79, 76, 76, 73, 77, 74, 75, 70]\n",
            "[63, 63, 62, 60, 59, 60, 60, 60, 67, 63, 68, 67, 67, 63, 60, 65, 55, 56, 58, 58, 57, 62, 59, 57]\n",
            "Epoch [385/500], Loss: 75.95022583007812, Learning Rate: 0.0\n",
            "[79, 79, 72, 82, 82, 83, 80, 79, 79, 75, 80, 74, 73, 89, 75, 75, 92, 94, 95, 93, 90, 94, 92, 91]\n",
            "[60, 60, 63, 62, 60, 63, 60, 63, 67, 70, 70, 70, 67, 67, 63, 60, 72, 74, 69, 72, 72, 71, 70, 68]\n",
            "Epoch [386/500], Loss: 67.15754699707031, Learning Rate: 0.0\n",
            "[81, 89, 91, 91, 93, 91, 89, 88, 86, 86, 84, 84, 88, 91, 89, 88, 75, 76, 71, 75, 78, 75, 74, 72]\n",
            "[72, 69, 64, 64, 65, 67, 69, 72, 67, 67, 60, 60, 72, 71, 69, 72, 57, 54, 57, 56, 55, 54, 57, 54]\n",
            "Epoch [387/500], Loss: 66.31049346923828, Learning Rate: 0.0\n",
            "[84, 67, 88, 84, 86, 88, 88, 81, 83, 84, 81, 76, 79, 84, 86, 88, 77, 74, 79, 78, 75, 78, 78, 77]\n",
            "[69, 72, 60, 65, 62, 60, 60, 72, 71, 66, 62, 55, 67, 64, 64, 60, 62, 58, 60, 62, 60, 60, 60, 60]\n",
            "Epoch [388/500], Loss: 62.28592300415039, Learning Rate: 0.0\n",
            "[79, 80, 79, 77, 79, 80, 79, 67, 84, 84, 80, 79, 82, 80, 79, 71, 89, 89, 89, 93, 88, 91, 90, 92]\n",
            "[60, 60, 60, 53, 60, 65, 67, 68, 67, 64, 65, 60, 55, 56, 60, 61, 64, 70, 67, 64, 66, 65, 67, 61]\n",
            "Epoch [389/500], Loss: 95.16001892089844, Learning Rate: 0.0\n",
            "[88, 89, 89, 86, 86, 86, 84, 84, 83, 84, 84, 84, 86, 86, 86, 86, 76, 73, 76, 77, 76, 77, 77, 76]\n",
            "[67, 65, 65, 58, 69, 68, 69, 66, 67, 60, 60, 60, 67, 66, 62, 67, 61, 59, 60, 60, 58, 58, 59, 59]\n",
            "Epoch [390/500], Loss: 72.7745590209961, Learning Rate: 0.0\n",
            "[91, 91, 91, 91, 94, 93, 91, 89, 91, 88, 84, 88, 90, 91, 70, 91, 86, 83, 83, 82, 86, 77, 84, 85]\n",
            "[71, 72, 67, 60, 62, 65, 60, 65, 71, 72, 72, 69, 72, 71, 74, 67, 60, 59, 64, 64, 63, 60, 60, 57]\n",
            "Epoch [391/500], Loss: 68.91812133789062, Learning Rate: 0.0\n",
            "[72, 79, 93, 72, 79, 81, 81, 79, 79, 81, 83, 84, 83, 81, 83, 79, 90, 93, 91, 94, 88, 93, 91, 88]\n",
            "[60, 59, 60, 57, 64, 60, 62, 55, 60, 65, 64, 64, 67, 60, 62, 55, 69, 72, 67, 70, 69, 68, 68, 64]\n",
            "Epoch [392/500], Loss: 72.79762268066406, Learning Rate: 0.0\n",
            "[89, 86, 86, 86, 67, 87, 89, 91, 91, 89, 89, 91, 91, 90, 89, 91, 88, 89, 91, 90, 86, 88, 87, 83]\n",
            "[65, 67, 67, 67, 67, 60, 62, 63, 67, 70, 70, 67, 71, 72, 70, 67, 66, 68, 67, 67, 67, 69, 66, 66]\n",
            "Epoch [393/500], Loss: 71.58855438232422, Learning Rate: 0.0\n",
            "[86, 81, 88, 91, 91, 74, 91, 91, 91, 88, 93, 93, 89, 88, 86, 86, 89, 89, 88, 90, 89, 88, 90, 90]\n",
            "[72, 71, 70, 69, 73, 74, 67, 67, 64, 69, 65, 70, 67, 69, 62, 62, 66, 70, 69, 66, 68, 69, 66, 68]\n",
            "Epoch [394/500], Loss: 70.7292251586914, Learning Rate: 0.0\n",
            "[86, 87, 86, 86, 86, 84, 84, 84, 86, 86, 71, 86, 87, 66, 87, 87, 83, 82, 79, 77, 85, 79, 81, 81]\n",
            "[60, 60, 65, 62, 67, 60, 60, 60, 58, 70, 69, 70, 72, 74, 67, 69, 58, 62, 65, 65, 59, 65, 66, 56]\n",
            "Epoch [395/500], Loss: 83.93589782714844, Learning Rate: 0.0\n",
            "[87, 86, 86, 84, 84, 84, 84, 86, 82, 82, 81, 82, 82, 84, 86, 87, 66, 68, 65, 66, 66, 67, 71, 64]\n",
            "[72, 67, 67, 60, 72, 69, 65, 58, 62, 65, 65, 58, 63, 63, 60, 60, 55, 48, 50, 54, 56, 51, 53, 56]\n",
            "Epoch [396/500], Loss: 83.93705749511719, Learning Rate: 0.0\n",
            "[72, 72, 72, 72, 72, 72, 70, 70, 75, 75, 77, 79, 80, 80, 80, 80, 88, 92, 81, 89, 87, 79, 88, 85]\n",
            "[60, 60, 60, 60, 60, 56, 56, 55, 55, 60, 60, 58, 60, 61, 56, 56, 66, 71, 69, 62, 64, 68, 66, 64]\n",
            "Epoch [397/500], Loss: 55.602108001708984, Learning Rate: 0.0\n",
            "[74, 72, 72, 72, 79, 76, 79, 81, 79, 92, 77, 76, 76, 74, 76, 87, 80, 78, 79, 78, 81, 78, 80, 83]\n",
            "[55, 60, 60, 60, 59, 60, 57, 53, 55, 57, 59, 60, 60, 65, 62, 67, 60, 62, 63, 60, 61, 60, 61, 63]\n",
            "Epoch [398/500], Loss: 81.18543243408203, Learning Rate: 0.0\n",
            "[82, 80, 71, 77, 77, 80, 82, 84, 82, 87, 86, 84, 82, 92, 86, 84, 75, 76, 73, 72, 76, 79, 76, 73]\n",
            "[62, 65, 60, 65, 65, 65, 63, 68, 63, 63, 67, 60, 67, 72, 70, 68, 63, 58, 59, 57, 60, 56, 58, 59]\n",
            "Epoch [399/500], Loss: 77.62517547607422, Learning Rate: 0.0\n",
            "[86, 86, 82, 82, 81, 82, 82, 84, 86, 87, 86, 65, 76, 86, 79, 82, 87, 89, 89, 86, 92, 87, 88, 88]\n",
            "[70, 67, 67, 63, 65, 58, 63, 72, 72, 72, 67, 63, 60, 67, 67, 67, 69, 69, 64, 67, 71, 66, 68, 66]\n",
            "Epoch [400/500], Loss: 65.67156219482422, Learning Rate: 0.0\n",
            "[79, 79, 75, 77, 92, 88, 74, 72, 79, 79, 77, 82, 79, 75, 77, 79, 85, 85, 81, 84, 91, 85, 83, 89]\n",
            "[60, 59, 60, 62, 63, 67, 55, 60, 60, 58, 56, 55, 63, 68, 68, 67, 64, 64, 66, 64, 62, 64, 65, 64]\n",
            "Epoch [401/500], Loss: 81.79833984375, Learning Rate: 0.0\n",
            "[91, 91, 91, 91, 94, 93, 91, 89, 91, 88, 84, 88, 90, 91, 70, 91, 81, 83, 79, 81, 81, 73, 82, 77]\n",
            "[71, 72, 67, 60, 62, 65, 60, 65, 71, 72, 72, 69, 72, 71, 74, 67, 58, 64, 64, 59, 63, 63, 63, 57]\n",
            "Epoch [402/500], Loss: 88.37018585205078, Learning Rate: 0.0\n",
            "[87, 86, 86, 84, 84, 86, 83, 84, 84, 84, 82, 82, 84, 65, 86, 86, 82, 79, 84, 83, 78, 79, 79, 80]\n",
            "[65, 70, 67, 64, 65, 66, 67, 60, 60, 68, 68, 67, 69, 70, 69, 71, 62, 60, 64, 59, 61, 60, 61, 59]\n",
            "Epoch [403/500], Loss: 78.60603332519531, Learning Rate: 0.0\n",
            "[77, 76, 76, 76, 79, 81, 83, 84, 84, 83, 83, 81, 76, 79, 94, 79, 77, 80, 76, 76, 79, 79, 77, 77]\n",
            "[55, 60, 60, 60, 60, 60, 59, 57, 60, 64, 64, 57, 60, 64, 65, 64, 61, 58, 58, 56, 61, 57, 61, 59]\n",
            "Epoch [404/500], Loss: 65.4064712524414, Learning Rate: 0.0\n",
            "[79, 77, 77, 79, 86, 87, 86, 87, 86, 84, 84, 84, 68, 84, 82, 80, 85, 86, 84, 86, 87, 84, 83, 84]\n",
            "[63, 57, 58, 63, 59, 60, 65, 67, 67, 60, 60, 60, 72, 68, 67, 65, 66, 65, 66, 66, 64, 63, 64, 65]\n",
            "Epoch [405/500], Loss: 58.058231353759766, Learning Rate: 0.0\n",
            "[84, 88, 88, 85, 89, 91, 89, 86, 91, 88, 88, 86, 84, 83, 83, 81, 68, 71, 69, 68, 68, 71, 69, 70]\n",
            "[60, 72, 69, 65, 62, 59, 60, 55, 59, 60, 64, 67, 69, 62, 64, 57, 56, 51, 51, 51, 56, 48, 55, 51]\n",
            "Epoch [406/500], Loss: 68.17308807373047, Learning Rate: 0.0\n",
            "[87, 86, 68, 84, 86, 86, 86, 86, 87, 89, 77, 84, 82, 80, 79, 79, 90, 89, 92, 91, 86, 90, 88, 86]\n",
            "[60, 62, 63, 60, 67, 67, 67, 67, 72, 70, 67, 68, 63, 65, 67, 67, 69, 69, 70, 68, 67, 69, 66, 67]\n",
            "Epoch [407/500], Loss: 87.80426788330078, Learning Rate: 0.0\n",
            "[79, 80, 87, 79, 86, 87, 87, 86, 86, 84, 84, 84, 66, 86, 82, 84, 62, 55, 59, 62, 59, 61, 62, 60]\n",
            "[67, 68, 61, 60, 70, 69, 67, 66, 67, 60, 60, 60, 72, 70, 68, 67, 51, 46, 47, 50, 48, 47, 50, 50]\n",
            "Epoch [408/500], Loss: 65.15827178955078, Learning Rate: 0.0\n",
            "[84, 86, 86, 87, 87, 86, 87, 89, 93, 87, 87, 86, 87, 86, 84, 84, 85, 90, 85, 80, 91, 81, 86, 89]\n",
            "[68, 65, 67, 60, 72, 67, 72, 69, 71, 72, 66, 67, 60, 65, 66, 67, 70, 66, 70, 61, 69, 66, 70, 64]\n",
            "Epoch [409/500], Loss: 81.07048034667969, Learning Rate: 0.0\n",
            "[79, 79, 75, 74, 75, 77, 79, 77, 75, 75, 91, 74, 75, 75, 77, 79, 85, 88, 87, 82, 87, 89, 79, 78]\n",
            "[55, 55, 60, 58, 55, 53, 51, 58, 60, 56, 53, 58, 51, 51, 58, 63, 66, 63, 66, 63, 64, 70, 62, 64]\n",
            "Epoch [410/500], Loss: 69.70164489746094, Learning Rate: 0.0\n",
            "[87, 92, 92, 91, 90, 89, 89, 87, 87, 86, 84, 82, 79, 82, 95, 84, 81, 78, 80, 83, 78, 77, 80, 78]\n",
            "[73, 72, 71, 70, 69, 70, 70, 63, 60, 65, 66, 67, 67, 63, 67, 68, 62, 60, 61, 60, 60, 59, 60, 60]\n",
            "Epoch [411/500], Loss: 60.488616943359375, Learning Rate: 0.0\n",
            "[84, 84, 83, 83, 84, 87, 86, 86, 84, 84, 84, 84, 79, 79, 70, 79, 77, 70, 78, 76, 72, 73, 74, 75]\n",
            "[56, 56, 55, 55, 63, 63, 67, 55, 60, 60, 60, 60, 60, 60, 60, 63, 56, 55, 56, 58, 54, 55, 58, 55]\n",
            "Epoch [412/500], Loss: 66.74554443359375, Learning Rate: 0.0\n",
            "[87, 87, 87, 86, 86, 86, 91, 91, 89, 87, 87, 86, 82, 86, 88, 89, 82, 83, 79, 79, 89, 79, 80, 82]\n",
            "[60, 60, 63, 67, 67, 67, 67, 71, 71, 72, 72, 72, 68, 68, 68, 65, 63, 63, 64, 61, 65, 63, 62, 64]\n",
            "Epoch [413/500], Loss: 85.0136489868164, Learning Rate: 0.0\n",
            "[75, 78, 84, 91, 68, 84, 88, 90, 91, 91, 91, 91, 90, 90, 91, 91, 89, 89, 90, 86, 88, 87, 84, 84]\n",
            "[67, 67, 67, 67, 71, 71, 72, 72, 71, 72, 74, 74, 62, 62, 67, 67, 67, 72, 61, 65, 67, 64, 65, 64]\n",
            "Epoch [414/500], Loss: 110.07231140136719, Learning Rate: 0.0\n",
            "[87, 89, 87, 85, 72, 85, 82, 80, 85, 85, 85, 85, 87, 83, 75, 67, 79, 77, 78, 79, 75, 77, 78, 78]\n",
            "[60, 61, 63, 65, 67, 68, 63, 56, 65, 64, 61, 66, 67, 68, 63, 56, 63, 60, 60, 62, 62, 59, 61, 62]\n",
            "Epoch [415/500], Loss: 71.3282241821289, Learning Rate: 0.0\n",
            "[84, 86, 86, 84, 82, 82, 79, 82, 82, 82, 84, 84, 86, 87, 87, 91, 87, 88, 91, 88, 86, 86, 89, 87]\n",
            "[72, 70, 70, 65, 67, 67, 63, 62, 62, 67, 65, 65, 62, 60, 60, 63, 66, 70, 66, 65, 65, 68, 66, 65]\n",
            "Epoch [416/500], Loss: 113.95055389404297, Learning Rate: 0.0\n",
            "[81, 84, 86, 83, 88, 79, 88, 86, 84, 85, 83, 84, 83, 84, 83, 81, 87, 85, 86, 82, 88, 82, 86, 85]\n",
            "[62, 64, 66, 67, 60, 72, 69, 65, 69, 67, 55, 60, 62, 64, 67, 60, 59, 63, 66, 65, 61, 67, 67, 59]\n",
            "Epoch [417/500], Loss: 67.97626495361328, Learning Rate: 0.0\n",
            "[84, 84, 84, 84, 86, 86, 86, 86, 86, 86, 88, 88, 88, 88, 94, 92, 91, 89, 85, 82, 92, 86, 84, 82]\n",
            "[60, 60, 60, 60, 67, 67, 67, 67, 67, 67, 72, 72, 60, 60, 67, 67, 67, 68, 71, 67, 64, 69, 67, 63]\n",
            "Epoch [418/500], Loss: 93.54054260253906, Learning Rate: 0.0\n",
            "[79, 79, 79, 79, 77, 81, 79, 77, 76, 76, 79, 79, 77, 77, 71, 74, 91, 92, 93, 93, 94, 97, 98, 94]\n",
            "[55, 55, 60, 64, 65, 62, 59, 62, 57, 57, 64, 60, 57, 55, 59, 64, 70, 76, 66, 68, 68, 66, 69, 64]\n",
            "Epoch [419/500], Loss: 94.74063873291016, Learning Rate: 0.0\n",
            "[84, 84, 82, 84, 80, 79, 80, 77, 77, 80, 82, 74, 82, 87, 86, 84, 75, 69, 71, 75, 72, 74, 71, 72]\n",
            "[60, 65, 70, 64, 65, 58, 60, 65, 65, 65, 67, 68, 63, 60, 67, 60, 56, 57, 54, 55, 56, 55, 56, 56]\n",
            "Epoch [420/500], Loss: 80.52446746826172, Learning Rate: 0.0\n",
            "[91, 91, 93, 84, 83, 81, 81, 84, 85, 95, 95, 84, 86, 88, 91, 91, 86, 89, 85, 85, 84, 85, 84, 80]\n",
            "[64, 67, 67, 64, 60, 60, 62, 67, 67, 67, 72, 69, 67, 64, 60, 61, 68, 64, 66, 65, 68, 69, 64, 66]\n",
            "Epoch [421/500], Loss: 65.84477233886719, Learning Rate: 0.0\n",
            "[79, 84, 86, 88, 89, 86, 86, 84, 84, 81, 83, 84, 81, 68, 78, 79, 82, 78, 79, 81, 81, 79, 79, 81]\n",
            "[65, 64, 59, 60, 65, 67, 67, 60, 60, 60, 59, 57, 62, 59, 62, 55, 62, 61, 62, 64, 60, 59, 61, 61]\n",
            "Epoch [422/500], Loss: 76.26432800292969, Learning Rate: 0.0\n",
            "[88, 86, 86, 86, 89, 89, 89, 86, 86, 91, 91, 91, 88, 89, 81, 88, 83, 87, 86, 86, 83, 85, 88, 87]\n",
            "[69, 62, 62, 62, 74, 69, 65, 70, 70, 67, 67, 67, 72, 69, 71, 72, 65, 66, 64, 62, 65, 63, 64, 62]\n",
            "Epoch [423/500], Loss: 75.01798248291016, Learning Rate: 0.0\n",
            "[77, 79, 79, 79, 81, 82, 81, 82, 79, 81, 81, 84, 84, 82, 79, 75, 78, 78, 71, 81, 77, 71, 77, 73]\n",
            "[58, 55, 62, 55, 65, 62, 60, 62, 63, 65, 62, 60, 57, 58, 63, 55, 59, 58, 61, 59, 60, 58, 56, 59]\n",
            "Epoch [424/500], Loss: 81.93698120117188, Learning Rate: 0.0\n",
            "[87, 87, 86, 87, 87, 86, 84, 89, 89, 86, 87, 86, 86, 84, 84, 84, 93, 95, 98, 97, 90, 94, 95, 97]\n",
            "[67, 70, 58, 63, 63, 70, 65, 68, 72, 67, 63, 67, 67, 60, 60, 60, 71, 73, 70, 69, 70, 71, 69, 70]\n",
            "Epoch [425/500], Loss: 53.4525260925293, Learning Rate: 0.0\n",
            "[75, 75, 75, 72, 75, 77, 79, 77, 75, 70, 72, 72, 75, 88, 75, 72, 79, 73, 80, 75, 80, 81, 79, 80]\n",
            "[60, 56, 55, 56, 56, 53, 51, 53, 55, 55, 60, 60, 60, 55, 56, 60, 61, 60, 63, 63, 60, 63, 63, 62]\n",
            "Epoch [426/500], Loss: 69.6541976928711, Learning Rate: 0.0\n",
            "[89, 88, 86, 84, 84, 86, 89, 88, 86, 81, 83, 84, 84, 89, 88, 86, 85, 87, 87, 84, 88, 86, 85, 82]\n",
            "[62, 60, 67, 60, 69, 67, 62, 64, 67, 65, 62, 60, 72, 69, 72, 72, 65, 61, 65, 64, 65, 69, 65, 64]\n",
            "Epoch [427/500], Loss: 72.87638092041016, Learning Rate: 0.0\n",
            "[81, 93, 79, 79, 79, 77, 77, 77, 79, 76, 79, 81, 81, 79, 79, 79, 79, 77, 82, 78, 82, 82, 82, 78]\n",
            "[55, 57, 58, 55, 60, 53, 53, 53, 59, 60, 52, 53, 57, 60, 59, 55, 62, 62, 63, 63, 61, 63, 64, 62]\n",
            "Epoch [428/500], Loss: 90.46398162841797, Learning Rate: 0.0\n",
            "[79, 81, 83, 84, 79, 74, 79, 77, 77, 70, 65, 94, 84, 81, 83, 84, 83, 81, 83, 82, 83, 81, 83, 84]\n",
            "[64, 65, 56, 57, 60, 65, 65, 62, 65, 69, 69, 69, 64, 65, 62, 60, 61, 64, 64, 64, 62, 61, 65, 60]\n",
            "Epoch [429/500], Loss: 70.12873840332031, Learning Rate: 0.0\n",
            "[83, 84, 84, 83, 83, 81, 81, 81, 84, 83, 79, 81, 83, 84, 84, 84, 81, 78, 81, 81, 78, 82, 81, 81]\n",
            "[59, 57, 60, 64, 52, 57, 57, 57, 57, 62, 64, 65, 62, 60, 60, 60, 65, 61, 63, 63, 61, 62, 62, 62]\n",
            "Epoch [430/500], Loss: 59.349754333496094, Learning Rate: 0.0\n",
            "[83, 81, 79, 84, 84, 84, 83, 83, 83, 84, 84, 84, 86, 86, 88, 86, 84, 90, 88, 89, 91, 87, 88, 88]\n",
            "[67, 69, 71, 69, 69, 69, 67, 67, 64, 69, 69, 65, 62, 62, 60, 62, 67, 69, 63, 66, 67, 67, 64, 68]\n",
            "Epoch [431/500], Loss: 72.21092987060547, Learning Rate: 0.0\n",
            "[79, 65, 74, 75, 75, 88, 86, 80, 79, 84, 82, 74, 79, 77, 77, 75, 87, 87, 82, 84, 90, 82, 85, 87]\n",
            "[63, 67, 55, 60, 60, 60, 60, 60, 60, 56, 56, 65, 63, 56, 58, 51, 62, 67, 68, 64, 65, 66, 65, 67]\n",
            "Epoch [432/500], Loss: 73.49807739257812, Learning Rate: 0.0\n",
            "[84, 84, 84, 84, 79, 79, 77, 75, 74, 74, 72, 72, 79, 81, 82, 65, 88, 92, 86, 89, 95, 91, 89, 90]\n",
            "[60, 60, 60, 60, 60, 58, 56, 53, 53, 55, 48, 48, 60, 65, 62, 63, 66, 68, 64, 66, 64, 67, 66, 65]\n",
            "Epoch [433/500], Loss: 75.42034912109375, Learning Rate: 0.0\n",
            "[87, 89, 91, 84, 91, 92, 89, 81, 87, 89, 91, 84, 87, 86, 85, 82, 94, 91, 94, 88, 89, 89, 89, 88]\n",
            "[72, 68, 65, 65, 71, 72, 74, 75, 67, 69, 71, 72, 67, 70, 64, 67, 70, 72, 71, 71, 69, 71, 71, 66]\n",
            "Epoch [434/500], Loss: 74.57855224609375, Learning Rate: 0.0\n",
            "[84, 84, 82, 82, 81, 81, 79, 79, 82, 82, 80, 79, 77, 77, 75, 75, 76, 72, 76, 76, 74, 67, 76, 73]\n",
            "[48, 51, 55, 58, 60, 62, 55, 55, 67, 63, 60, 63, 56, 58, 51, 51, 55, 55, 59, 53, 55, 57, 56, 55]\n",
            "Epoch [435/500], Loss: 87.61666870117188, Learning Rate: 0.0\n",
            "[86, 84, 84, 86, 90, 82, 84, 82, 87, 87, 84, 87, 87, 86, 84, 89, 85, 86, 85, 83, 86, 83, 78, 77]\n",
            "[70, 68, 68, 67, 72, 70, 68, 67, 65, 65, 70, 63, 60, 65, 68, 62, 64, 57, 68, 62, 66, 73, 59, 67]\n",
            "Epoch [436/500], Loss: 96.56922149658203, Learning Rate: 0.0\n",
            "[78, 82, 74, 82, 87, 81, 86, 87, 87, 71, 84, 89, 89, 86, 87, 86, 90, 94, 92, 92, 93, 94, 90, 93]\n",
            "[72, 67, 68, 63, 67, 70, 58, 63, 63, 70, 65, 69, 72, 67, 63, 67, 68, 72, 71, 67, 69, 69, 67, 69]\n",
            "Epoch [437/500], Loss: 91.51190185546875, Learning Rate: 0.0\n",
            "[92, 91, 91, 91, 68, 92, 89, 89, 94, 87, 87, 86, 89, 86, 84, 82, 74, 71, 71, 76, 74, 69, 74, 73]\n",
            "[70, 63, 63, 63, 75, 74, 72, 74, 75, 72, 68, 70, 63, 65, 65, 58, 58, 55, 60, 56, 58, 57, 56, 56]\n",
            "Epoch [438/500], Loss: 68.54938507080078, Learning Rate: 0.0\n",
            "[87, 92, 92, 91, 87, 89, 89, 87, 87, 86, 84, 82, 81, 82, 82, 84, 89, 91, 88, 85, 89, 84, 90, 89]\n",
            "[67, 65, 68, 72, 72, 68, 68, 63, 72, 66, 69, 67, 70, 67, 67, 64, 67, 71, 70, 65, 68, 67, 71, 63]\n",
            "Epoch [439/500], Loss: 93.44872283935547, Learning Rate: 0.0\n",
            "[84, 86, 86, 91, 89, 87, 87, 86, 84, 84, 83, 84, 86, 87, 86, 84, 81, 81, 84, 83, 80, 84, 84, 83]\n",
            "[72, 72, 71, 70, 68, 67, 66, 67, 72, 65, 67, 68, 67, 67, 67, 60, 65, 64, 63, 64, 63, 63, 64, 63]\n",
            "Epoch [440/500], Loss: 79.70596313476562, Learning Rate: 0.0\n",
            "[82, 80, 79, 80, 77, 75, 73, 72, 79, 80, 79, 77, 79, 82, 84, 82, 78, 76, 77, 79, 78, 77, 77, 76]\n",
            "[58, 65, 63, 60, 61, 58, 58, 60, 60, 65, 65, 61, 61, 58, 58, 58, 62, 59, 62, 60, 63, 63, 58, 65]\n",
            "Epoch [441/500], Loss: 76.1505355834961, Learning Rate: 0.0\n",
            "[86, 84, 72, 84, 84, 86, 84, 82, 81, 79, 78, 79, 84, 82, 81, 82, 83, 86, 84, 83, 86, 83, 86, 84]\n",
            "[65, 66, 67, 60, 65, 58, 57, 55, 62, 63, 62, 55, 65, 62, 60, 58, 64, 67, 62, 67, 64, 63, 64, 64]\n",
            "Epoch [442/500], Loss: 71.32173156738281, Learning Rate: 0.0\n",
            "[89, 88, 86, 86, 84, 86, 86, 88, 86, 86, 84, 74, 83, 83, 84, 84, 79, 85, 79, 78, 80, 81, 79, 72]\n",
            "[64, 60, 67, 67, 69, 67, 64, 60, 64, 60, 57, 65, 62, 67, 60, 60, 64, 63, 59, 56, 63, 63, 60, 60]\n",
            "Epoch [443/500], Loss: 68.15131378173828, Learning Rate: 0.0\n",
            "[84, 83, 81, 68, 77, 81, 81, 79, 84, 83, 81, 79, 80, 81, 81, 79, 77, 79, 80, 76, 82, 80, 80, 79]\n",
            "[60, 55, 57, 59, 60, 53, 57, 60, 57, 65, 65, 67, 69, 66, 59, 64, 60, 61, 60, 59, 60, 60, 63, 58]\n",
            "Epoch [444/500], Loss: 77.80603790283203, Learning Rate: 0.0\n",
            "[86, 71, 87, 89, 89, 91, 91, 89, 87, 86, 86, 84, 91, 89, 87, 86, 87, 90, 84, 86, 85, 86, 87, 82]\n",
            "[67, 72, 72, 68, 70, 67, 63, 60, 60, 55, 55, 60, 60, 62, 63, 67, 66, 67, 65, 65, 63, 66, 64, 62]\n",
            "Epoch [445/500], Loss: 77.3564682006836, Learning Rate: 0.0\n",
            "[88, 90, 84, 83, 92, 92, 86, 71, 89, 86, 86, 86, 88, 84, 86, 78, 81, 82, 77, 84, 81, 79, 80, 78]\n",
            "[64, 69, 69, 67, 67, 69, 71, 72, 65, 67, 67, 67, 68, 69, 71, 72, 62, 61, 63, 60, 63, 63, 59, 65]\n",
            "Epoch [446/500], Loss: 76.54826354980469, Learning Rate: 0.0\n",
            "[91, 91, 91, 89, 91, 92, 91, 89, 87, 86, 86, 86, 69, 87, 87, 89, 89, 94, 89, 87, 84, 90, 84, 81]\n",
            "[75, 71, 72, 74, 74, 62, 63, 63, 65, 67, 67, 67, 79, 77, 75, 74, 68, 66, 67, 68, 69, 68, 67, 65]\n",
            "Epoch [447/500], Loss: 73.01093292236328, Learning Rate: 0.0\n",
            "[84, 84, 70, 80, 84, 87, 86, 86, 84, 84, 84, 84, 79, 79, 80, 79, 79, 75, 73, 78, 80, 74, 75, 80]\n",
            "[64, 66, 67, 67, 63, 60, 67, 55, 60, 60, 60, 60, 60, 63, 56, 60, 60, 57, 64, 58, 61, 61, 56, 62]\n",
            "Epoch [448/500], Loss: 74.36259460449219, Learning Rate: 0.0\n",
            "[86, 87, 89, 86, 87, 86, 84, 84, 82, 82, 87, 89, 82, 87, 92, 91, 82, 81, 81, 79, 81, 86, 78, 78]\n",
            "[67, 72, 69, 70, 67, 65, 65, 65, 58, 58, 67, 70, 75, 75, 72, 75, 65, 63, 64, 60, 62, 63, 62, 61]\n",
            "Epoch [449/500], Loss: 75.43701934814453, Learning Rate: 0.0\n",
            "[89, 86, 86, 82, 82, 84, 84, 75, 86, 73, 80, 87, 79, 84, 86, 87, 91, 93, 93, 89, 86, 92, 87, 83]\n",
            "[69, 70, 66, 67, 63, 68, 69, 70, 71, 72, 72, 72, 67, 63, 62, 60, 64, 69, 63, 66, 67, 72, 63, 65]\n",
            "Epoch [450/500], Loss: 70.4782943725586, Learning Rate: 0.0\n",
            "[86, 86, 83, 82, 79, 79, 79, 79, 84, 84, 88, 89, 91, 93, 91, 91, 87, 84, 86, 82, 87, 85, 85, 85]\n",
            "[66, 62, 67, 62, 67, 67, 67, 67, 64, 65, 72, 69, 67, 65, 72, 72, 63, 63, 67, 67, 64, 66, 67, 62]\n",
            "Epoch [451/500], Loss: 81.01074981689453, Learning Rate: 0.0\n",
            "[84, 82, 81, 79, 79, 75, 77, 81, 77, 74, 74, 72, 72, 72, 75, 77, 87, 87, 87, 89, 86, 87, 89, 88]\n",
            "[54, 55, 50, 55, 59, 60, 62, 63, 60, 53, 55, 48, 48, 60, 60, 56, 68, 70, 68, 67, 68, 67, 66, 68]\n",
            "Epoch [452/500], Loss: 73.34717559814453, Learning Rate: 0.0\n",
            "[81, 82, 81, 82, 79, 77, 75, 72, 79, 79, 77, 75, 72, 71, 79, 79, 78, 77, 71, 81, 75, 73, 78, 71]\n",
            "[65, 62, 65, 62, 63, 68, 67, 60, 59, 59, 57, 60, 60, 63, 63, 59, 61, 61, 59, 58, 61, 58, 57, 62]\n",
            "Epoch [453/500], Loss: 84.79742431640625, Learning Rate: 0.0\n",
            "[78, 79, 79, 79, 76, 77, 79, 81, 81, 93, 79, 79, 79, 77, 77, 77, 88, 93, 89, 92, 90, 86, 89, 91]\n",
            "[57, 55, 55, 55, 60, 57, 55, 53, 55, 57, 58, 55, 60, 53, 53, 53, 68, 65, 75, 65, 66, 72, 65, 69]\n",
            "Epoch [454/500], Loss: 72.19876098632812, Learning Rate: 0.0\n",
            "[84, 84, 83, 84, 84, 89, 89, 91, 87, 89, 86, 84, 86, 70, 86, 84, 78, 79, 71, 75, 84, 71, 75, 77]\n",
            "[62, 67, 55, 60, 72, 69, 70, 63, 68, 65, 67, 60, 67, 72, 65, 68, 61, 62, 59, 55, 63, 59, 59, 62]\n",
            "Epoch [455/500], Loss: 100.84893798828125, Learning Rate: 0.0\n",
            "[85, 85, 85, 85, 87, 83, 88, 65, 87, 89, 87, 85, 72, 85, 82, 80, 87, 82, 87, 86, 83, 82, 85, 86]\n",
            "[73, 70, 65, 66, 67, 68, 63, 56, 60, 61, 63, 65, 67, 68, 63, 56, 65, 66, 69, 66, 62, 67, 67, 63]\n",
            "Epoch [456/500], Loss: 72.75213623046875, Learning Rate: 0.0\n",
            "[84, 84, 88, 89, 67, 93, 91, 91, 93, 95, 96, 91, 88, 91, 91, 91, 70, 75, 72, 72, 72, 75, 70, 68]\n",
            "[72, 65, 60, 74, 76, 72, 67, 67, 74, 67, 69, 71, 72, 74, 67, 67, 54, 52, 52, 49, 57, 63, 54, 58]\n",
            "Epoch [457/500], Loss: 73.02090454101562, Learning Rate: 0.0\n",
            "[86, 84, 86, 83, 83, 79, 82, 82, 80, 84, 84, 84, 80, 84, 84, 84, 84, 85, 86, 82, 85, 87, 82, 83]\n",
            "[67, 68, 65, 67, 67, 55, 67, 62, 60, 63, 68, 65, 67, 60, 60, 60, 67, 66, 65, 63, 65, 64, 64, 63]\n",
            "Epoch [458/500], Loss: 77.84654235839844, Learning Rate: 0.0\n",
            "[76, 77, 79, 79, 77, 76, 84, 94, 76, 76, 74, 89, 77, 76, 74, 74, 89, 89, 90, 86, 87, 93, 86, 87]\n",
            "[64, 67, 64, 61, 62, 64, 67, 67, 60, 60, 65, 65, 62, 60, 57, 59, 70, 70, 69, 67, 68, 68, 67, 65]\n",
            "Epoch [459/500], Loss: 66.01167297363281, Learning Rate: 0.0\n",
            "[91, 89, 89, 87, 87, 83, 91, 87, 89, 87, 86, 70, 90, 82, 84, 87, 80, 80, 81, 79, 80, 79, 81, 79]\n",
            "[67, 70, 58, 63, 63, 75, 72, 72, 68, 68, 65, 68, 68, 63, 68, 67, 63, 62, 58, 62, 64, 57, 60, 60]\n",
            "Epoch [460/500], Loss: 68.88654327392578, Learning Rate: 0.0\n",
            "[84, 67, 84, 86, 87, 84, 85, 70, 82, 82, 82, 80, 77, 77, 79, 79, 89, 90, 90, 91, 88, 90, 91, 90]\n",
            "[72, 70, 68, 65, 67, 69, 70, 70, 67, 63, 60, 60, 56, 56, 55, 55, 67, 70, 70, 68, 69, 72, 67, 69]\n",
            "Epoch [461/500], Loss: 96.04720306396484, Learning Rate: 0.0\n",
            "[84, 84, 66, 87, 86, 76, 89, 86, 84, 84, 87, 87, 86, 87, 89, 82, 95, 97, 91, 88, 93, 89, 85, 83]\n",
            "[72, 72, 74, 72, 70, 68, 65, 67, 60, 60, 72, 72, 67, 65, 62, 63, 72, 67, 68, 67, 73, 69, 67, 64]\n",
            "Epoch [462/500], Loss: 71.84962463378906, Learning Rate: 0.0\n",
            "[83, 84, 84, 84, 79, 81, 79, 77, 77, 83, 73, 91, 84, 86, 86, 84, 73, 71, 75, 73, 76, 74, 76, 71]\n",
            "[62, 60, 60, 60, 60, 65, 62, 62, 65, 69, 69, 69, 66, 67, 66, 64, 59, 57, 57, 60, 58, 58, 60, 59]\n",
            "Epoch [463/500], Loss: 87.23069763183594, Learning Rate: 0.0\n",
            "[84, 86, 80, 86, 84, 83, 81, 79, 84, 86, 87, 89, 88, 86, 84, 84, 89, 90, 87, 88, 91, 87, 88, 91]\n",
            "[72, 71, 72, 67, 69, 71, 74, 67, 69, 71, 72, 69, 67, 67, 60, 60, 66, 70, 61, 65, 68, 63, 64, 65]\n",
            "Epoch [464/500], Loss: 78.65511322021484, Learning Rate: 0.0\n",
            "[91, 89, 87, 86, 87, 95, 91, 89, 91, 91, 87, 89, 87, 86, 91, 89, 89, 85, 86, 86, 88, 87, 86, 90]\n",
            "[75, 71, 72, 67, 60, 72, 75, 70, 67, 68, 72, 65, 72, 67, 63, 62, 67, 70, 69, 68, 66, 67, 66, 68]\n",
            "Epoch [465/500], Loss: 107.10555267333984, Learning Rate: 0.0\n",
            "[84, 84, 73, 83, 86, 83, 81, 81, 81, 81, 83, 84, 81, 81, 79, 79, 79, 78, 81, 81, 82, 84, 82, 82]\n",
            "[60, 60, 67, 67, 71, 71, 74, 69, 62, 62, 63, 64, 60, 62, 67, 67, 65, 62, 60, 60, 62, 57, 61, 61]\n",
            "Epoch [466/500], Loss: 62.82450866699219, Learning Rate: 0.0\n",
            "[86, 86, 86, 84, 84, 86, 92, 77, 81, 84, 83, 88, 81, 80, 80, 81, 70, 66, 71, 73, 68, 74, 70, 72]\n",
            "[67, 67, 68, 69, 72, 76, 76, 76, 68, 69, 64, 65, 60, 64, 64, 69, 59, 52, 56, 56, 54, 56, 55, 57]\n",
            "Epoch [467/500], Loss: 76.2678451538086, Learning Rate: 0.0\n",
            "[81, 81, 87, 84, 86, 86, 88, 88, 88, 70, 75, 88, 88, 88, 86, 84, 88, 96, 92, 92, 92, 94, 90, 89]\n",
            "[65, 62, 64, 69, 65, 67, 60, 60, 60, 72, 72, 69, 69, 65, 65, 64, 72, 69, 68, 66, 72, 68, 66, 69]\n",
            "Epoch [468/500], Loss: 80.07398223876953, Learning Rate: 0.0\n",
            "[92, 88, 74, 72, 79, 79, 77, 82, 79, 75, 77, 79, 79, 82, 84, 87, 85, 95, 86, 82, 85, 88, 85, 77]\n",
            "[63, 67, 55, 60, 60, 58, 56, 55, 63, 68, 68, 67, 67, 63, 63, 60, 66, 67, 64, 60, 65, 67, 63, 61]\n",
            "Epoch [469/500], Loss: 107.61797332763672, Learning Rate: 0.0\n",
            "[82, 82, 81, 82, 82, 84, 86, 87, 86, 84, 84, 86, 90, 82, 84, 82, 93, 91, 92, 94, 87, 89, 92, 90]\n",
            "[67, 64, 65, 58, 63, 72, 74, 72, 70, 68, 68, 67, 72, 70, 68, 67, 70, 71, 72, 69, 68, 70, 69, 67]\n",
            "Epoch [470/500], Loss: 67.00763702392578, Learning Rate: 0.0\n",
            "[76, 84, 82, 80, 79, 77, 77, 79, 86, 87, 87, 86, 86, 84, 84, 84, 85, 84, 78, 84, 85, 83, 87, 85]\n",
            "[72, 68, 67, 65, 63, 68, 68, 67, 67, 60, 63, 67, 67, 60, 60, 60, 65, 68, 66, 65, 64, 61, 66, 62]\n",
            "Epoch [471/500], Loss: 84.70669555664062, Learning Rate: 0.0\n",
            "[82, 82, 79, 77, 75, 75, 75, 92, 82, 65, 84, 86, 87, 87, 86, 86, 83, 82, 86, 82, 80, 82, 82, 80]\n",
            "[70, 60, 63, 58, 51, 51, 51, 67, 67, 67, 68, 59, 60, 63, 67, 67, 63, 59, 64, 64, 60, 65, 62, 61]\n",
            "Epoch [472/500], Loss: 77.19564819335938, Learning Rate: 0.0\n",
            "[75, 87, 86, 84, 83, 84, 86, 86, 91, 91, 86, 87, 89, 87, 86, 84, 94, 97, 95, 89, 96, 97, 91, 97]\n",
            "[72, 68, 65, 68, 62, 60, 67, 67, 60, 63, 67, 72, 68, 65, 67, 60, 72, 72, 72, 69, 71, 66, 72, 65]\n",
            "Epoch [473/500], Loss: 111.18563079833984, Learning Rate: 0.0\n",
            "[84, 82, 80, 79, 80, 82, 82, 79, 87, 82, 82, 86, 72, 89, 86, 82, 85, 83, 84, 88, 85, 85, 85, 87]\n",
            "[65, 62, 58, 63, 70, 67, 65, 65, 63, 62, 60, 67, 72, 68, 65, 62, 63, 65, 64, 64, 62, 64, 64, 63]\n",
            "Epoch [474/500], Loss: 74.98198699951172, Learning Rate: 0.0\n",
            "[83, 84, 86, 86, 86, 87, 89, 86, 87, 86, 84, 82, 87, 89, 91, 87, 88, 93, 92, 88, 89, 86, 97, 89]\n",
            "[62, 63, 67, 67, 67, 72, 69, 70, 67, 63, 65, 58, 60, 62, 63, 68, 69, 70, 71, 74, 75, 68, 67, 64]\n",
            "Epoch [475/500], Loss: 107.84060668945312, Learning Rate: 0.0\n",
            "[79, 76, 74, 74, 72, 72, 79, 81, 79, 79, 84, 84, 83, 83, 81, 81, 83, 83, 84, 80, 87, 83, 83, 81]\n",
            "[48, 52, 55, 55, 48, 48, 59, 62, 55, 55, 57, 54, 55, 64, 60, 62, 64, 63, 66, 63, 63, 66, 63, 63]\n",
            "Epoch [476/500], Loss: 80.14385223388672, Learning Rate: 0.0\n",
            "[76, 79, 79, 79, 78, 79, 79, 76, 77, 77, 76, 71, 74, 68, 74, 90, 87, 89, 89, 86, 86, 88, 88, 88]\n",
            "[60, 64, 60, 62, 62, 55, 55, 60, 62, 64, 64, 67, 64, 64, 65, 67, 69, 68, 65, 68, 72, 68, 65, 69]\n",
            "Epoch [477/500], Loss: 82.83641052246094, Learning Rate: 0.0\n",
            "[84, 88, 89, 91, 89, 88, 86, 88, 88, 88, 86, 89, 86, 77, 86, 84, 90, 94, 96, 92, 90, 93, 97, 93]\n",
            "[60, 72, 69, 67, 62, 64, 67, 60, 69, 64, 67, 62, 65, 69, 65, 60, 73, 75, 69, 75, 75, 70, 70, 69]\n",
            "Epoch [478/500], Loss: 102.00439453125, Learning Rate: 0.0\n",
            "[79, 84, 85, 84, 86, 69, 89, 79, 81, 82, 79, 82, 77, 75, 74, 85, 70, 67, 70, 71, 67, 72, 70, 69]\n",
            "[67, 63, 62, 64, 66, 67, 60, 55, 60, 58, 51, 55, 58, 58, 58, 65, 57, 52, 53, 55, 54, 53, 55, 54]\n",
            "Epoch [479/500], Loss: 71.3509750366211, Learning Rate: 0.0\n",
            "[88, 86, 86, 84, 86, 81, 88, 89, 89, 88, 88, 86, 88, 88, 88, 89, 94, 95, 92, 91, 89, 90, 87, 86]\n",
            "[62, 67, 55, 60, 67, 79, 76, 74, 68, 69, 69, 62, 69, 72, 73, 74, 72, 65, 74, 70, 69, 74, 68, 69]\n",
            "Epoch [480/500], Loss: 70.82882690429688, Learning Rate: 0.0\n",
            "[84, 89, 88, 91, 86, 72, 81, 79, 79, 86, 88, 86, 88, 89, 79, 84, 86, 88, 91, 90, 88, 92, 90, 90]\n",
            "[64, 62, 60, 64, 67, 64, 62, 67, 67, 71, 74, 67, 64, 62, 65, 69, 71, 70, 68, 68, 66, 66, 67, 67]\n",
            "Epoch [481/500], Loss: 87.25511932373047, Learning Rate: 0.0\n",
            "[72, 71, 71, 69, 76, 77, 74, 76, 74, 72, 72, 72, 72, 72, 76, 74, 78, 81, 76, 78, 76, 80, 74, 72]\n",
            "[57, 50, 52, 57, 57, 62, 65, 64, 55, 48, 48, 48, 60, 53, 60, 55, 59, 56, 59, 59, 59, 63, 58, 62]\n",
            "Epoch [482/500], Loss: 75.30779266357422, Learning Rate: 0.0\n",
            "[84, 87, 86, 86, 84, 84, 84, 84, 79, 92, 80, 79, 81, 77, 75, 75, 87, 84, 81, 84, 86, 77, 85, 82]\n",
            "[63, 60, 65, 67, 60, 60, 60, 60, 60, 63, 65, 67, 70, 58, 63, 63, 62, 67, 67, 62, 61, 62, 65, 61]\n",
            "Epoch [483/500], Loss: 73.68163299560547, Learning Rate: 0.0\n",
            "[72, 76, 77, 66, 79, 77, 76, 74, 79, 81, 83, 84, 83, 81, 81, 79, 79, 79, 82, 80, 80, 82, 80, 78]\n",
            "[48, 60, 62, 64, 57, 59, 60, 55, 67, 66, 64, 64, 62, 62, 50, 55, 63, 61, 60, 64, 62, 61, 63, 60]\n",
            "Epoch [484/500], Loss: 87.11466979980469, Learning Rate: 0.0\n",
            "[79, 79, 81, 83, 84, 79, 79, 79, 81, 79, 77, 76, 74, 74, 72, 72, 91, 90, 85, 88, 91, 86, 93, 87]\n",
            "[52, 52, 60, 59, 57, 59, 60, 60, 65, 65, 64, 60, 65, 62, 60, 60, 68, 74, 71, 69, 66, 67, 69, 64]\n",
            "Epoch [485/500], Loss: 72.65363311767578, Learning Rate: 0.0\n",
            "[80, 79, 77, 77, 80, 82, 84, 84, 82, 80, 79, 79, 75, 79, 69, 77, 86, 89, 84, 85, 89, 86, 85, 85]\n",
            "[53, 55, 58, 58, 59, 58, 57, 58, 61, 65, 63, 63, 62, 64, 65, 53, 66, 65, 64, 67, 66, 65, 66, 66]\n",
            "Epoch [486/500], Loss: 78.37161254882812, Learning Rate: 0.0\n",
            "[91, 96, 95, 93, 95, 96, 93, 91, 91, 96, 91, 93, 88, 91, 79, 88, 86, 91, 91, 90, 92, 86, 87, 88]\n",
            "[72, 76, 79, 74, 77, 76, 74, 67, 72, 69, 64, 65, 69, 71, 74, 69, 66, 71, 65, 66, 66, 68, 65, 69]\n",
            "Epoch [487/500], Loss: 85.17681121826172, Learning Rate: 0.0\n",
            "[81, 79, 77, 65, 77, 74, 74, 72, 84, 84, 84, 83, 79, 81, 81, 76, 82, 80, 82, 78, 83, 81, 80, 81]\n",
            "[62, 59, 57, 60, 57, 53, 53, 48, 48, 52, 54, 55, 58, 57, 53, 60, 62, 60, 62, 61, 63, 62, 65, 59]\n",
            "Epoch [488/500], Loss: 60.44660568237305, Learning Rate: 0.0\n",
            "[77, 80, 82, 84, 82, 87, 86, 84, 82, 92, 86, 84, 82, 80, 72, 77, 71, 69, 71, 73, 69, 73, 71, 70]\n",
            "[65, 65, 63, 68, 63, 63, 67, 60, 67, 72, 70, 68, 62, 65, 60, 65, 58, 53, 55, 56, 54, 54, 56, 55]\n",
            "Epoch [489/500], Loss: 88.85883331298828, Learning Rate: 0.0\n",
            "[87, 89, 91, 87, 87, 91, 92, 91, 89, 89, 87, 87, 91, 89, 87, 84, 80, 81, 82, 80, 82, 82, 82, 84]\n",
            "[67, 67, 63, 75, 75, 75, 72, 70, 68, 70, 63, 63, 63, 63, 72, 60, 65, 62, 64, 61, 64, 62, 62, 63]\n",
            "Epoch [490/500], Loss: 64.85881805419922, Learning Rate: 0.0\n",
            "[73, 84, 82, 80, 79, 77, 77, 79, 86, 74, 87, 86, 86, 84, 84, 84, 70, 64, 64, 67, 68, 66, 66, 69]\n",
            "[67, 68, 70, 72, 70, 68, 68, 67, 67, 72, 68, 65, 67, 60, 60, 60, 54, 48, 53, 56, 51, 49, 54, 53]\n",
            "Epoch [491/500], Loss: 65.09530639648438, Learning Rate: 0.0\n",
            "[82, 82, 87, 89, 82, 87, 92, 91, 89, 89, 87, 87, 86, 87, 89, 91, 92, 94, 98, 92, 91, 92, 94, 92]\n",
            "[58, 58, 67, 70, 75, 75, 72, 75, 68, 70, 63, 63, 70, 72, 74, 75, 71, 70, 72, 73, 67, 75, 69, 69]\n",
            "Epoch [492/500], Loss: 67.38690185546875, Learning Rate: 0.0\n",
            "[87, 84, 86, 84, 81, 84, 84, 83, 88, 89, 86, 88, 84, 84, 88, 84, 90, 91, 92, 89, 92, 87, 91, 92]\n",
            "[67, 64, 60, 60, 62, 57, 60, 64, 61, 62, 67, 64, 69, 66, 67, 60, 67, 73, 69, 68, 69, 68, 69, 66]\n",
            "Epoch [493/500], Loss: 75.81871795654297, Learning Rate: 0.0\n",
            "[79, 79, 81, 81, 83, 83, 84, 84, 86, 86, 84, 83, 81, 83, 81, 81, 83, 81, 81, 80, 82, 80, 79, 79]\n",
            "[60, 64, 65, 65, 67, 64, 60, 60, 59, 62, 66, 67, 60, 57, 60, 62, 64, 62, 61, 62, 66, 62, 61, 63]\n",
            "Epoch [494/500], Loss: 71.76626586914062, Learning Rate: 0.0\n",
            "[88, 90, 82, 69, 69, 69, 73, 76, 73, 74, 88, 74, 76, 77, 79, 86, 86, 88, 91, 86, 90, 91, 86, 84]\n",
            "[64, 62, 64, 57, 57, 57, 52, 57, 55, 53, 52, 50, 61, 59, 57, 62, 66, 68, 63, 64, 68, 69, 64, 67]\n",
            "Epoch [495/500], Loss: 73.62136840820312, Learning Rate: 0.0\n",
            "[84, 84, 86, 86, 86, 86, 86, 86, 79, 79, 71, 87, 67, 75, 84, 65, 92, 94, 95, 94, 90, 96, 95, 93]\n",
            "[60, 60, 67, 67, 67, 67, 67, 67, 67, 67, 64, 64, 65, 65, 69, 69, 70, 75, 67, 72, 73, 69, 69, 67]\n",
            "Epoch [496/500], Loss: 92.45801544189453, Learning Rate: 0.0\n",
            "[95, 84, 88, 88, 93, 89, 89, 91, 84, 89, 86, 86, 84, 86, 89, 92, 82, 84, 85, 82, 86, 83, 88, 83]\n",
            "[72, 64, 62, 60, 65, 69, 69, 72, 69, 65, 67, 67, 60, 71, 71, 72, 65, 61, 65, 70, 65, 65, 62, 63]\n",
            "Epoch [497/500], Loss: 81.14290618896484, Learning Rate: 0.0\n",
            "[87, 79, 76, 82, 79, 81, 77, 79, 79, 82, 84, 87, 86, 84, 83, 84, 89, 92, 92, 90, 90, 88, 92, 91]\n",
            "[72, 67, 70, 62, 63, 68, 56, 55, 60, 67, 65, 63, 65, 68, 67, 60, 68, 67, 72, 70, 72, 69, 65, 70]\n",
            "Epoch [498/500], Loss: 88.06291198730469, Learning Rate: 0.0\n",
            "[87, 89, 89, 87, 87, 86, 84, 82, 81, 82, 82, 84, 82, 80, 80, 75, 88, 94, 94, 90, 91, 92, 95, 94]\n",
            "[72, 68, 68, 63, 72, 66, 69, 67, 70, 67, 67, 64, 62, 65, 68, 72, 72, 73, 70, 72, 74, 69, 70, 69]\n",
            "Epoch [499/500], Loss: 92.34778594970703, Learning Rate: 0.0\n",
            "[83, 84, 84, 84, 87, 87, 87, 86, 87, 84, 84, 83, 69, 87, 87, 86, 81, 82, 81, 79, 81, 80, 81, 81]\n",
            "[55, 60, 60, 60, 60, 60, 63, 67, 63, 68, 68, 67, 72, 72, 69, 70, 62, 64, 61, 62, 63, 59, 64, 59]\n",
            "Epoch [500/500], Loss: 75.52479553222656, Learning Rate: 0.0\n",
            "[84, 81, 83, 84, 81, 68, 78, 79, 79, 84, 86, 88, 89, 88, 84, 88, 89, 89, 91, 89, 90, 93, 89, 92]\n",
            "[60, 60, 59, 57, 62, 59, 62, 55, 67, 64, 64, 60, 60, 72, 67, 60, 71, 70, 70, 68, 71, 68, 68, 69]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_int = random.randint(0, len(sequences))\n",
        "input_sequence = torch.tensor([sequences[random_int]], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "GOE4pfLJXSF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a prediction for the single example\n",
        "with torch.no_grad():\n",
        "    prediction = model(input_sequence, output_size)\n",
        "\n",
        "predicted_notes = prediction.squeeze().tolist()\n",
        "bass_prediction = predicted_notes[0]\n",
        "melody_prediction = predicted_notes[1]"
      ],
      "metadata": {
        "id": "YOFmW6-JXTZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_bass_pred = [int(round(note)) for note in bass_prediction]\n",
        "int_melody_pred = [int(round(note)) for note in melody_prediction]\n",
        "\n",
        "melody_input = [int(note) for note in input_sequence.tolist()[0][1]]\n",
        "bass_input = [int(note) for note in input_sequence.tolist()[0][0]]"
      ],
      "metadata": {
        "id": "f6Mv7xiZ75xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_melody = melody_input + int_melody_pred\n",
        "full_bass = bass_input + int_bass_pred\n",
        "print(full_melody)\n",
        "print(full_bass)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA8UXQNxXYJY",
        "outputId": "aab728f6-ec5e-42be-f9b0-da5c02f071d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[91, 86, 86, 88, 90, 91, 91, 91, 91, 93, 91, 86, 89, 88, 86, 84, 84, 84, 84, 86, 89, 88, 91, 91, 86, 85, 87, 85, 86, 91, 88, 88]\n",
            "[64, 67, 67, 72, 69, 67, 67, 67, 72, 65, 64, 67, 62, 64, 67, 60, 60, 60, 72, 71, 69, 72, 60, 64, 67, 66, 66, 64, 64, 67, 66, 64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_sample = np.array([full_melody,\n",
        "                       full_melody,\n",
        "                       full_bass,\n",
        "                       full_bass\n",
        "                       ])"
      ],
      "metadata": {
        "id": "Wi5piVFjXZZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNb77mCJ5AZZ",
        "outputId": "aec52979-6e50-46ca-cd7c-67db337fa9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[91, 86, 86, 88, 90, 91, 91, 91, 91, 93, 91, 86, 89, 88, 86, 84,\n",
              "        84, 84, 84, 86, 89, 88, 91, 91, 86, 85, 87, 85, 86, 91, 88, 88],\n",
              "       [91, 86, 86, 88, 90, 91, 91, 91, 91, 93, 91, 86, 89, 88, 86, 84,\n",
              "        84, 84, 84, 86, 89, 88, 91, 91, 86, 85, 87, 85, 86, 91, 88, 88],\n",
              "       [64, 67, 67, 72, 69, 67, 67, 67, 72, 65, 64, 67, 62, 64, 67, 60,\n",
              "        60, 60, 72, 71, 69, 72, 60, 64, 67, 66, 66, 64, 64, 67, 66, 64],\n",
              "       [64, 67, 67, 72, 69, 67, 67, 67, 72, 65, 64, 67, 62, 64, 67, 60,\n",
              "        60, 60, 72, 71, 69, 72, 60, 64, 67, 66, 66, 64, 64, 67, 66, 64]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RH3u-iO35BXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}